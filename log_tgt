[root@s63 pkg]# ./nvmf_tgt -L all
[2023-07-11 11:27:50.453803] Starting SPDK v23.09-pre git sha1 d10dc6a / DPDK 23.03.0 initialization...
[2023-07-11 11:27:50.453862] [ DPDK EAL parameters: nvmf --no-shconf -c 0x1 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid61838 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2023-07-11 11:27:50.500742] app.c: 767:spdk_app_start: *NOTICE*: Total cores available: 1
[2023-07-11 11:27:50.554997] thread.c: 343:_thread_lib_init: *DEBUG*: spdk_msg_mempool was created with size: 262143
[2023-07-11 11:27:50.555063] thread.c: 552:spdk_thread_create: *DEBUG*: Allocating new thread (1, app_thread)
[2023-07-11 11:27:50.555830] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 0
[2023-07-11 11:27:50.581296] thread.c:2138:spdk_io_device_register: *DEBUG*: Registering io_device iobuf (0xae3540) on thread app_thread
[2023-07-11 11:27:50.581321] thread.c:2138:spdk_io_device_register: *DEBUG*: Registering io_device accel (0xae2fa0) on thread app_thread
[2023-07-11 11:27:50.581335] accel_sw.c: 605:sw_accel_module_init: *NOTICE*: Accel framework software module initialized.
[2023-07-11 11:27:50.581353] thread.c:2138:spdk_io_device_register: *DEBUG*: Registering io_device sw_accel_module (0xae3140) on thread app_thread
[2023-07-11 11:27:50.581358] accel.c:2523:spdk_accel_initialize: *DEBUG*: OPC 0x0 now assigned to software
[2023-07-11 11:27:50.581363] accel.c:2523:spdk_accel_initialize: *DEBUG*: OPC 0x1 now assigned to software
[2023-07-11 11:27:50.581368] accel.c:2523:spdk_accel_initialize: *DEBUG*: OPC 0x2 now assigned to software
[2023-07-11 11:27:50.581372] accel.c:2523:spdk_accel_initialize: *DEBUG*: OPC 0x3 now assigned to software
[2023-07-11 11:27:50.581376] accel.c:2523:spdk_accel_initialize: *DEBUG*: OPC 0x4 now assigned to software
[2023-07-11 11:27:50.581381] accel.c:2523:spdk_accel_initialize: *DEBUG*: OPC 0x5 now assigned to software
[2023-07-11 11:27:50.581385] accel.c:2523:spdk_accel_initialize: *DEBUG*: OPC 0x6 now assigned to software
[2023-07-11 11:27:50.581389] accel.c:2523:spdk_accel_initialize: *DEBUG*: OPC 0x7 now assigned to software
[2023-07-11 11:27:50.581394] accel.c:2523:spdk_accel_initialize: *DEBUG*: OPC 0x8 now assigned to software
[2023-07-11 11:27:50.581398] accel.c:2523:spdk_accel_initialize: *DEBUG*: OPC 0x9 now assigned to software
[2023-07-11 11:27:50.581403] accel.c:2523:spdk_accel_initialize: *DEBUG*: OPC 0xa now assigned to software
[2023-07-11 11:27:50.581414] bdev_raid.c: 895:raid_bdev_get_ctx_size: *DEBUG*: raid_bdev_get_ctx_size
[2023-07-11 11:27:50.601197] thread.c:2138:spdk_io_device_register: *DEBUG*: Registering io_device bdev_mgr (0xae2e20) on thread app_thread
[2023-07-11 11:27:50.601744] thread.c:2138:spdk_io_device_register: *DEBUG*: Registering io_device bdev_malloc (0xae0020) on thread app_thread
[2023-07-11 11:27:50.601755] thread.c:2138:spdk_io_device_register: *DEBUG*: Registering io_device null_bdev (0xae0100) on thread app_thread
[2023-07-11 11:27:50.601761] thread.c:2138:spdk_io_device_register: *DEBUG*: Registering io_device nvme_poll_groups (0xae0380) on thread app_thread
[2023-07-11 11:27:50.601766] thread.c:2138:spdk_io_device_register: *DEBUG*: Registering io_device aio_module (0xae1420) on thread app_thread
[2023-07-11 11:27:50.601776] thread.c:2138:spdk_io_device_register: *DEBUG*: Registering io_device nvmf_tgt (0x2373570) on thread app_thread
[2023-07-11 11:27:50.601813] thread.c: 552:spdk_thread_create: *DEBUG*: Allocating new thread (2, nvmf_tgt_poll_group_0)
[2023-07-11 11:27:50.601836] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 3 voluntary context switches and 6 involuntary context switches in the last second.
[2023-07-11 11:27:50.601847] thread.c:2327:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 1
[2023-07-11 11:27:51.604188] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:27:52.606545] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:27:53.608894] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 11 involuntary context switches in the last second.
[2023-07-11 11:27:54.611251] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:27:55.613599] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:27:56.615952] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:27:57.618310] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:27:58.620658] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:27:59.623011] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:00.625369] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:28:01.627717] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:28:02.630070] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:03.632424] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:04.634777] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:05.637135] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:28:06.639483] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:28:07.641836] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 6 involuntary context switches in the last second.
[2023-07-11 11:28:08.644189] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:09.646542] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:28:10.648900] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:11.651248] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:28:12.653601] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 6 involuntary context switches in the last second.
[2023-07-11 11:28:13.655954] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:28:14.658307] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:15.660660] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:16.022216] rdma.c:2594:nvmf_rdma_create: *INFO*: *** RDMA Transport Init ***
  Transport opts:  max_ioq_depth=128, max_io_size=131072,
  max_io_qpairs_per_ctrlr=127, io_unit_size=8192,
  in_capsule_data_size=4096, max_aq_depth=128,
  num_shared_buffers=4095, num_cqe=4096, max_srq_depth=4096, no_srq=0,  acceptor_backlog=100, no_wr_batching=0 abort_timeout_sec=1
[2023-07-11 11:28:16.046261] rdma.c:2459:create_ib_device: *DEBUG*: New device 0x238f530 is added to RDMA trasport
[2023-07-11 11:28:16.047742] rdma.c:2489:create_ib_device: *NOTICE*: Create IB device mlx5_0(0x238f530/0x7fadd822d150) succeed.
[2023-07-11 11:28:16.047762] rdma.c:2459:create_ib_device: *DEBUG*: New device 0x2391cb0 is added to RDMA trasport
[2023-07-11 11:28:16.049152] rdma.c:2489:create_ib_device: *NOTICE*: Create IB device mlx5_1(0x2391cb0/0x7fadd81eb150) succeed.
[2023-07-11 11:28:16.049166] rdma.c:2459:create_ib_device: *DEBUG*: New device 0x23931b0 is added to RDMA trasport
[2023-07-11 11:28:16.050576] rdma.c:2489:create_ib_device: *NOTICE*: Create IB device mlx5_2(0x23931b0/0x7fadd3f69150) succeed.
[2023-07-11 11:28:16.050590] rdma.c:2459:create_ib_device: *DEBUG*: New device 0x23946b0 is added to RDMA trasport
[2023-07-11 11:28:16.051997] rdma.c:2489:create_ib_device: *NOTICE*: Create IB device mlx5_3(0x23946b0/0x7fadd3f27150) succeed.
[2023-07-11 11:28:16.052020] rdma.c:3881:nvmf_rdma_poller_create: *DEBUG*: Create poller 0x2395e70 on device 0x238f530 in poll group 0x2395da0.
[2023-07-11 11:28:16.056328] rdma.c: 724:nvmf_rdma_resources_create: *DEBUG*: Command Array: 0x20000709f000 Length: 40000
[2023-07-11 11:28:16.056341] rdma.c: 726:nvmf_rdma_resources_create: *DEBUG*: Completion Array: 0x200019426000 Length: 10000
[2023-07-11 11:28:16.056345] rdma.c: 730:nvmf_rdma_resources_create: *DEBUG*: In Capsule Data Array: 0x200019c00000 Length: 1000000
[2023-07-11 11:28:16.057864] rdma.c:3881:nvmf_rdma_poller_create: *DEBUG*: Create poller 0x2396760 on device 0x2391cb0 in poll group 0x2395da0.
[2023-07-11 11:28:16.062136] rdma.c: 724:nvmf_rdma_resources_create: *DEBUG*: Command Array: 0x20000705e000 Length: 40000
[2023-07-11 11:28:16.062147] rdma.c: 726:nvmf_rdma_resources_create: *DEBUG*: Completion Array: 0x20001ac26000 Length: 10000
[2023-07-11 11:28:16.062151] rdma.c: 730:nvmf_rdma_resources_create: *DEBUG*: In Capsule Data Array: 0x20001b400000 Length: 1000000
[2023-07-11 11:28:16.063589] rdma.c:3881:nvmf_rdma_poller_create: *DEBUG*: Create poller 0x23a0020 on device 0x23931b0 in poll group 0x2395da0.
[2023-07-11 11:28:16.088900] rdma.c: 724:nvmf_rdma_resources_create: *DEBUG*: Command Array: 0x20000701d000 Length: 40000
[2023-07-11 11:28:16.088911] rdma.c: 726:nvmf_rdma_resources_create: *DEBUG*: Completion Array: 0x20000700c000 Length: 10000
[2023-07-11 11:28:16.088915] rdma.c: 730:nvmf_rdma_resources_create: *DEBUG*: In Capsule Data Array: 0x20001cc00000 Length: 1000000
[2023-07-11 11:28:16.090441] rdma.c:3881:nvmf_rdma_poller_create: *DEBUG*: Create poller 0x23a0820 on device 0x23946b0 in poll group 0x2395da0.
[2023-07-11 11:28:16.094722] rdma.c: 724:nvmf_rdma_resources_create: *DEBUG*: Command Array: 0x20000b2b5000 Length: 40000
[2023-07-11 11:28:16.094733] rdma.c: 726:nvmf_rdma_resources_create: *DEBUG*: Completion Array: 0x20001dc26000 Length: 10000
[2023-07-11 11:28:16.094737] rdma.c: 730:nvmf_rdma_resources_create: *DEBUG*: In Capsule Data Array: 0x20001e400000 Length: 1000000
[2023-07-11 11:28:16.096190] thread.c:2327:spdk_get_io_channel: *DEBUG*: Get io_channel 0x23a0e10 for io_device iobuf (0xae3540) on thread nvmf_tgt_poll_group_0 refcnt 1
[2023-07-11 11:28:16.100601] jsonrpc_server_tcp.c: 275:jsonrpc_server_conn_recv: *DEBUG*: remote closed connection
[2023-07-11 11:28:16.220887] jsonrpc_server_tcp.c: 275:jsonrpc_server_conn_recv: *DEBUG*: remote closed connection
[2023-07-11 11:28:16.663015] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 214 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:28:17.665367] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:28:18.667723] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 6 involuntary context switches in the last second.
[2023-07-11 11:28:19.670073] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:20.672426] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:21.674779] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 9 involuntary context switches in the last second.
[2023-07-11 11:28:22.239248] jsonrpc_server_tcp.c: 275:jsonrpc_server_conn_recv: *DEBUG*: remote closed connection
[2023-07-11 11:28:22.355525] jsonrpc_server_tcp.c: 275:jsonrpc_server_conn_recv: *DEBUG*: remote closed connection
[2023-07-11 11:28:22.677132] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:23.679485] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 6 involuntary context switches in the last second.
[2023-07-11 11:28:24.681839] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 9 involuntary context switches in the last second.
[2023-07-11 11:28:25.684192] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:28:26.686545] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 9 involuntary context switches in the last second.
[2023-07-11 11:28:27.688902] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:28.691251] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:29.693604] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:30.695957] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:31.698310] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:32.700664] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 6 involuntary context switches in the last second.
[2023-07-11 11:28:33.703017] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:34.705370] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 6 involuntary context switches in the last second.
[2023-07-11 11:28:35.707723] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:36.710076] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:28:37.712429] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:28:38.714782] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 6 involuntary context switches in the last second.
[2023-07-11 11:28:39.717136] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:40.719492] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 6 involuntary context switches in the last second.
[2023-07-11 11:28:41.473412] rdma.c:2926:nvmf_rdma_listen: *NOTICE*: *** NVMe/RDMA Target Listening on 172.17.29.63 port 4420 ***
[2023-07-11 11:28:41.481161] jsonrpc_server_tcp.c: 275:jsonrpc_server_conn_recv: *DEBUG*: remote closed connection
[2023-07-11 11:28:41.609466] jsonrpc_server_tcp.c: 275:jsonrpc_server_conn_recv: *DEBUG*: remote closed connection
[2023-07-11 11:28:41.721842] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 2 voluntary context switches and 9 involuntary context switches in the last second.
[2023-07-11 11:28:42.308522] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:42.308530] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:42.308535] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:42.308540] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:42.308544] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:42.308549] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:42.308553] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:42.308558] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 32
[2023-07-11 11:28:42.308562] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 31
[2023-07-11 11:28:42.308566] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 32 R/W Depth: 16
[2023-07-11 11:28:42.309437] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:42.309452] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x23a0ea0
[2023-07-11 11:28:42.309851] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:42.310534] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:42.310543] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:42.310553] rdma.c:1459:nvmf_rdma_fill_wr_sgl: *DEBUG*: sge[0] 0x2000194a0438 addr 0x200007200000, len 1024
[2023-07-11 11:28:42.310558] rdma.c:1851:nvmf_rdma_request_parse_sgl: *DEBUG*: Request 0x2000194a0000 took 1 buffer/s from central pool
[2023-07-11 11:28:42.310562] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 3
[2023-07-11 11:28:42.310567] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 4
[2023-07-11 11:28:42.310579] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:42.310594] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:0 cid:45056 SGL KEYED DATA BLOCK INVALIDATE KEY 0x4834d3c00 len:0x400 key:0x1fe18e
[2023-07-11 11:28:42.310609] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 0 sqsize 31
[2023-07-11 11:28:42.310614] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:42.310619] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0xffff
[2023-07-11 11:28:42.310624] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:42.310629] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:42.310633] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:42.310639] ctrlr.c: 814:_nvmf_ctrlr_connect: *DEBUG*: Connect Admin Queue for controller ID 0xffff
[2023-07-11 11:28:42.310655] ctrlr.c: 487:nvmf_ctrlr_create: *DEBUG*: cap 0x201e01007f
[2023-07-11 11:28:42.310660] ctrlr.c: 488:nvmf_ctrlr_create: *DEBUG*: vs 0x10300
[2023-07-11 11:28:42.310665] ctrlr.c: 489:nvmf_ctrlr_create: *DEBUG*: cc 0x0
[2023-07-11 11:28:42.310668] ctrlr.c: 490:nvmf_ctrlr_create: *DEBUG*: csts 0x0
[2023-07-11 11:28:42.310674] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:42.310680] ctrlr.c: 212:nvmf_ctrlr_start_keep_alive_timer: *DEBUG*: Ctrlr add keep alive poller
[2023-07-11 11:28:42.310688] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:42.310695] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:45056 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:42.310704] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:42.310708] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:42.310713] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:42.310727] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:42.310732] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:42.310745] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:42.310751] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:42.310758] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC PROPERTY GET qid:0 cid:16395 SGL KEYED DATA BLOCK ADDRESS 0x0 len:0x0 key:0x0
[2023-07-11 11:28:42.310767] ctrlr.c:1406:nvmf_property_get: *DEBUG*: size 1, offset 0x0
[2023-07-11 11:28:42.310772] ctrlr.c:1429:nvmf_property_get: *DEBUG*: name: cap
[2023-07-11 11:28:42.310776] ctrlr.c:1447:nvmf_property_get: *DEBUG*: response value: 0x201e01007f
[2023-07-11 11:28:42.310781] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:16395 cdw0:1e01007f sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:42.310788] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:42.310792] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:42.310797] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:42.310802] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:42.310811] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:42.310816] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:42.310822] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:42.310827] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:42.310833] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC PROPERTY SET qid:0 cid:16396 SGL KEYED DATA BLOCK ADDRESS 0x0 len:0x0 key:0x0
[2023-07-11 11:28:42.310843] ctrlr.c:1464:nvmf_property_set: *DEBUG*: size 0, offset 0x14, value 0x460001
[2023-07-11 11:28:42.310847] ctrlr.c:1488:nvmf_property_set: *DEBUG*: name: cc
[2023-07-11 11:28:42.310851] ctrlr.c:1153:nvmf_prop_set_cc: *DEBUG*: cur CC: 0x00000000
[2023-07-11 11:28:42.310856] ctrlr.c:1154:nvmf_prop_set_cc: *DEBUG*: new CC: 0x00460001
[2023-07-11 11:28:42.310860] ctrlr.c:1164:nvmf_prop_set_cc: *DEBUG*: Property Set CC Enable!
[2023-07-11 11:28:42.310864] ctrlr.c:1232:nvmf_prop_set_cc: *DEBUG*: Prop Set IOSQES = 6 (64 bytes)
[2023-07-11 11:28:42.310868] ctrlr.c:1239:nvmf_prop_set_cc: *DEBUG*: Prop Set IOCQES = 4 (16 bytes)
[2023-07-11 11:28:42.310874] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:16396 cdw0:0 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:42.310882] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:42.310886] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:42.310891] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:42.310896] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:42.310905] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:42.310911] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:42.310916] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:42.310921] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:42.310927] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC PROPERTY GET qid:0 cid:20489 SGL KEYED DATA BLOCK ADDRESS 0x0 len:0x0 key:0x0
[2023-07-11 11:28:42.310934] ctrlr.c:1406:nvmf_property_get: *DEBUG*: size 0, offset 0x1c
[2023-07-11 11:28:42.310939] ctrlr.c:1429:nvmf_property_get: *DEBUG*: name: csts
[2023-07-11 11:28:42.310943] ctrlr.c:1447:nvmf_property_get: *DEBUG*: response value: 0x1
[2023-07-11 11:28:42.310948] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:20489 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:42.310955] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:42.310960] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:42.310965] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:42.310969] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:42.310978] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:42.310983] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:42.310989] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:42.310994] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:42.311000] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC PROPERTY GET qid:0 cid:20490 SGL KEYED DATA BLOCK ADDRESS 0x0 len:0x0 key:0x0
[2023-07-11 11:28:42.311008] ctrlr.c:1406:nvmf_property_get: *DEBUG*: size 0, offset 0x8
[2023-07-11 11:28:42.311012] ctrlr.c:1429:nvmf_property_get: *DEBUG*: name: vs
[2023-07-11 11:28:42.311016] ctrlr.c:1447:nvmf_property_get: *DEBUG*: response value: 0x10300
[2023-07-11 11:28:42.311021] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:20490 cdw0:10300 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:42.311029] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:42.311033] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:42.311038] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:42.311042] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:42.311052] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:42.311057] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:42.311063] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:42.311068] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:42.311074] rdma.c:1459:nvmf_rdma_fill_wr_sgl: *DEBUG*: sge[0] 0x2000194a0438 addr 0x200007200000, len 4096
[2023-07-11 11:28:42.311079] rdma.c:1851:nvmf_rdma_request_parse_sgl: *DEBUG*: Request 0x2000194a0000 took 1 buffer/s from central pool
[2023-07-11 11:28:42.311084] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:42.311090] nvme_qpair.c: 225:nvme_admin_qpair_print_command: *NOTICE*: IDENTIFY (06) qid:0 cid:20491 nsid:0 cdw10:00000001 cdw11:00000000 SGL KEYED DATA BLOCK INVALIDATE KEY 0x2087786000 len:0x1000 key:0x1fe18f
[2023-07-11 11:28:42.311101] ctrlr.c:3084:nvmf_ctrlr_identify: *DEBUG*: Received identify command with CNS 0x01
[2023-07-11 11:28:42.311109] ctrlr.c:2754:spdk_nvmf_ctrlr_identify_ctrlr: *DEBUG*: ctrlr data: maxcmd 0x80
[2023-07-11 11:28:42.311114] ctrlr.c:2755:spdk_nvmf_ctrlr_identify_ctrlr: *DEBUG*: sgls data: 0x100005
[2023-07-11 11:28:42.311121] ctrlr_bdev.c:  47:nvmf_subsystem_bdev_io_type_supported: *DEBUG*: All devices in Subsystem nqn.2022-06.io.spdk:cnode216 support io_type 3
[2023-07-11 11:28:42.311129] ctrlr_bdev.c:  47:nvmf_subsystem_bdev_io_type_supported: *DEBUG*: All devices in Subsystem nqn.2022-06.io.spdk:cnode216 support io_type 9
[2023-07-11 11:28:42.311134] ctrlr.c:2833:spdk_nvmf_ctrlr_identify_ctrlr: *DEBUG*: ext ctrlr data: ioccsz 0x104
[2023-07-11 11:28:42.311139] ctrlr.c:2835:spdk_nvmf_ctrlr_identify_ctrlr: *DEBUG*: ext ctrlr data: iorcsz 0x1
[2023-07-11 11:28:42.311144] ctrlr.c:2837:spdk_nvmf_ctrlr_identify_ctrlr: *DEBUG*: ext ctrlr data: icdoff 0x0
[2023-07-11 11:28:42.311149] ctrlr.c:2839:spdk_nvmf_ctrlr_identify_ctrlr: *DEBUG*: ext ctrlr data: ctrattr 0x0
[2023-07-11 11:28:42.311154] ctrlr.c:2841:spdk_nvmf_ctrlr_identify_ctrlr: *DEBUG*: ext ctrlr data: msdbd 0x10
[2023-07-11 11:28:42.311159] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:20491 cdw0:0 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:42.311168] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:42.311172] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 8
[2023-07-11 11:28:42.311176] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:42.311181] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 10
[2023-07-11 11:28:42.311186] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 10
[2023-07-11 11:28:42.311197] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:42.311202] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:42.311210] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:42.311215] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:42.311220] rdma.c:1459:nvmf_rdma_fill_wr_sgl: *DEBUG*: sge[0] 0x2000194a0438 addr 0x200007200000, len 8192
[2023-07-11 11:28:42.311225] rdma.c:1459:nvmf_rdma_fill_wr_sgl: *DEBUG*: sge[1] 0x2000194a0448 addr 0x200007202000, len 8192
[2023-07-11 11:28:42.311231] rdma.c:1459:nvmf_rdma_fill_wr_sgl: *DEBUG*: sge[2] 0x2000194a0458 addr 0x200007204000, len 2064
[2023-07-11 11:28:42.311236] rdma.c:1851:nvmf_rdma_request_parse_sgl: *DEBUG*: Request 0x2000194a0000 took 3 buffer/s from central pool
[2023-07-11 11:28:42.311241] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:42.311247] nvme_qpair.c: 225:nvme_admin_qpair_print_command: *NOTICE*: GET LOG PAGE (02) qid:0 cid:20492 nsid:ffffffff cdw10:1203000c cdw11:00000000 SGL KEYED DATA BLOCK INVALIDATE KEY 0x2c5a38000 len:0x4810 key:0x1fe190
[2023-07-11 11:28:42.311259] ctrlr.c:2556:nvmf_ctrlr_get_log_page: *DEBUG*: Get log page: LID=0x0C offset=0x0 len=0x4810 rae=0
[2023-07-11 11:28:42.311269] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:20492 cdw0:0 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:42.311276] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:42.311281] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 8
[2023-07-11 11:28:42.311285] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:42.311290] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 10
[2023-07-11 11:28:42.311294] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 10
[2023-07-11 11:28:42.311311] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:42.311317] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:42.311323] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:42.311328] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:42.311332] rdma.c:1459:nvmf_rdma_fill_wr_sgl: *DEBUG*: sge[0] 0x2000194a0438 addr 0x200007204000, len 4096
[2023-07-11 11:28:42.311337] rdma.c:1851:nvmf_rdma_request_parse_sgl: *DEBUG*: Request 0x2000194a0000 took 1 buffer/s from central pool
[2023-07-11 11:28:42.311341] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:42.311347] nvme_qpair.c: 225:nvme_admin_qpair_print_command: *NOTICE*: IDENTIFY (06) qid:0 cid:24585 nsid:0 cdw10:00000006 cdw11:00000000 SGL KEYED DATA BLOCK INVALIDATE KEY 0x2087786000 len:0x1000 key:0x1fe191
[2023-07-11 11:28:42.311356] ctrlr.c:3084:nvmf_ctrlr_identify: *DEBUG*: Received identify command with CNS 0x06
[2023-07-11 11:28:42.311360] ctrlr.c:2943:spdk_nvmf_ctrlr_identify_iocs_specific: *DEBUG*: Returning zero filled struct for the iocs specific ctrlr identify command and CSI 0x00
[2023-07-11 11:28:42.311367] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:24585 cdw0:0 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:42.311374] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:42.311379] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 8
[2023-07-11 11:28:42.311383] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:42.311387] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 10
[2023-07-11 11:28:42.311392] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 10
[2023-07-11 11:28:42.311404] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:42.311408] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:42.311414] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:42.311419] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:42.311428] nvme_qpair.c: 215:nvme_admin_qpair_print_command: *NOTICE*: SET FEATURES NUMBER OF QUEUES cid:24586 cdw10:00000007 SGL KEYED DATA BLOCK ADDRESS 0x0 len:0x0 key:0x0
[2023-07-11 11:28:42.311437] ctrlr.c:1965:nvmf_ctrlr_set_features_number_of_queues: *DEBUG*: Set Features - Number of Queues, cdw11 0x3e003e
[2023-07-11 11:28:42.311442] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:24586 cdw0:7e007e sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:42.311451] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:42.311455] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:42.311460] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:42.311465] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:42.311474] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:42.311480] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:42.318540] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:42.318626] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:42.368662] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:42.368668] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:42.368673] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:42.368677] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:42.368682] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:42.368686] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:42.368690] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:42.368695] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:42.368699] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:42.368703] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:42.369463] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:42.369479] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x23bf2e0
[2023-07-11 11:28:42.369821] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:42.378682] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:42.378801] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:42.428803] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:42.428810] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:42.428814] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:42.428819] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:42.428823] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:42.428827] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:42.428831] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:42.428836] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:42.428840] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:42.428844] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:42.429644] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:42.429659] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x23bfa90
[2023-07-11 11:28:42.430001] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:42.438824] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:42.438913] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:42.488945] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:42.488951] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:42.488956] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:42.488961] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:42.488965] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:42.488969] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:42.488974] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:42.488978] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:42.488982] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:42.488986] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:42.489632] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:42.489648] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2389b60
[2023-07-11 11:28:42.489989] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:42.498965] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:42.499056] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:42.549087] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:42.549093] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:42.549098] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:42.549102] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:42.549106] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:42.549110] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:42.549114] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:42.549119] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:42.549124] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:42.549128] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:42.549922] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:42.549937] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x238a310
[2023-07-11 11:28:42.550279] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:42.559108] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:42.559199] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:42.609229] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:42.609236] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:42.609240] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:42.609244] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:42.609248] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:42.609252] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:42.609257] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:42.609261] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:42.609265] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:42.609269] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:42.610063] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:42.610078] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x238ac30
[2023-07-11 11:28:42.610427] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:42.619250] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:42.619338] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:42.669372] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:42.669378] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:42.669383] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:42.669387] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:42.669392] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:42.669396] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:42.669400] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:42.669405] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:42.669409] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:42.669414] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:42.670197] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:42.670211] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x23f44d0
[2023-07-11 11:28:42.670564] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:42.679392] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:42.679486] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:42.724196] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 129 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:42.729513] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:42.729518] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:42.729523] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:42.729527] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:42.729531] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:42.729535] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:42.729539] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:42.729543] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:42.729548] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:42.729552] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:42.730301] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:42.730317] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x23fce20
[2023-07-11 11:28:42.730650] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:42.739534] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:42.739625] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:42.789655] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:42.789661] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:42.789666] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:42.789670] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:42.789675] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:42.789679] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:42.789683] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:42.789688] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:42.789691] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:42.789695] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:42.790485] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:42.790499] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24057c0
[2023-07-11 11:28:42.790839] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:42.799676] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:42.799765] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:42.849797] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:42.849804] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:42.849808] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:42.849812] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:42.849817] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:42.849820] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:42.849824] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:42.849828] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:42.849833] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:42.849836] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:42.850623] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:42.850637] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x240e160
[2023-07-11 11:28:42.850979] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:42.859818] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:42.859910] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:42.909940] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:42.909947] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:42.909952] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:42.909956] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:42.909960] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:42.909965] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:42.909969] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:42.909973] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:42.909977] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:42.909981] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:42.910640] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:42.910654] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2416b00
[2023-07-11 11:28:42.910991] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:42.919960] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:42.920049] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:42.970082] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:42.970088] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:42.970092] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:42.970097] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:42.970101] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:42.970105] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:42.970109] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:42.970114] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:42.970118] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:42.970122] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:42.970767] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:42.970780] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x241f400
[2023-07-11 11:28:42.971125] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:42.980103] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:42.980201] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.030224] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.030230] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.030235] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.030239] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.030243] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.030247] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.030251] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.030256] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.030261] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.030265] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.030907] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.030921] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2427b90
[2023-07-11 11:28:43.031272] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.040245] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.040334] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.090366] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.090372] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.090376] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.090380] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.090384] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.090388] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.090392] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.090396] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.090400] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.090404] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.091059] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.091072] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2430490
[2023-07-11 11:28:43.091404] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.100386] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.100480] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.150508] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.150514] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.150518] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.150522] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.150527] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.150531] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.150535] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.150539] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.150543] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.150549] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.151186] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.151202] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2438c20
[2023-07-11 11:28:43.151543] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.160529] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.160621] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.210650] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.210656] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.210660] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.210664] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.210669] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.210673] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.210678] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.210682] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.210686] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.210690] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.211334] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.211347] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2441520
[2023-07-11 11:28:43.211685] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.220671] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.220766] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.270792] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.270798] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.270803] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.270807] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.270811] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.270816] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.270820] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.270824] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.270830] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.270834] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.271509] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.271524] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2449cb0
[2023-07-11 11:28:43.271874] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.280814] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.280909] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.330935] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.330941] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.330945] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.330950] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.330954] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.330958] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.330962] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.330967] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.330971] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.330975] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.331655] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.331669] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24525b0
[2023-07-11 11:28:43.332013] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.340956] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.341060] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.391076] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.391082] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.391087] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.391091] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.391095] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.391099] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.391104] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.391108] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.391112] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.391116] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.391762] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.391775] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x245ad40
[2023-07-11 11:28:43.392113] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.401097] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.401200] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.451219] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.451225] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.451229] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.451233] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.451237] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.451241] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.451245] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.451249] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.451253] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.451257] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.451884] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.451897] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2463640
[2023-07-11 11:28:43.452243] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.461240] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.461340] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.511361] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.511367] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.511372] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.511376] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.511380] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.511384] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.511388] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.511393] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.511397] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.511401] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.512034] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.512048] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x246bdd0
[2023-07-11 11:28:43.512385] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.521382] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.521484] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.571503] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.571509] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.571513] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.571517] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.571521] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.571526] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.571530] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.571534] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.571538] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.571542] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.572216] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.572230] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24746d0
[2023-07-11 11:28:43.572570] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.581524] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.581630] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.631645] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.631650] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.631655] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.631659] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.631664] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.631668] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.631672] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.631676] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.631680] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.631685] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.632352] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.632366] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x247ce60
[2023-07-11 11:28:43.632712] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.641665] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.641764] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.691789] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.691795] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.691800] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.691804] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.691808] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.691812] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.691816] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.691821] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.691825] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.691829] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.692500] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.692515] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2485760
[2023-07-11 11:28:43.692850] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.701808] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.701904] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.726549] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 295 voluntary context switches and 5 involuntary context switches in the last second.
[2023-07-11 11:28:43.751928] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.751934] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.751939] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.751943] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.751947] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.751951] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.751955] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.751959] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.751964] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.751968] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.752625] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.752639] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x248def0
[2023-07-11 11:28:43.752995] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.761950] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.762047] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.812072] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.812077] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.812082] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.812086] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.812090] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.812094] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.812099] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.812103] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.812107] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.812111] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.812773] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.812787] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24967f0
[2023-07-11 11:28:43.813134] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.822092] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.822183] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.826425] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:43.826431] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:43.826439] nvme_qpair.c: 225:nvme_admin_qpair_print_command: *NOTICE*: KEEP ALIVE (18) qid:0 cid:49152 nsid:0 cdw10:00000000 cdw11:00000000 
[2023-07-11 11:28:43.826449] ctrlr.c:3506:nvmf_ctrlr_keep_alive: *DEBUG*: Keep Alive
[2023-07-11 11:28:43.826455] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:49152 cdw0:0 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:43.826463] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:43.826468] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:43.826473] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:43.826478] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:43.826489] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:43.826495] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:43.872214] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.872220] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.872224] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.872228] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.872233] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.872237] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.872241] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.872245] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.872249] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.872254] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.872887] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.872901] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x249ef80
[2023-07-11 11:28:43.873270] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.882234] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.882328] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.932356] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.932362] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.932366] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.932371] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.932374] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.932379] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.932383] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.932387] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.932391] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.932395] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.933048] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.933063] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24a7880
[2023-07-11 11:28:43.933414] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:43.942376] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:43.942471] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:43.992497] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:43.992503] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:43.992508] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:43.992512] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:43.992516] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:43.992520] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:43.992525] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:43.992529] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:43.992533] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:43.992537] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:43.993202] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:43.993215] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24b0010
[2023-07-11 11:28:43.993556] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.002518] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.002607] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.052640] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.052646] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.052650] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.052655] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.052659] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.052663] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.052667] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.052671] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.052676] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.052680] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.053327] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.053343] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24b8910
[2023-07-11 11:28:44.053691] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.062660] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.062751] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.112781] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.112788] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.112792] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.112797] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.112801] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.112805] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.112810] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.112814] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.112818] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.112822] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.113468] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.113482] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24c10a0
[2023-07-11 11:28:44.113838] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.122802] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.122891] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.172923] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.172929] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.172934] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.172938] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.172942] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.172946] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.172950] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.172954] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.172958] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.172963] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.173574] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.173589] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24c99a0
[2023-07-11 11:28:44.173925] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.182944] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.183045] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.233065] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.233071] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.233076] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.233080] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.233084] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.233088] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.233093] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.233097] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.233101] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.233105] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.233754] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.233768] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24d2130
[2023-07-11 11:28:44.234118] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.243086] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.243173] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.293208] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.293214] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.293218] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.293222] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.293226] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.293231] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.293235] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.293239] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.293243] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.293247] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.293904] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.293919] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24daa30
[2023-07-11 11:28:44.294259] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.303228] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.303315] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.353349] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.353355] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.353360] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.353364] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.353368] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.353372] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.353376] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.353380] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.353385] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.353389] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.354046] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.354059] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24e31c0
[2023-07-11 11:28:44.354399] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.363371] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.363455] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.413491] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.413497] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.413501] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.413505] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.413509] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.413514] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.413518] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.413522] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.413528] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.413532] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.414175] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.414189] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24ebac0
[2023-07-11 11:28:44.414542] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.423512] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.423598] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.473634] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.473639] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.473644] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.473648] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.473652] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.473656] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.473661] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.473665] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.473669] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.473673] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.474315] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.474329] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24f4250
[2023-07-11 11:28:44.474673] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.483655] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.483746] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.533776] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.533782] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.533787] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.533791] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.533795] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.533799] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.533803] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.533808] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.533812] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.533816] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.534451] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.534465] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x24fcb50
[2023-07-11 11:28:44.534809] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.543797] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.543889] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.593918] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.593924] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.593929] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.593933] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.593937] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.593941] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.593945] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.593950] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.593954] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.593958] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.594601] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.594614] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x25052e0
[2023-07-11 11:28:44.594948] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.603939] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.604032] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.654061] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.654066] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.654071] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.654075] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.654079] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.654082] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.654086] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.654091] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.654095] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.654099] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.654741] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.654755] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x250dbe0
[2023-07-11 11:28:44.655089] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.664081] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.664186] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.714203] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.714209] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.714213] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.714217] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.714223] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.714227] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.714231] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.714235] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.714240] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.714243] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.714870] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.714884] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2516370
[2023-07-11 11:28:44.715232] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.724223] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.724328] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.728902] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 289 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:44.774345] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.774350] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.774355] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.774359] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.774363] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.774367] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.774371] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.774375] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.774379] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.774384] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.775014] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.775027] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x251ec70
[2023-07-11 11:28:44.775367] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.784366] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.784470] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.834487] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.834492] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.834497] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.834501] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.834505] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.834510] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.834514] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.834518] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.834522] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.834526] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.835167] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.835181] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2527400
[2023-07-11 11:28:44.835509] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.844508] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.844605] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.894629] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.894635] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.894640] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.894644] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.894648] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.894652] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.894657] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.894661] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.894666] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.894670] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.895301] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.895314] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x252fd00
[2023-07-11 11:28:44.895648] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.904650] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.904743] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:44.954772] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:44.954779] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:44.954783] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:44.954787] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:44.954792] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:44.954795] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:44.954800] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:44.954804] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:44.954808] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:44.954812] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:44.955490] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:44.955504] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2538490
[2023-07-11 11:28:44.955849] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:44.964792] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:44.964897] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.014914] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.014920] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.014924] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.014928] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.014932] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.014937] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.014941] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.014946] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.014949] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.014954] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.015581] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.015594] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2540d90
[2023-07-11 11:28:45.015943] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.024934] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.025038] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.075056] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.075062] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.075067] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.075071] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.075075] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.075079] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.075084] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.075088] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.075092] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.075096] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.075771] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.075784] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2549520
[2023-07-11 11:28:45.076132] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.085077] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.085186] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.135198] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.135204] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.135209] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.135213] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.135217] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.135221] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.135225] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.135229] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.135233] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.135238] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.135878] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.135891] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2551e20
[2023-07-11 11:28:45.136219] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.145219] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.145329] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.195340] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.195346] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.195351] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.195355] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.195359] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.195363] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.195367] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.195372] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.195376] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.195380] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.196026] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.196039] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x255a5b0
[2023-07-11 11:28:45.196414] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.205361] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.205461] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.255482] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.255489] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.255493] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.255497] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.255501] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.255506] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.255510] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.255514] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.255520] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.255524] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.256240] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.256254] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2562eb0
[2023-07-11 11:28:45.256599] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.265503] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.265603] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.315625] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.315631] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.315635] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.315640] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.315644] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.315648] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.315652] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.315656] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.315661] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.315665] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.316328] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.316341] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x256b640
[2023-07-11 11:28:45.316686] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.325645] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.325744] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.362440] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:45.362446] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:45.362455] nvme_qpair.c: 225:nvme_admin_qpair_print_command: *NOTICE*: KEEP ALIVE (18) qid:0 cid:53248 nsid:0 cdw10:00000000 cdw11:00000000 
[2023-07-11 11:28:45.362464] ctrlr.c:3506:nvmf_ctrlr_keep_alive: *DEBUG*: Keep Alive
[2023-07-11 11:28:45.362470] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:53248 cdw0:0 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:45.362478] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:45.362482] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:45.362487] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:45.362491] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:45.362502] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:45.362507] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:45.375767] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.375773] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.375778] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.375782] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.375786] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.375790] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.375794] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.375799] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.375803] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.375807] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.376438] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.376453] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2573f40
[2023-07-11 11:28:45.376789] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.385787] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.385886] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.435909] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.435915] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.435919] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.435923] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.435927] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.435931] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.435935] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.435940] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.435943] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.435948] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.436603] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.436617] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x257c6d0
[2023-07-11 11:28:45.436967] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.445929] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.446019] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.496050] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.496056] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.496060] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.496064] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.496068] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.496072] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.496076] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.496080] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.496084] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.496089] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.496742] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.496755] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2584fd0
[2023-07-11 11:28:45.497108] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.506071] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.506163] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.556193] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.556199] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.556203] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.556208] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.556212] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.556216] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.556220] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.556225] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.556230] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.556233] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.556886] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.556900] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x258d760
[2023-07-11 11:28:45.557249] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.566213] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.566308] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.616336] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.616342] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.616347] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.616351] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.616355] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.616359] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.616364] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.616368] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.616372] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.616377] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.617027] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.617041] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x2596060
[2023-07-11 11:28:45.617389] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.626356] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.626447] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.676477] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.676483] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.676488] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.676492] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.676496] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.676500] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.676504] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.676509] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.676513] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.676516] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.677168] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.677182] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x259e7f0
[2023-07-11 11:28:45.677526] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.686498] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.686593] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.731255] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 270 voluntary context switches and 6 involuntary context switches in the last second.
[2023-07-11 11:28:45.736619] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.736624] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.736628] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.736632] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.736637] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.736640] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.736644] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.736649] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.736653] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.736656] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.737276] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.737289] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x25a70f0
[2023-07-11 11:28:45.737630] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.746640] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.746729] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.796762] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.796768] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.796772] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.796776] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.796780] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.796783] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.796788] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.796792] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.796796] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.796800] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.797447] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.797463] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x25af880
[2023-07-11 11:28:45.797810] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.806782] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.806870] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.856904] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.856910] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.856914] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.856918] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.856923] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.856927] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.856931] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.856935] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.856939] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.856943] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.857583] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.857597] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x25b8180
[2023-07-11 11:28:45.857945] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.866925] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.867014] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.917045] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.917051] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.917055] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.917059] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.917063] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.917067] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.917071] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.917076] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.917080] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.917084] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.917734] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.917753] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x25c0910
[2023-07-11 11:28:45.918096] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.927066] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.927156] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:45.977188] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:45.977194] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:45.977198] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:45.977202] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:45.977206] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:45.977210] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:45.977215] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:45.977219] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:45.977223] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:45.977227] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:45.977875] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:45.977888] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x25c9210
[2023-07-11 11:28:45.978235] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:45.987208] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:45.987304] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:46.037330] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:46.037335] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:46.037340] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:46.037344] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:46.037349] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:46.037353] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:46.037357] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:46.037361] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:46.037366] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:46.037370] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:46.038014] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:46.038028] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x25d19a0
[2023-07-11 11:28:46.038371] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:46.047350] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:46.047440] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:46.097471] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_CONNECT_REQUEST
[2023-07-11 11:28:46.097477] rdma.c:1277:nvmf_rdma_connect: *DEBUG*: Connect Recv on fabric intf name mlx5_0, dev_name uverbs0
[2023-07-11 11:28:46.097482] rdma.c:1281:nvmf_rdma_connect: *DEBUG*: Listen Id was 0x23aa270 with verbs 0x7fadd822d150. ListenAddr: 0x2316d90
[2023-07-11 11:28:46.097486] rdma.c:1287:nvmf_rdma_connect: *DEBUG*: Calculating Queue Depth
[2023-07-11 11:28:46.097490] rdma.c:1293:nvmf_rdma_connect: *DEBUG*: Target Max Queue Depth: 128
[2023-07-11 11:28:46.097495] rdma.c:1298:nvmf_rdma_connect: *DEBUG*: Local NIC Max Send/Recv Queue Depth: 32768 Max Read/Write Queue Depth: 16
[2023-07-11 11:28:46.097499] rdma.c:1305:nvmf_rdma_connect: *DEBUG*: Host (Initiator) NIC Max Incoming RDMA R/W operations: 16 Max Outgoing RDMA R/W operations: 0
[2023-07-11 11:28:46.097503] rdma.c:1314:nvmf_rdma_connect: *DEBUG*: Host Receive Queue Size: 128
[2023-07-11 11:28:46.097507] rdma.c:1315:nvmf_rdma_connect: *DEBUG*: Host Send Queue Size: 127
[2023-07-11 11:28:46.097511] rdma.c:1321:nvmf_rdma_connect: *DEBUG*: Final Negotiated Queue Depth: 128 R/W Depth: 16
[2023-07-11 11:28:46.098152] rdma.c:1033:nvmf_rdma_qpair_initialize: *ERROR*: SSSS qp timeout:0, retry_cnt:0, rnr_retry:0
[2023-07-11 11:28:46.098171] rdma.c:1042:nvmf_rdma_qpair_initialize: *DEBUG*: New RDMA Connection: 0x25db3e0
[2023-07-11 11:28:46.098510] rdma.c:1228:nvmf_rdma_event_accept: *DEBUG*: Sent back the accept
[2023-07-11 11:28:46.098936] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.098942] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.098949] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.098953] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.098962] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:1 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.098976] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 1 sqsize 127
[2023-07-11 11:28:46.098980] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.098985] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.098989] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.098994] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.098998] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.099004] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.099010] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.099017] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.099023] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:1 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.099031] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.099036] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.099040] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.099053] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.099058] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.099074] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.099079] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.099083] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.099088] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.099095] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:2 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.099105] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 2 sqsize 127
[2023-07-11 11:28:46.099110] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.099114] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.099119] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.099124] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.099128] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.099134] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.099139] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.099145] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.099151] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:2 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.099158] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.099163] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.099167] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.099178] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.099182] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.099201] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.099205] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.099210] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.099214] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.099220] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:3 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.099228] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 3 sqsize 127
[2023-07-11 11:28:46.099232] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.099236] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.099240] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.099244] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.099248] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.099253] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.099258] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.099263] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.099269] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:3 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.099277] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.099281] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.099286] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.099297] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.099302] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.099324] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.099328] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.099334] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.099339] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.099345] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:4 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.099353] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 4 sqsize 127
[2023-07-11 11:28:46.099358] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.099362] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.099368] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.099372] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.099377] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.099381] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.099386] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.099392] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.099397] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:4 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.099404] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.099409] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.099414] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.099425] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.099429] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.099453] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.099458] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.099462] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.099466] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.099472] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:5 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.099480] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 5 sqsize 127
[2023-07-11 11:28:46.099484] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.099488] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.099493] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.099497] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.099501] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.099506] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.099510] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.099516] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.099521] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:5 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.099528] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.099532] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.099537] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.099548] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.099552] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.099570] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.099576] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.099580] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.099584] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.099589] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:6 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.099599] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 6 sqsize 127
[2023-07-11 11:28:46.099603] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.099607] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.099612] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.099616] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.099621] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.099626] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.099632] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.099637] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.099643] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:6 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.099652] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.099657] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.099661] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.099672] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.099677] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.099695] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.099700] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.099705] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.099710] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.099716] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:7 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.099724] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 7 sqsize 127
[2023-07-11 11:28:46.099728] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.099732] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.099736] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.099740] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.099744] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.099749] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.099754] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.099759] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.099764] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:7 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.099771] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.099776] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.099780] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.099791] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.099796] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.099817] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.099821] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.099826] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.099830] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.099836] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:8 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.099845] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 8 sqsize 127
[2023-07-11 11:28:46.099849] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.099854] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.099858] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.099863] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.099867] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.099872] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.099877] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.099883] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.099888] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:8 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.099896] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.099901] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.099906] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.099918] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.099923] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.099940] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.099944] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.099948] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.099952] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.099958] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:9 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.099967] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 9 sqsize 127
[2023-07-11 11:28:46.099972] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.099976] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.099981] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.099986] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.099991] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.099997] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.100002] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.100008] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.100013] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:9 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.100020] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.100025] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.100029] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.100040] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.100045] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.100066] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.100071] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.100076] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.100080] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.100086] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:10 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.100095] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 10 sqsize 127
[2023-07-11 11:28:46.100099] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.100103] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.100108] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.100112] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.100116] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.100121] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.100126] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.100132] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.100139] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:10 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.100148] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.100153] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.100158] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.100169] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.100174] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.100192] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.100197] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.100201] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.100206] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.100211] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:11 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.100221] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 11 sqsize 127
[2023-07-11 11:28:46.100226] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.100230] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.100235] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.100240] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.100244] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.100248] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.100253] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.100258] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.100263] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:11 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.100271] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.100275] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.100280] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.100291] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.100296] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.100317] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.100322] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.100326] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.100330] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.100335] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:12 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.100344] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 12 sqsize 127
[2023-07-11 11:28:46.100348] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.100352] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.100357] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.100361] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.100366] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.100371] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.100376] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.100382] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.100388] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:12 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.100396] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.100400] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.100405] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.100415] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.100419] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.100441] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.100446] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.100450] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.100455] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.100461] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:13 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.100471] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 13 sqsize 127
[2023-07-11 11:28:46.100475] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.100480] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.100484] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.100488] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.100492] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.100497] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.100501] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.100507] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.100512] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:13 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.100520] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.100524] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.100529] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.100540] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.100545] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.100562] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.100566] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.100570] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.100574] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.100579] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:14 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.100588] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 14 sqsize 127
[2023-07-11 11:28:46.100592] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.100596] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.100600] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.100605] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.100609] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.100614] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.100620] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.100626] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.100632] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:14 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.100640] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.100644] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.100649] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.100659] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.100663] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.100679] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.100684] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.100689] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.100693] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.100699] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:15 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.100709] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 15 sqsize 127
[2023-07-11 11:28:46.100713] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.100717] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.100721] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.100725] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.100729] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.100734] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.100738] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.100744] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.100749] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:15 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.100757] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.100762] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.100767] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.100778] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.100783] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.100806] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.100811] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.100815] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.100819] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.100824] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:16 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.100834] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 16 sqsize 127
[2023-07-11 11:28:46.100838] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.100843] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.100848] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.100853] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.100858] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.100863] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.100868] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.100874] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.100879] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:16 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.100885] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.100889] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.100894] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.100904] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.100908] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.100936] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.100942] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.100946] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.100951] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.100956] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:17 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.100965] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 17 sqsize 127
[2023-07-11 11:28:46.100968] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.100972] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.100976] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.100980] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.100985] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.100990] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.100995] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.101001] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.101006] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:17 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.101015] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.101020] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.101024] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.101035] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.101039] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.101081] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.101086] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.101091] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.101095] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.101101] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:18 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.101111] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 18 sqsize 127
[2023-07-11 11:28:46.101114] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.101118] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.101122] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.101126] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.101130] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.101135] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.101140] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.101145] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.101151] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:18 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.101159] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.101164] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.121635] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.121646] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_ESTABLISHED
[2023-07-11 11:28:46.121738] rdma.c:3543:nvmf_process_cm_event: *ERROR*: SSSS qp timeout:19, retry_cnt:7, rnr_retry:7
[2023-07-11 11:28:46.121749] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.121754] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.121758] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.121763] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.121768] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.121772] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.121779] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:19 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.121788] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 19 sqsize 127
[2023-07-11 11:28:46.121793] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.121798] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.121803] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.121808] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.121812] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.121817] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.121822] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.121827] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.121833] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:19 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.121840] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.121845] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.121849] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.121861] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.121866] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.121889] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.121894] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.121898] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.121902] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.121908] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:20 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.121917] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 20 sqsize 127
[2023-07-11 11:28:46.121921] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.121926] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.121930] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.121935] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.121940] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.121945] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.121951] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.121957] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.121962] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:20 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.121970] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.121975] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.121979] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.121990] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.121994] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.122018] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.122023] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.122028] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.122033] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.122039] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:21 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.122049] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 21 sqsize 127
[2023-07-11 11:28:46.122053] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.122057] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.122061] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.122065] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.122069] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.122074] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.122080] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.122085] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.122091] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:21 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.122100] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.122104] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.122109] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.122120] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.122125] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.122148] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.122153] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.122158] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.122162] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.122168] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:22 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.122178] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 22 sqsize 127
[2023-07-11 11:28:46.122183] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.122187] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.122192] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.122197] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.122201] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.122206] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.122211] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.122216] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.122221] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:22 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.122229] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.122234] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.122239] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.122250] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.122255] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.122279] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.122284] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.122288] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.122292] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.122297] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:23 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.122307] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 23 sqsize 127
[2023-07-11 11:28:46.122311] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.122316] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.122320] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.122325] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.122330] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.122335] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.122341] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.122347] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.122353] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:23 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.122361] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.122365] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.122369] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.122380] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.122384] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.122405] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.122411] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.122416] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.122421] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.122430] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:24 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.122440] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 24 sqsize 127
[2023-07-11 11:28:46.122444] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.122448] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.122452] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.122457] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.122461] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.122466] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.122471] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.122477] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.122482] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:24 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.122490] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.122495] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.122500] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.122512] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.122517] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.122540] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.122544] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.122548] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.122553] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.122559] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:25 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.122569] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 25 sqsize 127
[2023-07-11 11:28:46.122573] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.122578] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.122583] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.122588] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.122592] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.122597] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.122602] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.122608] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.122613] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:25 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.122620] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.122624] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.122629] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.122640] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.122645] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.122669] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.122674] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.122678] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.122683] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.122688] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:26 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.122697] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 26 sqsize 127
[2023-07-11 11:28:46.122701] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.122705] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.122710] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.122715] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.122719] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.122725] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.122730] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.122737] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.122742] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:26 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.122750] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.122755] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.122759] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.122770] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.122774] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.122799] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.122804] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.122809] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.122815] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.122821] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:27 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.122830] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 27 sqsize 127
[2023-07-11 11:28:46.122834] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.122838] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.122843] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.122847] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.122851] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.122855] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.122860] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.122866] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.122872] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:27 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.122880] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.122885] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.122890] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.122902] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.122907] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.122928] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.122932] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.122936] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.122941] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.122946] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:28 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.122955] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 28 sqsize 127
[2023-07-11 11:28:46.122960] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.122966] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.122972] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.122976] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.122981] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.122986] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.122991] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.122997] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.123002] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:28 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.123009] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.123013] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.123017] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.123028] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.123033] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.123054] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.123060] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.123064] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.123067] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.123072] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:29 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.123081] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 29 sqsize 127
[2023-07-11 11:28:46.123085] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.123089] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.123094] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.123099] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.123103] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.123108] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.123114] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.123120] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.123126] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:29 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.123134] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.123139] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.123144] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.123154] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.123159] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.123181] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.123186] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.123191] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.123196] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.123202] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:30 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.123211] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 30 sqsize 127
[2023-07-11 11:28:46.123216] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.123220] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.123224] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.123229] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.123233] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.123238] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.123243] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.123248] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.123254] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:30 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.123262] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.123267] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.123272] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.123283] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.123288] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.123312] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.123317] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.123321] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.123325] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.123331] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:31 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.123340] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 31 sqsize 127
[2023-07-11 11:28:46.123344] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.123349] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.123354] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.123359] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.123364] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.123369] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.123374] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.123380] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.123385] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:31 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.123392] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.123396] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.123400] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.123411] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.123415] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.123439] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.123445] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.123449] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.123454] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.123460] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:32 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.123468] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 32 sqsize 127
[2023-07-11 11:28:46.123472] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.123476] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.123481] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.123486] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.123491] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.123495] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.123501] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.123507] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.123513] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:32 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.123521] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.123526] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.123532] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.123542] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.123546] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.123567] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.123572] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.123576] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.123581] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.123587] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:33 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.123597] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 33 sqsize 127
[2023-07-11 11:28:46.123602] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.123606] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.123611] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.123616] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.123620] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.123625] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.123630] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.123635] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.123640] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:33 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.123648] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.123652] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.123657] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.123668] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.123673] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.123690] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.123695] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.123699] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.123703] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.123708] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:34 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.123717] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 34 sqsize 127
[2023-07-11 11:28:46.123721] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.123725] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.123730] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.123735] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.123740] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.123745] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.123750] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.123756] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.123762] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:34 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.123770] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.123775] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.123779] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.123789] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.123793] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.123813] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.123818] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.123823] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.123829] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.123835] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:35 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.123844] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 35 sqsize 127
[2023-07-11 11:28:46.123848] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.123852] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.123856] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.123860] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.123865] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.123870] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.123875] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.123881] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.123886] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:35 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.123894] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.123899] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.123905] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.123916] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.123921] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.123940] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.123944] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.123948] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.123952] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.123958] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:36 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.123967] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 36 sqsize 127
[2023-07-11 11:28:46.123972] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.123977] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.123982] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.123987] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.123991] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.123996] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.124002] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.124007] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.124012] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:36 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.124019] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.124024] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.124028] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.124038] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.124043] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.124060] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.124065] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.124070] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.124075] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.124081] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:37 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.124090] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 37 sqsize 127
[2023-07-11 11:28:46.124094] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.124098] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.124102] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.124106] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.124111] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.124116] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.124121] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.124127] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.124133] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:37 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.124142] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.124147] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.124151] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.124162] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.124166] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.124186] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.124190] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.124195] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.124199] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.124205] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:38 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.124215] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 38 sqsize 127
[2023-07-11 11:28:46.124219] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.124224] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.124228] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.124234] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.124238] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.124242] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.124247] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.124253] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.124258] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:38 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.124266] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.124271] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.124275] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.124286] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.124291] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.124309] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.124314] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.124318] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.124323] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.124328] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:39 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.124336] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 39 sqsize 127
[2023-07-11 11:28:46.124340] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.124344] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.124349] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.124354] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.124358] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.124363] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.124369] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.124375] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.124380] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:39 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.124388] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.124392] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.124397] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.124407] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.124411] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.124428] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.124433] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.124438] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.124443] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.124449] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:40 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.124462] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 40 sqsize 127
[2023-07-11 11:28:46.124467] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.124471] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.124475] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.124480] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.124484] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.124488] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.124493] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.124498] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.124504] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:40 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.124512] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.124517] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.124522] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.124533] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.124539] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.124554] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.124558] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.124562] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.124567] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.124572] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:41 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.124581] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 41 sqsize 127
[2023-07-11 11:28:46.124585] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.124590] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.124594] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.124600] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.124605] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.124610] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.124616] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.124622] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.124627] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:41 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.124634] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.124638] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.124643] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.124654] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.124658] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.124681] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.124687] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.124691] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.124696] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.124702] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:42 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.124711] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 42 sqsize 127
[2023-07-11 11:28:46.124715] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.124719] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.124723] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.124727] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.124732] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.124737] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.124742] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.124748] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.124753] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:42 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.124762] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.124767] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.124772] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.124783] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.124787] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.124812] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.124817] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.124821] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.124826] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.124832] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:43 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.124841] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 43 sqsize 127
[2023-07-11 11:28:46.124846] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.124850] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.124855] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.124860] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.124864] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.124869] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.124874] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.124879] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.124885] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:43 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.124892] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.124897] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.124902] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.124913] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.124918] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.124934] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.124939] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.124943] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.124948] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.124953] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:44 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.124961] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 44 sqsize 127
[2023-07-11 11:28:46.124965] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.124969] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.124974] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.124979] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.124983] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.124988] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.124993] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.124999] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.125006] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:44 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.125014] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.125019] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.125023] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.125034] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.125038] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.125054] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.125059] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.125064] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.125069] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.125075] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:45 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.125085] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 45 sqsize 127
[2023-07-11 11:28:46.125090] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.125094] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.125099] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.125103] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.125107] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.125112] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.125117] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.125123] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.125128] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:45 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.125136] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.125141] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.125146] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.125157] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.125162] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.125185] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.125190] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.125194] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.125198] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.125203] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:46 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.125212] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 46 sqsize 127
[2023-07-11 11:28:46.125217] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.125221] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.125227] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.125232] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.125237] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.125242] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.125247] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.125253] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.125259] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:46 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.125266] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.125270] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.125274] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.125285] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.125289] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.125311] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.125316] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.125321] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.125326] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.125331] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:47 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.125340] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 47 sqsize 127
[2023-07-11 11:28:46.125344] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.125348] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.125353] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.125357] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.125361] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.125366] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.125371] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.125377] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.125383] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:47 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.125392] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.125396] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.125402] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.125413] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.125418] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.125433] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.125438] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.125442] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.125446] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.125452] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:48 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.125461] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 48 sqsize 127
[2023-07-11 11:28:46.125466] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.125470] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.125475] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.125480] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.125485] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.125490] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.125495] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.125501] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.125506] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:48 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.125513] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.125518] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.125522] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.125533] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.125538] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.125563] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.125569] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.125574] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.125578] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.125583] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:49 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.125592] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 49 sqsize 127
[2023-07-11 11:28:46.125596] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.125600] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.125604] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.125610] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.125614] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.125619] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.125625] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.125631] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.125637] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:49 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.125645] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.125650] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.125655] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.125665] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.125670] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.125699] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.125705] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.125710] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.125715] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.125721] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:50 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.125731] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 50 sqsize 127
[2023-07-11 11:28:46.125735] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.125739] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.125744] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.125748] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.125752] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.125757] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.125762] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.125767] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.125773] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:50 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.125781] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.125786] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.125791] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.125803] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.125808] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.125837] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.125842] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.125846] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.125851] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.125856] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:51 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.125866] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 51 sqsize 127
[2023-07-11 11:28:46.125870] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.125875] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.125880] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.125885] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.125889] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.125895] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.125900] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.125905] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.125910] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:51 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.125917] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.125921] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.125926] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.125937] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.125943] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.125973] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.125979] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.125983] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.125987] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.125992] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:52 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.126001] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 52 sqsize 127
[2023-07-11 11:28:46.126005] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.126010] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.126014] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.126019] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.126024] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.126030] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.126035] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.126041] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.126047] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:52 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.126055] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.126059] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.126063] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.126074] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.126078] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.126102] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.126107] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.126112] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.126117] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.126123] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:53 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.126132] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 53 sqsize 127
[2023-07-11 11:28:46.126136] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.126140] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.126144] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.126148] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.126152] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.126157] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.126162] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.126168] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.126173] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:53 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.126181] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.126186] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.126192] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.126203] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.126208] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.126228] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.126232] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.126237] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.126241] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.126247] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:54 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.126256] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 54 sqsize 127
[2023-07-11 11:28:46.126261] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.126265] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.126270] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.126275] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.126280] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.126285] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.126290] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.126296] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.126301] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:54 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.126308] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.126312] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.126317] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.126330] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.126335] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.126358] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.126363] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.126368] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.126372] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.126378] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:55 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.126386] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 55 sqsize 127
[2023-07-11 11:28:46.126390] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.126394] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.126399] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.126404] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.126408] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.126413] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.126418] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.126427] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.126433] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:55 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.126441] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.126446] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.126451] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.126462] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.126466] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.126487] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.126492] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.126497] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.126501] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.126507] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:56 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.126517] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 56 sqsize 127
[2023-07-11 11:28:46.126522] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.126526] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.126531] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.126535] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.126539] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.126544] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.126549] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.126554] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.126559] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:56 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.126568] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.126572] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.126577] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.126588] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.126594] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.126613] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.126618] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.126622] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.126626] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.126631] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:57 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.126640] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 57 sqsize 127
[2023-07-11 11:28:46.126645] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.126649] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.126653] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.126659] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.126664] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.126669] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.126675] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.126681] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.126686] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:57 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.126694] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.126698] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.126702] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.126713] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.126718] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.126739] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.126744] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.126749] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.126754] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.126760] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:58 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.126770] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 58 sqsize 127
[2023-07-11 11:28:46.126774] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.126778] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.126783] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.126787] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.126791] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.126795] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.126800] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.126806] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.126812] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:58 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.126820] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.126825] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.126830] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.126841] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.126846] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.126863] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.126868] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.126872] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.126876] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.126882] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:59 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.126891] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 59 sqsize 127
[2023-07-11 11:28:46.126896] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.126901] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.126906] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.126911] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.126915] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.126920] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.126925] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.126931] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.126936] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:59 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.126943] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.126947] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.126951] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.126963] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.126968] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.126991] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.126997] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.127001] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.127005] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.127011] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:60 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.127019] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 60 sqsize 127
[2023-07-11 11:28:46.127023] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.127027] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.127032] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.127036] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.127041] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.127045] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.127051] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.127057] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.127063] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:60 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.127071] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.127076] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.127081] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.127092] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.127096] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.127118] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.127123] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.127128] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.127133] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.127139] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:61 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.127148] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 61 sqsize 127
[2023-07-11 11:28:46.127153] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.127158] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.127162] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.127166] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.127170] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.127175] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.127180] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.127185] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.127190] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:61 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.127199] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.127203] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.127208] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.127219] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.127224] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.127242] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.127247] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.127251] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.127255] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.127261] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:62 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.127270] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 62 sqsize 127
[2023-07-11 11:28:46.127275] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.127278] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.127283] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.127289] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.127294] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.127299] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.127305] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.127311] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.127317] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:62 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.127324] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.127328] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.127332] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.127343] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.127347] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.127367] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.127372] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 2
[2023-07-11 11:28:46.127378] rdma.c:1860:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400
[2023-07-11 11:28:46.127383] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.127388] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:63 cid:8192 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 11:28:46.127397] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 63 sqsize 127
[2023-07-11 11:28:46.127402] ctrlr.c: 747:_nvmf_ctrlr_connect: *DEBUG*: Connect data:
[2023-07-11 11:28:46.127406] ctrlr.c: 748:_nvmf_ctrlr_connect: *DEBUG*:   cntlid:  0x0001
[2023-07-11 11:28:46.127410] ctrlr.c: 756:_nvmf_ctrlr_connect: *DEBUG*:   hostid: 7687b1ab-93f5-4362-85f8-1971ab0c8a68 ***
[2023-07-11 11:28:46.127414] ctrlr.c: 757:_nvmf_ctrlr_connect: *DEBUG*:   subnqn: "nqn.2022-06.io.spdk:cnode216"
[2023-07-11 11:28:46.127419] ctrlr.c: 758:_nvmf_ctrlr_connect: *DEBUG*:   hostnqn: "nqn.2014-08.org.nvmexpress:uuid:62b6c286-c57a-41cc-9174-4fce499c3f19"
[2023-07-11 11:28:46.127427] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.127432] ctrlr.c: 653:_nvmf_ctrlr_add_io_qpair: *DEBUG*: Connect I/O Queue for controller id 0x1
[2023-07-11 11:28:46.127437] ctrlr.c: 255:ctrlr_add_qpair_and_send_rsp: *DEBUG*: connect capsule response: cntlid = 0x0001
[2023-07-11 11:28:46.127443] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:63 cid:8192 cdw0:1 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.127451] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.127456] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.127462] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.127473] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.127478] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.131136] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.131142] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.131148] nvme_qpair.c: 215:nvme_admin_qpair_print_command: *NOTICE*: SET FEATURES ASYNC EVENT CONFIGURATION cid:24587 cdw10:0000000b SGL KEYED DATA BLOCK ADDRESS 0x0 len:0x0 key:0x0
[2023-07-11 11:28:46.131159] ctrlr.c:2111:nvmf_ctrlr_set_features_async_event_configuration: *DEBUG*: Set Features - Async Event Configuration, cdw11 0x00000900
[2023-07-11 11:28:46.131165] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:24587 cdw0:0 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.131173] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 7
[2023-07-11 11:28:46.131177] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 9
[2023-07-11 11:28:46.131181] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.131186] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 11
[2023-07-11 11:28:46.131196] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 12
[2023-07-11 11:28:46.131201] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 0
[2023-07-11 11:28:46.132457] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 1
[2023-07-11 11:28:46.132463] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 5
[2023-07-11 11:28:46.132469] nvme_qpair.c: 225:nvme_admin_qpair_print_command: *NOTICE*: ASYNC EVENT REQUEST (0c) qid:0 cid:31 nsid:0 cdw10:00000000 cdw11:00000000 
[2023-07-11 11:28:46.132479] ctrlr.c:2125:nvmf_ctrlr_async_event_request: *DEBUG*: Async Event Request
[2023-07-11 11:28:46.132483] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0000 entering state 6
[2023-07-11 11:28:46.132488] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 1
[2023-07-11 11:28:46.132492] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 2
[2023-07-11 11:28:46.132498] rdma.c:1459:nvmf_rdma_fill_wr_sgl: *DEBUG*: sge[0] 0x2000194a0998 addr 0x200007204000, len 4096
[2023-07-11 11:28:46.132503] rdma.c:1851:nvmf_rdma_request_parse_sgl: *DEBUG*: Request 0x2000194a0560 took 1 buffer/s from central pool
[2023-07-11 11:28:46.132507] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 5
[2023-07-11 11:28:46.132514] nvme_qpair.c: 225:nvme_admin_qpair_print_command: *NOTICE*: IDENTIFY (06) qid:0 cid:22 nsid:0 cdw10:00000002 cdw11:00000000 SGL KEYED DATA BLOCK INVALIDATE KEY 0x10a31d000 len:0x1000 key:0x1fe192
[2023-07-11 11:28:46.132523] ctrlr.c:3084:nvmf_ctrlr_identify: *DEBUG*: Received identify command with CNS 0x02
[2023-07-11 11:28:46.132531] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:22 cdw0:0 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.132539] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 7
[2023-07-11 11:28:46.132543] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 8
[2023-07-11 11:28:46.132548] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 9
[2023-07-11 11:28:46.132553] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 10
[2023-07-11 11:28:46.132558] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 10
[2023-07-11 11:28:46.132571] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 12
[2023-07-11 11:28:46.132576] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 0
[2023-07-11 11:28:46.733608] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 120 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:46.898454] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 1
[2023-07-11 11:28:46.898460] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 5
[2023-07-11 11:28:46.898466] nvme_qpair.c: 225:nvme_admin_qpair_print_command: *NOTICE*: KEEP ALIVE (18) qid:0 cid:57344 nsid:0 cdw10:00000000 cdw11:00000000 
[2023-07-11 11:28:46.898476] ctrlr.c:3506:nvmf_ctrlr_keep_alive: *DEBUG*: Keep Alive
[2023-07-11 11:28:46.898481] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:57344 cdw0:0 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:46.898489] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 7
[2023-07-11 11:28:46.898493] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 9
[2023-07-11 11:28:46.898498] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 11
[2023-07-11 11:28:46.898502] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 11
[2023-07-11 11:28:46.898513] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 12
[2023-07-11 11:28:46.898518] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 0
[2023-07-11 11:28:47.735961] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.



[2023-07-11 11:28:48.434471] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 1
[2023-07-11 11:28:48.434477] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 5
[2023-07-11 11:28:48.434483] nvme_qpair.c: 225:nvme_admin_qpair_print_command: *NOTICE*: KEEP ALIVE (18) qid:0 cid:61440 nsid:0 cdw10:00000000 cdw11:00000000 
[2023-07-11 11:28:48.434493] ctrlr.c:3506:nvmf_ctrlr_keep_alive: *DEBUG*: Keep Alive
[2023-07-11 11:28:48.434498] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:61440 cdw0:0 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:48.434506] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 7
[2023-07-11 11:28:48.434510] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 9
[2023-07-11 11:28:48.434514] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 11
[2023-07-11 11:28:48.434518] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 11
[2023-07-11 11:28:48.434528] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 12
[2023-07-11 11:28:48.434533] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 0
[2023-07-11 11:28:48.738314] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:28:49.740667] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 8 involuntary context switches in the last second.
[2023-07-11 11:28:49.970484] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 1
[2023-07-11 11:28:49.970489] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 5
[2023-07-11 11:28:49.970495] nvme_qpair.c: 225:nvme_admin_qpair_print_command: *NOTICE*: KEEP ALIVE (18) qid:0 cid:0 nsid:0 cdw10:00000000 cdw11:00000000 
[2023-07-11 11:28:49.970505] ctrlr.c:3506:nvmf_ctrlr_keep_alive: *DEBUG*: Keep Alive
[2023-07-11 11:28:49.970510] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:0 cid:0 cdw0:0 sqhd:0000 p:0 m:0 dnr:0
[2023-07-11 11:28:49.970518] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 7
[2023-07-11 11:28:49.970522] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 9
[2023-07-11 11:28:49.970526] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 11
[2023-07-11 11:28:49.970531] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 11
[2023-07-11 11:28:49.970540] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 12
[2023-07-11 11:28:49.970545] rdma.c:2048:nvmf_rdma_request_process: *DEBUG*: Request 0x2000194a0560 entering state 0
[2023-07-11 11:28:50.743020] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
^C[2023-07-11 11:28:51.395695] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 2
[2023-07-11 11:28:51.396218] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 0
[2023-07-11 11:28:51.396224] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 3
[2023-07-11 11:28:51.396733] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 1
[2023-07-11 11:28:51.396739] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 4
[2023-07-11 11:28:51.397264] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 2
[2023-07-11 11:28:51.397270] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 5
[2023-07-11 11:28:51.397841] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 3
[2023-07-11 11:28:51.397848] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 6
[2023-07-11 11:28:51.398255] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 4
[2023-07-11 11:28:51.398261] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 7
[2023-07-11 11:28:51.398955] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 5
[2023-07-11 11:28:51.398962] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 8
[2023-07-11 11:28:51.399711] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 6
[2023-07-11 11:28:51.399717] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 9
[2023-07-11 11:28:51.400415] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 7
[2023-07-11 11:28:51.400424] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 10
[2023-07-11 11:28:51.401378] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 8
[2023-07-11 11:28:51.401384] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 11
[2023-07-11 11:28:51.402131] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 9
[2023-07-11 11:28:51.402138] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 12
[2023-07-11 11:28:51.402979] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 10
[2023-07-11 11:28:51.402985] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 13
[2023-07-11 11:28:51.403465] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 11
[2023-07-11 11:28:51.403471] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 14
[2023-07-11 11:28:51.404336] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 12
[2023-07-11 11:28:51.404342] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 15
[2023-07-11 11:28:51.405220] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 13
[2023-07-11 11:28:51.405226] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 16
[2023-07-11 11:28:51.406046] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 14
[2023-07-11 11:28:51.406052] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 17
[2023-07-11 11:28:51.406909] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 15
[2023-07-11 11:28:51.406914] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 18
[2023-07-11 11:28:51.407721] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 16
[2023-07-11 11:28:51.407727] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 19
[2023-07-11 11:28:51.408540] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 17
[2023-07-11 11:28:51.408546] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 20
[2023-07-11 11:28:51.409430] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 18
[2023-07-11 11:28:51.409437] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 21
[2023-07-11 11:28:51.410297] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 19
[2023-07-11 11:28:51.410304] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 22
[2023-07-11 11:28:51.411146] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 20
[2023-07-11 11:28:51.411152] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 23
[2023-07-11 11:28:51.411648] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 21
[2023-07-11 11:28:51.411654] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 24
[2023-07-11 11:28:51.412550] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 22
[2023-07-11 11:28:51.412556] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 25
[2023-07-11 11:28:51.413057] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 23
[2023-07-11 11:28:51.413062] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 26
[2023-07-11 11:28:51.413932] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 24
[2023-07-11 11:28:51.413938] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 27
[2023-07-11 11:28:51.414780] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 25
[2023-07-11 11:28:51.414786] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 28
[2023-07-11 11:28:51.415632] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 26
[2023-07-11 11:28:51.415637] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 29
[2023-07-11 11:28:51.416478] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 27
[2023-07-11 11:28:51.416483] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 30
[2023-07-11 11:28:51.417306] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 28
[2023-07-11 11:28:51.417311] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 31
[2023-07-11 11:28:51.418189] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 29
[2023-07-11 11:28:51.418195] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 32
[2023-07-11 11:28:51.418660] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 30
[2023-07-11 11:28:51.418666] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 33
[2023-07-11 11:28:51.419188] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 31
[2023-07-11 11:28:51.419194] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 34
[2023-07-11 11:28:51.420104] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 32
[2023-07-11 11:28:51.420110] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 35
[2023-07-11 11:28:51.420557] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 33
[2023-07-11 11:28:51.420563] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 36
[2023-07-11 11:28:51.421448] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 34
[2023-07-11 11:28:51.421453] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 37
[2023-07-11 11:28:51.421942] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 35
[2023-07-11 11:28:51.421949] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 38
[2023-07-11 11:28:51.422765] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 36
[2023-07-11 11:28:51.422771] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 39
[2023-07-11 11:28:51.423613] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 37
[2023-07-11 11:28:51.423619] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 40
[2023-07-11 11:28:51.424515] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 38
[2023-07-11 11:28:51.424520] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 41
[2023-07-11 11:28:51.425025] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 39
[2023-07-11 11:28:51.425031] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 42
[2023-07-11 11:28:51.425865] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 40
[2023-07-11 11:28:51.425871] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 43
[2023-07-11 11:28:51.426720] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 41
[2023-07-11 11:28:51.426726] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 44
[2023-07-11 11:28:51.427600] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 42
[2023-07-11 11:28:51.427606] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 45
[2023-07-11 11:28:51.428054] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 43
[2023-07-11 11:28:51.428060] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 46
[2023-07-11 11:28:51.429031] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 44
[2023-07-11 11:28:51.429037] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 47
[2023-07-11 11:28:51.429859] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 45
[2023-07-11 11:28:51.429865] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 48
[2023-07-11 11:28:51.430737] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 46
[2023-07-11 11:28:51.430742] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 49
[2023-07-11 11:28:51.431557] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 47
[2023-07-11 11:28:51.431563] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 50
[2023-07-11 11:28:51.432460] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 48
[2023-07-11 11:28:51.432466] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 51
[2023-07-11 11:28:51.432956] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 49
[2023-07-11 11:28:51.432961] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 52
[2023-07-11 11:28:51.433893] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 50
[2023-07-11 11:28:51.433899] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 53
[2023-07-11 11:28:51.434736] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 51
[2023-07-11 11:28:51.434741] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 54
[2023-07-11 11:28:51.435196] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 52
[2023-07-11 11:28:51.435201] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 55
[2023-07-11 11:28:51.436096] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 53
[2023-07-11 11:28:51.436103] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 56
[2023-07-11 11:28:51.436915] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 54
[2023-07-11 11:28:51.436921] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 57
[2023-07-11 11:28:51.437766] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 55
[2023-07-11 11:28:51.437771] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 58
[2023-07-11 11:28:51.438253] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 56
[2023-07-11 11:28:51.438259] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 59
[2023-07-11 11:28:51.439124] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 57
[2023-07-11 11:28:51.439130] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 60
[2023-07-11 11:28:51.439925] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 58
[2023-07-11 11:28:51.439931] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 61
[2023-07-11 11:28:51.440753] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 59
[2023-07-11 11:28:51.440759] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 62
[2023-07-11 11:28:51.441573] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 60
[2023-07-11 11:28:51.441579] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 63
[2023-07-11 11:28:51.442471] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 61
[2023-07-11 11:28:51.442476] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 64
[2023-07-11 11:28:51.443327] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 62
[2023-07-11 11:28:51.443333] thread.c:2300:spdk_get_io_channel: *DEBUG*: Get io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 65
[2023-07-11 11:28:51.444160] nvmf.c:1118:_nvmf_transport_qpair_fini_complete: *DEBUG*: Finish destroying qid 63
[2023-07-11 11:28:51.444170] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444175] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444181] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444186] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444191] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444197] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444202] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444207] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444212] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444218] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444223] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444228] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444233] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444238] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444243] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444248] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444253] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444258] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444263] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444268] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444273] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444278] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444283] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444288] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444293] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444298] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444303] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.444313] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x23a0ea0
[2023-07-11 11:28:51.444319] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x23bf2e0
[2023-07-11 11:28:51.444324] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x23bfa90
[2023-07-11 11:28:51.444329] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2389b60
[2023-07-11 11:28:51.444333] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x238a310
[2023-07-11 11:28:51.444338] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x238ac30
[2023-07-11 11:28:51.444343] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x23f44d0
[2023-07-11 11:28:51.444348] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x23fce20
[2023-07-11 11:28:51.444353] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24057c0
[2023-07-11 11:28:51.444358] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x240e160
[2023-07-11 11:28:51.444362] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2416b00
[2023-07-11 11:28:51.444367] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x241f400
[2023-07-11 11:28:51.444372] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2427b90
[2023-07-11 11:28:51.444376] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2430490
[2023-07-11 11:28:51.444381] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2438c20
[2023-07-11 11:28:51.444386] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2441520
[2023-07-11 11:28:51.444391] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2449cb0
[2023-07-11 11:28:51.444396] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24525b0
[2023-07-11 11:28:51.444400] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x245ad40
[2023-07-11 11:28:51.444405] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2463640
[2023-07-11 11:28:51.444409] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x246bdd0
[2023-07-11 11:28:51.444414] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24746d0
[2023-07-11 11:28:51.444419] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x247ce60
[2023-07-11 11:28:51.444428] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2485760
[2023-07-11 11:28:51.444433] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x248def0
[2023-07-11 11:28:51.444437] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24967f0
[2023-07-11 11:28:51.444442] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x249ef80
[2023-07-11 11:28:51.444447] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24a7880
[2023-07-11 11:28:51.444452] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24b0010
[2023-07-11 11:28:51.444457] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24b8910
[2023-07-11 11:28:51.444461] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24c10a0
[2023-07-11 11:28:51.444466] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24c99a0
[2023-07-11 11:28:51.444471] rdma.c:3763:nvmf_process_ib_events: *DEBUG*: Device mlx5_0: 32 events processed
[2023-07-11 11:28:51.444485] nvmf.c:1100:_nvmf_ctrlr_free_from_qpair: *DEBUG*: Last qpair 63, destroy ctrlr 0x1
[2023-07-11 11:28:51.444491] subsystem.c:2016:nvmf_subsystem_remove_ctrlr: *DEBUG*: remove ctrlr 0x2384560 id 0x1 from subsys 0x238b1b0 nqn.2022-06.io.spdk:cnode216
[2023-07-11 11:28:51.446361] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 65
[2023-07-11 11:28:51.448039] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 64
[2023-07-11 11:28:51.449707] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 63
[2023-07-11 11:28:51.451181] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 62
[2023-07-11 11:28:51.452535] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 61
[2023-07-11 11:28:51.454215] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 60
[2023-07-11 11:28:51.455933] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 59
[2023-07-11 11:28:51.455944] subsystem.c: 448:spdk_nvmf_subsystem_destroy: *DEBUG*: subsystem is 0x2373880 nqn.2014-08.org.nvmexpress.discovery
[2023-07-11 11:28:51.455951] subsystem.c: 448:spdk_nvmf_subsystem_destroy: *DEBUG*: subsystem is 0x238b1b0 nqn.2022-06.io.spdk:cnode216
[2023-07-11 11:28:51.455962] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.455969] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.455974] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.455979] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.455985] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24d2130
[2023-07-11 11:28:51.455990] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24daa30
[2023-07-11 11:28:51.455996] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24e31c0
[2023-07-11 11:28:51.456001] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24ebac0
[2023-07-11 11:28:51.456006] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24f4250
[2023-07-11 11:28:51.456011] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x24fcb50
[2023-07-11 11:28:51.456015] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x25052e0
[2023-07-11 11:28:51.456020] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x250dbe0
[2023-07-11 11:28:51.456025] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2516370
[2023-07-11 11:28:51.456030] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x251ec70
[2023-07-11 11:28:51.456035] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2527400
[2023-07-11 11:28:51.456040] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x252fd00
[2023-07-11 11:28:51.456044] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2538490
[2023-07-11 11:28:51.456050] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2540d90
[2023-07-11 11:28:51.456055] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2549520
[2023-07-11 11:28:51.456060] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2551e20
[2023-07-11 11:28:51.456064] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x255a5b0
[2023-07-11 11:28:51.456070] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2562eb0
[2023-07-11 11:28:51.456074] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x256b640
[2023-07-11 11:28:51.456079] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2573f40
[2023-07-11 11:28:51.456084] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x257c6d0
[2023-07-11 11:28:51.456088] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2584fd0
[2023-07-11 11:28:51.456093] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x258d760
[2023-07-11 11:28:51.456098] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x2596060
[2023-07-11 11:28:51.456103] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x259e7f0
[2023-07-11 11:28:51.456108] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x25a70f0
[2023-07-11 11:28:51.456112] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x25af880
[2023-07-11 11:28:51.456117] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x25b8180
[2023-07-11 11:28:51.456122] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x25c0910
[2023-07-11 11:28:51.456126] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x25c9210
[2023-07-11 11:28:51.456131] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x25d19a0
[2023-07-11 11:28:51.456136] rdma.c:3692:nvmf_process_ib_event: *DEBUG*: Last WQE reached event received for rqpair 0x25db3e0
[2023-07-11 11:28:51.456141] rdma.c:3763:nvmf_process_ib_events: *DEBUG*: Device mlx5_0: 32 events processed
[2023-07-11 11:28:51.458023] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 58
[2023-07-11 11:28:51.459836] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 57
[2023-07-11 11:28:51.461487] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 56
[2023-07-11 11:28:51.462821] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 55
[2023-07-11 11:28:51.464595] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 54
[2023-07-11 11:28:51.466292] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 53
[2023-07-11 11:28:51.467586] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 52
[2023-07-11 11:28:51.468923] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 51
[2023-07-11 11:28:51.468935] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.468941] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.468947] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.468952] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.470745] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 50
[2023-07-11 11:28:51.472526] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 49
[2023-07-11 11:28:51.474219] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 48
[2023-07-11 11:28:51.475615] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 47
[2023-07-11 11:28:51.477443] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 46
[2023-07-11 11:28:51.478868] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 45
[2023-07-11 11:28:51.480672] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 44
[2023-07-11 11:28:51.482311] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 43
[2023-07-11 11:28:51.482323] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.482329] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.482334] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.482340] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.484012] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 42
[2023-07-11 11:28:51.485870] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 41
[2023-07-11 11:28:51.487558] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 40
[2023-07-11 11:28:51.488893] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 39
[2023-07-11 11:28:51.490273] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 38
[2023-07-11 11:28:51.491642] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 37
[2023-07-11 11:28:51.493397] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 36
[2023-07-11 11:28:51.495152] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 35
[2023-07-11 11:28:51.495165] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.495171] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.495176] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.495181] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.496900] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 34
[2023-07-11 11:28:51.496909] ctrlr.c: 531:_nvmf_ctrlr_destruct: *DEBUG*: Destroy ctrlr 0x1
[2023-07-11 11:28:51.496913] ctrlr.c:  86:nvmf_ctrlr_stop_keep_alive_timer: *DEBUG*: Stop keep alive poller
[2023-07-11 11:28:51.496920] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 33
[2023-07-11 11:28:51.498680] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 32
[2023-07-11 11:28:51.500509] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 31
[2023-07-11 11:28:51.502216] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 30
[2023-07-11 11:28:51.503954] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 29
[2023-07-11 11:28:51.505772] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 28
[2023-07-11 11:28:51.505784] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.505790] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.505795] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.507484] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 27
[2023-07-11 11:28:51.508875] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 26
[2023-07-11 11:28:51.510643] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 25
[2023-07-11 11:28:51.512440] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 24
[2023-07-11 11:28:51.514148] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 23
[2023-07-11 11:28:51.515555] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 22
[2023-07-11 11:28:51.517195] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 21
[2023-07-11 11:28:51.519068] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 20
[2023-07-11 11:28:51.519080] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.519086] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.519091] rdma.c:3509:nvmf_process_cm_event: *DEBUG*: Acceptor Event: RDMA_CM_EVENT_DISCONNECTED
[2023-07-11 11:28:51.520776] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 19
[2023-07-11 11:28:51.522514] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 18
[2023-07-11 11:28:51.524232] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 17
[2023-07-11 11:28:51.526089] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 16
[2023-07-11 11:28:51.527716] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 15
[2023-07-11 11:28:51.529282] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 14
[2023-07-11 11:28:51.530873] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 13
[2023-07-11 11:28:51.532728] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 12
[2023-07-11 11:28:51.534433] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 11
[2023-07-11 11:28:51.536116] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 10
[2023-07-11 11:28:51.537735] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 9
[2023-07-11 11:28:51.539208] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 8
[2023-07-11 11:28:51.540710] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 7
[2023-07-11 11:28:51.542190] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 6
[2023-07-11 11:28:51.543680] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 5
[2023-07-11 11:28:51.545543] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 4
[2023-07-11 11:28:51.546724] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 3
[2023-07-11 11:28:51.548026] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 2
[2023-07-11 11:28:51.549363] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0 refcnt 1
[2023-07-11 11:28:51.549382] thread.c:2365:put_io_channel: *DEBUG*: Releasing io_channel 0x231a000 for io_device nvmf_tgt (0x2373570) on thread nvmf_tgt_poll_group_0
[2023-07-11 11:28:51.554434] rdma.c:4081:nvmf_rdma_poller_destroy: *DEBUG*: Destroyed RDMA shared queue 0x2315710
[2023-07-11 11:28:51.560059] rdma.c:4081:nvmf_rdma_poller_destroy: *DEBUG*: Destroyed RDMA shared queue 0x2328940
^C[2023-07-11 11:28:51.565861] rdma.c:4081:nvmf_rdma_poller_destroy: *DEBUG*: Destroyed RDMA shared queue 0x2316fd0
[2023-07-11 11:28:51.571504] rdma.c:4081:nvmf_rdma_poller_destroy: *DEBUG*: Destroyed RDMA shared queue 0x2317690
[2023-07-11 11:28:51.572297] thread.c:2429:spdk_put_io_channel: *DEBUG*: Putting io_channel 0x23a0e10 for io_device iobuf (0xae3540) on thread nvmf_tgt_poll_group_0 refcnt 1
[2023-07-11 11:28:51.572321] thread.c: 689:spdk_thread_exit: *DEBUG*: Exit thread nvmf_tgt_poll_group_0
[2023-07-11 11:28:51.572328] thread.c: 634:thread_exit: *INFO*: thread nvmf_tgt_poll_group_0 still has messages
[2023-07-11 11:28:51.572338] thread.c:2237:spdk_io_device_unregister: *DEBUG*: Unregistering io_device nvmf_tgt (0x2373570) from thread app_thread
[2023-07-11 11:28:51.572344] thread.c:2180:io_device_free: *DEBUG*: io_device nvmf_tgt (0x2373570) needs to unregister from thread app_thread
[2023-07-11 11:28:51.572353] thread.c:2365:put_io_channel: *DEBUG*: Releasing io_channel 0x23a0e10 for io_device iobuf (0xae3540) on thread nvmf_tgt_poll_group_0
[2023-07-11 11:28:51.572360] thread.c:2161:_finish_unregister: *DEBUG*: Finishing unregistration of io_device nvmf_tgt (0x2373570) on thread app_thread
[2023-07-11 11:28:51.572369] rdma.c:2953:nvmf_rdma_stop_listen_ex: *DEBUG*: Port 172.17.29.63:4420 removed. need retry: 0
[2023-07-11 11:28:51.576773] rdma.c:2738:destroy_ib_device: *DEBUG*: IB device [0x238f530] is destroyed.
[2023-07-11 11:28:51.580944] rdma.c:2738:destroy_ib_device: *DEBUG*: IB device [0x2391cb0] is destroyed.
[2023-07-11 11:28:51.585225] rdma.c:2738:destroy_ib_device: *DEBUG*: IB device [0x23931b0] is destroyed.
[2023-07-11 11:28:51.589430] rdma.c:2738:destroy_ib_device: *DEBUG*: IB device [0x23946b0] is destroyed.
[2023-07-11 11:28:51.589770] bdev_raid.c: 859:raid_bdev_fini_start: *DEBUG*: raid_bdev_fini_start
[2023-07-11 11:28:51.589780] bdev.c:2232:bdev_finish_unregister_bdevs_iter: *DEBUG*: Done unregistering bdevs
[2023-07-11 11:28:51.589790] thread.c:2237:spdk_io_device_unregister: *DEBUG*: Unregistering io_device aio_module (0xae1420) from thread app_thread
[2023-07-11 11:28:51.589796] thread.c:2237:spdk_io_device_unregister: *DEBUG*: Unregistering io_device nvme_poll_groups (0xae0380) from thread app_thread
[2023-07-11 11:28:51.589801] thread.c:2237:spdk_io_device_unregister: *DEBUG*: Unregistering io_device null_bdev (0xae0100) from thread app_thread
[2023-07-11 11:28:51.589806] thread.c:2180:io_device_free: *DEBUG*: io_device null_bdev (0xae0100) needs to unregister from thread app_thread
[2023-07-11 11:28:51.589811] thread.c:2161:_finish_unregister: *DEBUG*: Finishing unregistration of io_device null_bdev (0xae0100) on thread app_thread
[2023-07-11 11:28:51.589823] thread.c:2237:spdk_io_device_unregister: *DEBUG*: Unregistering io_device bdev_malloc (0xae0020) from thread app_thread
[2023-07-11 11:28:51.589829] bdev_raid.c: 876:raid_bdev_exit: *DEBUG*: raid_bdev_exit
[2023-07-11 11:28:51.590217] thread.c:2237:spdk_io_device_unregister: *DEBUG*: Unregistering io_device bdev_mgr (0xae2e20) from thread app_thread
[2023-07-11 11:28:51.590226] thread.c:2180:io_device_free: *DEBUG*: io_device bdev_mgr (0xae2e20) needs to unregister from thread app_thread
[2023-07-11 11:28:51.590232] thread.c:2161:_finish_unregister: *DEBUG*: Finishing unregistration of io_device bdev_mgr (0xae2e20) on thread app_thread
[2023-07-11 11:28:51.602862] thread.c:2237:spdk_io_device_unregister: *DEBUG*: Unregistering io_device accel (0xae2fa0) from thread app_thread
[2023-07-11 11:28:51.602881] thread.c:2180:io_device_free: *DEBUG*: io_device accel (0xae2fa0) needs to unregister from thread app_thread
[2023-07-11 11:28:51.602891] thread.c:2161:_finish_unregister: *DEBUG*: Finishing unregistration of io_device accel (0xae2fa0) on thread app_thread
[2023-07-11 11:28:51.602899] thread.c:2237:spdk_io_device_unregister: *DEBUG*: Unregistering io_device sw_accel_module (0xae3140) from thread app_thread
[2023-07-11 11:28:51.602905] thread.c:2237:spdk_io_device_unregister: *DEBUG*: Unregistering io_device iobuf (0xae3540) from thread app_thread
[2023-07-11 11:28:51.602911] thread.c:2180:io_device_free: *DEBUG*: io_device iobuf (0xae3540) needs to unregister from thread app_thread
[2023-07-11 11:28:51.602915] thread.c:2161:_finish_unregister: *DEBUG*: Finishing unregistration of io_device iobuf (0xae3540) on thread app_thread
[2023-07-11 11:28:51.642060] reactor.c:1380:spdk_for_each_reactor: *DEBUG*: Starting reactor iteration from 0
[2023-07-11 11:28:51.642088] reactor.c:1332:on_reactor: *DEBUG*: Completed reactor iteration
[2023-07-11 11:28:51.642097] thread.c: 689:spdk_thread_exit: *DEBUG*: Exit thread app_thread
[2023-07-11 11:28:51.642103] thread.c: 727:spdk_thread_destroy: *DEBUG*: Destroy thread nvmf_tgt_poll_group_0
[2023-07-11 11:28:51.651237] thread.c: 727:spdk_thread_destroy: *DEBUG*: Destroy thread app_thread

