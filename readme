spdk_tgt源码分析, SPDK NVMe-oF target代码分析（转载+改编）: https://www.cnblogs.com/whl320124/articles/10450250.html
initiator_spdk_tgt, spdk-nvmf指南: https://blog.csdn.net/zxpoiu/article/details/116003444

spdk

shell巧用数组:
sed -r 's/CONFIG_([[:alnum:]_]+)=(.*)/CONFIG[\1]=\2/g' $rootdir/CONFIG > $rootdir/CONFIG.sh
declare -A CONFIG
source $rootdir/CONFIG.sh

配置：将长选项中的 ~ 文字视为用户的 $HOME


编译, 安装依赖, 安装日志: spdk_install_dep_log
git clone https://github.com/spdk/spdk
cd spdk
git submodule update --init
yum install -y vim git

sudo scripts/pkgdep.sh
sudo scripts/pkgdep.sh --all

./configure # ./configure --enable-lto --enable-werror
make

# 捕获异常
trap 'set +e; trap - ERR; echo "Error!"; exit 1;' ERR


io路径
hello_world.c
参考源码分析: https://blog.csdn.net/wade1010/article/details/128782710
demo: 从hello_world程序来剖析SDPK NVMe用户态驱动，对NVMe盘的初始化、管理和读写操作
examples/nvme/hello_world/hello_world.c -> main
spdk_env_opts_init(&opts)
main
  parse_args
  spdk_env_opts_init
  spdk_env_init
    build_eal_cmdline
    rte_eal_init
    spdk_env_dpdk_post_init
    dpdk_args = calloc(g_eal_cmdline_argcount, sizeof(char *))  二级指针分配
    rte_eal_init(g_eal_cmdline_argcount, dpdk_args)
    spdk_env_dpdk_post_init
      pci_env_init
        TAILQ_FOREACH
        register_rte_driver(driver)
        _pci_env_init()
      mem_map_init
        spdk_mem_map_alloc
        rte_mem_event_callback_register("spdk", memory_hotplug_cb, NULL)
        rte_memseg_contig_walk(memory_iter_cb, NULL)
      vtophys_init
        vtophys_notify
        vtophys_check_contiguous_entries
        spdk_mem_map_alloc
  spdk_vmd_init
    spdk_pci_enumerate(spdk_pci_vmd_get_driver(), vmd_enum_cb, &g_vmd_container)
  spdk_nvme_probe(NULL, NULL, probe_cb, attach_cb, NULL)
    spdk_nvme_trid_populate_transport
    spdk_nvme_probe_async
    nvme_init_controllers
  hello_world
    spdk_nvme_ctrlr_alloc_io_qpair
    spdk_nvme_ctrlr_map_cmb LBA start  number of LBAs 
    spdk_nvme_ns_cmd_write
    write_complete
    spdk_nvme_qpair_process_completions
  spdk_vmd_fini

example
打印所有nvme设备： build/examples/identify.c

app more example

发现nvme设备(spdk_nvme_probe)
发现SSD设备的时候，从SPDK进入到DPDK中，函数调用栈为
00 hello_word.c
01 -> main()
02 --> spdk_nvme_probe()
03 ---> spdk_nvme_probe_async()
04 ----> nvme_probe_internal()
05 -----> nvme_transport_ctrlr_scan()
06 -----> nvme_pcie_ctrlr_scan()  
07 ------> spdk_pci_enumerate()		
08 ------> spdk_pci_enumerate()
09 ------> rte_bus_probe()				             			    | SPDK |
   =========================================================================
10 -------> mlx5_common_dev_probe()                                 | DPDK |
11 --------> drivers_probe()
12 ---------> mlx5_bus_match()
13 ----------> mlx5_dev_pci_match()

probe流程详解:
struct spdk_nvme_probe_ctx {
	struct spdk_nvme_transport_id		trid;
	void					*cb_ctx;
	spdk_nvme_probe_cb			probe_cb;
	spdk_nvme_attach_cb			attach_cb;
	spdk_nvme_remove_cb			remove_cb;
	TAILQ_HEAD(, spdk_nvme_ctrlr)		init_ctrlrs;
};

rc = spdk_nvme_probe(NULL, NULL, probe_cb, attach_cb, NULL);  		20.02版本的调用	if (spdk_nvme_probe(NULL, probe_ctx, probe_cb, attach_cb, remove_cb)) 		20.10中已删除	probe_cb	
	spdk_nvme_trid_populate_transport 初始化trid  设置pcie类型SPDK_NVME_TRANSPORT_NAME_PCIE
	probe_ctx = spdk_nvme_probe_async(trid, cb_ctx, probe_cb, attach_cb, remove_cb);  开始进行probe_ctx的初始化
				nvme_driver_init  初始化g_spdk_nvme_driver的全局变量，其中包括锁，driver队列，热插拔fd的connect获取，uuid的初始化  只允许一个进程去做，加锁，避免引起混乱
				probe_ctx = calloc(1, sizeof(*probe_ctx));  创建上下文 类型为：struct spdk_nvme_probe_ctx 
				nvme_probe_ctx_init(probe_ctx, trid, cb_ctx, probe_cb, attach_cb, remove_cb);  变量的赋值，初始化probe_ctx->init_ctrlrs的队列，用来存放需要初始化的nvme 控制器
				rc = nvme_probe_internal(probe_ctx, false); 
					nvme_transport_ctrlr_scan(probe_ctx, direct_connect)； 
					nvme_pcie_ctrlr_scan
						enum_ctx.probe_ctx = probe_ctx;
						bool enum_ctx.has_pci_addr ？ 判断probe_ctx->trid.traddr中内容是否为空， 不为空，说明有特殊指定pci，则调用spdk_pci_device_attach，为空说明没有特殊指定pci  则调用spdk_pci_enumerate
						spdk_pci_enumerate(spdk_pci_nvme_get_driver(), pcie_nvme_enum_cb, &enum_ctx); 本example传入的参数为NULL，则直接执行enumerate流程
							cleanup_pci_devices  清除pci设备，状态dev->internal.removed为true的，从g_pci_devices中移除，在热插拔链表g_pci_hotplugged_devices中遍历，移除该队列并插入到g_pci_devices的尾部
							TAILQ_FOREACH(dev, &g_pci_devices, internal.tailq) {  开始遍历g_pci_devices，过程中需要加g_pci_mutex锁，防止列表信息变化。 实际上此时g_pci_devices为空，直接退出
							}
							scan_pci_bus					重新把所有的bus上的设备扫一遍，主要是pci bus
							driver->cb_fn = enum_cb;
							driver->cb_arg = enum_ctx;      把pcie_nvme_enum_cb和arg传入作为nvme driver的回调
							rte_bus_probe					调用pci_probe 把pci设备进行绑定
								----------------------------------------------------------------------------------------------------------一个个匹配，所以以下流程对于满足条件的pci设备会走多次
								rte_pci_probe_one_driver	device和驱动匹配上，如果不是匹配的，则退出。
								rte_pci_map_device			进行pci地址映射，以在用户空间访问
								ret = dr->probe(dr, dev);	调用pci_device_init函数，作为driver的probe。  pci_env_init函数中进行的probe函数的指定
									pci_device_init			
										rc = driver->cb_fn(driver->cb_arg, dev);  主要进行addr等基础信息的赋值传递，同时执行一开始传入的回调函数
										pcie_nvme_enum_cb						
											nvme_get_ctrlr_by_trid_unsafe		去g_nvme_attached_ctrlrs和g_spdk_nvme_driver->shared_attached_ctrlrs两个链表中搜索ctrlr，用来进行判断是否已创建
											nvme_ctrlr_probe(&trid, enum_ctx->probe_ctx, pci_dev);  有一个条件，用户传入过pci，则只创建传入的pci的ctrlr，否则全部创建
												spdk_nvme_ctrlr_get_default_ctrlr_opts				获取默认的ctrlr的opts参数
												probe_ctx->probe_cb(probe_ctx->cb_ctx, trid, &opts) 调用传入的probe_cb，打印了"Attaching to %s\n", trid->traddr
												ctrlr = nvme_transport_ctrlr_construct(trid, &opts, devhandle);  开始创建ctrlr
													spdk_pci_device_claim(pci_dev)										先claim pci设备，保证唯一性
													pctrlr = spdk_zmalloc(sizeof(struct nvme_pcie_ctrlr), 64, NULL,		创建nvme的ctrlr
																			SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_SHARE);
													rc = nvme_ctrlr_construct(&pctrlr->ctrlr);			初始化spdk_nvme_ctrlr的所有信息，初始化ctrlr->ctrlr_lock、active_procs等资源
													rc = nvme_pcie_ctrlr_allocate_bars(pctrlr);
															rc = spdk_pci_device_map_bar(pctrlr->devhandle, 0, &addr, &phys_addr, &size);  把之前映射好的地址读取过来，保存在参数中
															pctrlr->regs = (volatile struct spdk_nvme_registers *)addr;						关联pctrlr的regs和实际的pci的addr
															pctrlr->regs_size = size;														赋值寄存器的size
															nvme_pcie_ctrlr_map_cmb(pctrlr);												把addr、phys_addr、size、offset进行传递
													
													spdk_pci_device_cfg_read16(pci_dev, &cmd_reg, 4);				读取中断管理的fd，写入404，使中断失效
													cmd_reg |= 0x404;
													spdk_pci_device_cfg_write16(pci_dev, cmd_reg, 4);
													
													nvme_ctrlr_get_cap(&pctrlr->ctrlr, &cap)
													nvme_ctrlr_get_vs(&pctrlr->ctrlr, &vs)
													nvme_ctrlr_init_cap(&pctrlr->ctrlr, &cap, &vs);					通过寄存器读取的方式获取cap和vs信息，初始化cap。主要信息是page_size、io_queue_size、io_queue_requests
													rc = nvme_pcie_ctrlr_construct_admin_qpair(&pctrlr->ctrlr, pctrlr->ctrlr.opts.admin_queue_size);         创建管理队列qpair
															pqpair = spdk_zmalloc(sizeof(*pqpair), 64, NULL, SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_SHARE);	使用的是大页内存
															rc = nvme_qpair_init(ctrlr->adminq, 0, /* qpair ID */ ctrlr, SPDK_NVME_QPRIO_URGENT, num_entries);  初始化qpair的下属队列等，0号qpair就是管理队列
															nvme_pcie_qpair_construct(ctrlr->adminq, NULL);				创建其他qpair所需的信息max_completions_cap、tracker等，tracker是一次性分配该qpair的所有数量，因为内存对齐和边界的要求。
																														然后将tracker装入到qpair中的tracker数组指针中进行保存，并插入free_tr中进行记录
															nvme_pcie_qpair_reset(qpair);								qpair中的队列信息清零
													rc = nvme_ctrlr_add_process(&pctrlr->ctrlr, pci_dev);				创建进程信息，用于多进程下的ctrlr管理
													
													ctrlr->remove_cb = probe_ctx->remove_cb;							remove_cb的传入
													ctrlr->cb_ctx = probe_ctx->cb_ctx;
													nvme_qpair_set_state(ctrlr->adminq, NVME_QPAIR_ENABLED);			设置qpair的状态
													TAILQ_INSERT_TAIL(&probe_ctx->init_ctrlrs, ctrlr, tailq);			插入控制器到probe_ctx->init_ctrlrs中，用于后续的状态初始化
										TAILQ_INSERT_TAIL(&g_pci_hotplugged_devices, dev, internal.tailq);				插入到热插拔的队列中
								--------------------------------------------------------------------------------------------------------------------------------
							cleanup_pci_devices();		把新probe的控制器给放入g_pci_devices中管理（此前为空）
						
	nvme_init_controllers(probe_ctx);		
		rc = spdk_nvme_probe_poll_async(probe_ctx);
			------------------------------------------------------------TAILQ_FOREACH_SAFE(ctrlr, &probe_ctx->init_ctrlrs, tailq, ctrlr_tmp) {      对每一个在init_ctrlrs队列中的ctrlr
			nvme_ctrlr_poll_internal(ctrlr, probe_ctx);		对每个在probe_ctx->init_ctrlrs的ctrlr执行，直到该队列为空，设置g_spdk_nvme_driver->initialized = true;返回0
				rc = nvme_ctrlr_process_init(ctrlr);		配置ctrlr的寄存器状态 cc.en，identify 、 construct namespace、 identify namespace 等，直到状态为NVME_CTRLR_STATE_READY才算成功
				TAILQ_REMOVE(&probe_ctx->init_ctrlrs, ctrlr, tailq);								移除初始化队列
				TAILQ_INSERT_TAIL(&g_spdk_nvme_driver->shared_attached_ctrlrs, ctrlr, tailq);		插入到g_spdk_nvme_driver->shared_attached_ctrlrs队列
				nvme_ctrlr_proc_get_ref(ctrlr);														移除inactive的proc，给当前的proc进行active_proc->ref++
				probe_ctx->attach_cb(probe_ctx->cb_ctx, &ctrlr->trid, ctrlr, &ctrlr->opts);			如果attach_cb有效，则进行attach_cb流程
			------------------------------------------------------------	循环执行
------------------------------------------------------------------------------------------------  到此为止example的spdk_nvme_probe流程就结束了 

probe代码详解, 参考文档: https://www.cnblogs.com/vlhn/p/7727016.html
03 - DPDK中环境抽象层（EAL: Environment Abstraction Layer）的函数rte_pci_match()是发现SSD设备的关键。
04 - DPDK的EAL在DPDK架构中所处的位置


from xb:
文档
向nvme设备提交io: doc/nvme_spec.md

git add xx
git commit --amend
git push origin xb
https://github.com/ssbandjl/spdk/blob/xb/doc/nvme_spec.md



bdev_malloc_create

examples/blob/hello_world/hello_blob.c





docker run -it -d --privileged --cap-add=ALL --name spdk eeb6ee3f44bd
docker run -it -d --privileged --cap-add=ALL --name spdk -v /root/project/stor/spdk/xb/spdk:/home/xb/project/stor/spdk/xb/spdk centos:centos7.9.2009
docker start spdk
docker exec -u root -it spdk bash -c 'cd /home/xb/project/stor/spdk/xb/spdk;exec "${SHELL:-sh}"'
docker exec -it spdk /bin/bash



daos块设备, https://docs.daos.io/v2.3/user/blockdev/
POSIX 容器可以通过 NVMe-oF 协议导出块设备。 这需要在第三方节点（例如专用节点或在客户端节点或 DAOS 存储节点上的特定核心上运行）上设置单独的 SPDK 服务，将 DAOS 容器导出为 NVMe 目标。 本节介绍如何配置 SPDK DAOS bdev 并通过 NVMe-oF 协议访问它。 它假定已经配置了 DAOS 系统。

git clone https://github.com/spdk/spdk.git
git submodule update --init
./configure --with-daos
make -j 16


sudo HUGE_EVEN_ALLOC=yes scripts/setup.sh
sudo ./build/bin/nvmf_tgt -m [21,22,23,24]
$ sudo ./build/bin/nvmf_tgt -m [21,22,23,24]
[2023-04-21 09:09:40.791150] Starting SPDK v23.05-pre git sha1 26b9be752 / DPDK 22.11.1 initialization...
[2023-04-21 09:09:40.791194] [ DPDK EAL parameters: nvmf --no-shconf -l 21,22,23,24 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid747434 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2023-04-21 09:09:40.830768] app.c: 738:spdk_app_start: *NOTICE*: Total cores available: 4
[2023-04-21 09:09:40.859580] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 22
[2023-04-21 09:09:40.859716] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 23
[2023-04-21 09:09:40.859843] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 24
[2023-04-21 09:09:40.859844] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 21
[2023-04-21 09:09:40.878692] accel_sw.c: 601:sw_accel_module_init: *NOTICE*: Accel framework software module initialized.

POOL_UUID="${POOL:-pool_label}"
CONT_UUID="${CONT:-const_label}"
DISK_UUID="${UUID:-`uuidgen`}"
NR_DISKS="${1:-1}"
BIND_IP="${TARGET_IP:-172.31.91.61}"


创建传输层:
参数: scripts/rpc.py -> def nvmf_create_transport(args) 
sudo ./scripts/rpc.py nvmf_create_transport -t TCP -u 2097152 -i 2097152  # -u: IO单元字节数, -i: 最大IO字节

for i in $(seq 1 "$NR_DISKS"); do
    sudo ./scripts/rpc.py bdev_daos_create disk$i ${POOL_UUID} ${CONT_UUID} 1048576 4096 --uuid ${DISK_UUID}
    subsystem=nqn.2016-06.io.spdk$i:cnode$i
    sudo scripts/rpc.py nvmf_create_subsystem $subsystem -a -s SPDK0000000000000$i -d SPDK_Virtual_Controller_$i
    sudo scripts/rpc.py nvmf_subsystem_add_ns $subsystem  disk$i
    sudo scripts/rpc.py nvmf_subsystem_add_listener $subsystem -t tcp -a ${BIND_IP} -s 4420
done

块设备：
创建daos bdev块设备
daos cont create --pool=test-pool --label=test-cont --type=POSIX
rpc.py bdev_daos_create daosdev0 test-pool test-cont 64 4096  # 块大小为4096， 共64块, 256KB


spdk块设备主流程
nvmf_tgt
nvmf_create_transport
bdev_daos_create
nvmf_create_subsystem
nvmf_subsystem_add_ns
nvmf_subsystem_add_listener


test_daos
cc -o /dev/null -x c -march=native -I/usr/local/include -L/usr/local/lib -I/opt/daos/include -L/opt/daos/lib64 -lgurt -ldaos -ldaos_common -ldfs test_daos_dfs.c

--with-shared

--with-daos=--with-daos=/path/to/daos/install/dir

打开另一个终端进行配置过程。 配置是通过 scripts/rpc.py 脚本完成的，之后可以将其转储到 json 文件中，稍后可以将其传递给 nvmf_tgt。 创建由 DAOS DFS 备份的几个磁盘的最短方法是使用以下脚本（称为 export_disk.sh）


sudo nvme connect-all -t tcp -a 172.31.91.61 -s 4420
$ sudo umount /testfs
$ sudo mkfs.ext4 -F -O mmp /dev/nvme1n1
$ sudo e2mmpstatus /dev/nvme1n1
e2mmpstatus: it is safe to mount '/dev/nvme1n1', MMP is clean
$ sudo mount /dev/nvme1n1 /testfs
$ sudo nvme disconnect-all


./scripts/setup.sh
./scripts/setup.sh reset

qa:
spdk找不到 ncurses.h, spdk_top.c:52:21: fatal error: ncurses.h: No such file or directory
yum install ncurses-devel

编译spdk:
cd build/external/debug/spdk
export CFLAGS='-O0 -g'
./configure --help
yum install ncurses-devel ncurses
./configure --prefix="/opt/daos/prereq/debug/spdk" --enable-debug --disable-tests --disable-unit-tests --disable-apps --without-vhost --without-crypto --without-pmdk --without-rbd --with-rdma --without-iscsi-initiator --without-isal --without-vtune --with-shared --enable-examples

Running commands in /home/xb/project/stor/daos/origin/docker/daos
command:cp -r cache/spdk /home/xb/project/stor/daos/origin/docker/daos/build/external/debug/spdk
Running commands in /home/xb/project/stor/daos/origin/docker/daos/build/external/debug/spdk
command:git checkout v21.07

编译选项:
reqs.define('spdk',
            retriever=retriever,
            commands=['./configure --prefix="$SPDK_PREFIX"'                \
                      ' --disable-tests --disable-unit-tests '             \
                      ' --disable-apps --without-vhost '                   \
                      ' --without-crypto --without-pmdk --without-rbd '    \
                      ' --with-rdma --without-iscsi-initiator '            \
                      ' --without-isal --without-vtune --with-shared',
                      'make $JOBS_OPT', 'make install',
                      'cp -r -P dpdk/build/lib/* "$SPDK_PREFIX/lib"',
                      'mkdir -p "$SPDK_PREFIX/include/dpdk"',
                      'cp -r -P dpdk/build/include/* '                     \
                      '"$SPDK_PREFIX/include/dpdk"',
                      'mkdir -p "$SPDK_PREFIX/share/spdk"',
                      'cp -r include scripts "$SPDK_PREFIX/share/spdk"'],
            headers=['spdk/nvme.h', 'dpdk/rte_eal.h'],
            extra_include_path=['/usr/include/dpdk',
                                '$SPDK_PREFIX/include/dpdk',
                                # debian dpdk rpm puts rte_config.h here
                                '/usr/include/x86_64-linux-gnu/dpdk'],
            patch_rpath=['lib'])


ips="182.200.53.61 182.200.53.62"
for ip in $ips; do 
  rsync -rapvu /opt/daos root@$ip:/opt/
done


command:cp -r -P dpdk/build/lib/* "/opt/daos/prereq/debug/spdk/lib"
command:mkdir -p "/opt/daos/prereq/debug/spdk/include/dpdk"
command:cp -r -P dpdk/build/include/* "/opt/daos/prereq/debug/spdk/include/dpdk"
command:mkdir -p "/opt/daos/prereq/debug/spdk/share/spdk"
command:cp -r include scripts "/opt/daos/prereq/debug/spdk/share/spdk"
Running commands in /home/xb/project/stor/daos/origin/docker/daos
command:patchelf --set-rpath '$ORIGIN:/opt/daos/prereq/debug/spdk/lib' /opt/daos/prereq/debug/spdk/lib/librte_bus_pci.so
Running commands in /home/xb/project/stor/daos/origin/docker/daos
command:patchelf --set-rpath '$ORIGIN:/opt/daos/prereq/debug/spdk/lib' /opt/daos/prereq/debug/spdk/lib/librte_bus_vdev.so
Running commands in /home/xb/project/stor/daos/origin/docker/daos
command:patchelf --set-rpath '$ORIGIN:/opt/daos/prereq/debug/spdk/lib' /opt/daos/prereq/debug/spdk/lib/librte_cmdline.so


spdk

编译, 安装依赖, 安装日志: spdk_install_dep_log
git clone https://github.com/spdk/spdk
cd spdk
git submodule update --init
yum install -y vim git

sudo scripts/pkgdep.sh
sudo scripts/pkgdep.sh --all

./configure
make

# 捕获异常
trap 'set +e; trap - ERR; echo "Error!"; exit 1;' ERR



参考源码分析: https://blog.csdn.net/wade1010/article/details/128782710, https://blog.csdn.net/weixin_44879249/article/details/110920401
demo: 从hello_world程序来剖析SDPK NVMe用户态驱动，对NVMe盘的初始化、管理和读写操作
export LD_LIBRARY_PATH=/opt/daos/prereq/debug/spdk/lib:$LD_LIBRARY_PATH
cd build/examples
gdb hello_world

examples/nvme/hello_world/hello_world.c -> main
spdk_env_opts_init(&opts)
main
  parse_args
  spdk_env_opts_init
  spdk_env_init -> spdk_env_init(const struct spdk_env_opts *opts)
    if (g_external_init == false) g_external_init=true
    build_eal_cmdline -> build_eal_cmdline(const struct spdk_env_opts *opts)
      push_arg -> EAL参数如下:
      -c 0x1
      --log-level=lib.eal:6
      --log-level=lib.cryptodev:5
      --log-level=user1:6
      --iova-mode=pa
      --base-virtaddr=0x200000000000
      --match-allocations
      --file-prefix=spdk0
      --proc-type=auto
    rte_eal_init -> rte_eal_init(int argc, char **argv) 初始化环境抽象层 (EAL)。 此函数将仅在 MAIN lcore 上执行，并尽快在应用程序的 main() 函数中执行。 它将 WORKER lcores 置于 WAIT 状态, 代码流程: https://blog.csdn.net/armlinuxww/article/details/90288725, https://zhuanlan.zhihu.com/p/439119807
      rte_eal_get_configuration
      eal_get_internal_configuration
      rte_eal_cpu_init
      eal_parse_args
      rte_config_init
      rte_bus_scan
      is_iommu_enabled
      rte_eal_iova_mode
      rte_eal_memory_init ...
    spdk_env_dpdk_post_init -> spdk_env_dpdk_post_init(bool legacy_mem)
      pci_env_init() -> pci_env_init(void)
        TAILQ_FOREACH(driver, &g_pci_drivers, tailq)
          register_rte_driver -> register_rte_driver(struct spdk_pci_driver *driver) 将 spdk_pci_driver 转换为 rte_pci_driver 并将其注册到 dpdk
        _pci_env_init -> _pci_env_init(void)
          scan_pci_bus -> scan_pci_bus(bool delay_init)
            rte_bus_scan
            spdk_get_ticks...
          spdk_process_is_primary 热移除
      mem_map_init -> mem_map_init(bool legacy_mem)
        spdk_mem_map_alloc
        rte_mem_event_callback_register
        rte_memseg_contig_walk
      vtophys_init -> vtophys_init(void)
        vtophys_iommu_init
    dpdk_args = calloc(g_eal_cmdline_argcount, sizeof(char *))  二级指针分配
    rte_eal_init(g_eal_cmdline_argcount, dpdk_args)
    spdk_env_dpdk_post_init
      pci_env_init
        TAILQ_FOREACH
        register_rte_driver(driver)
        _pci_env_init()
      mem_map_init
        spdk_mem_map_alloc
        rte_mem_event_callback_register("spdk", memory_hotplug_cb, NULL)
        rte_memseg_contig_walk(memory_iter_cb, NULL)
      vtophys_init
        vtophys_notify
        vtophys_check_contiguous_entries
        spdk_mem_map_alloc
  spdk_vmd_init
    spdk_pci_enumerate(spdk_pci_vmd_get_driver(), vmd_enum_cb, &g_vmd_container)
  spdk_nvme_probe(NULL, NULL, probe_cb, attach_cb, NULL) 枚举传输 ID 指示的总线，并在需要时将用户空间 NVMe 驱动程序附加到找到的每个设备。 此函数不是线程安全的，一次只能从一个线程调用，而没有其他线程正在主动使用任何 NVMe 设备。 如果从辅助进程调用，则只会探测已附加到主进程中的用户空间驱动程序的设备。 如果多次调用，则只会报告尚未附加到 SPDK NVMe 驱动程序的设备。 要停止使用控制器并释放其关联资源，请使用 attach_cb() 函数中的 spdk_nvme_ctrlr 实例调用 spdk_nvme_detach(), 发现ssd, 参考: https://zhuanlan.zhihu.com/p/555930316, https://www.cnblogs.com/vlhn/p/7727016.html, proby__by_class_code, Class Code (SPDK_PCI_CLASS_NVME=0x010802)扯上了关系。 全局变量g_nvme_pci_drv就是在L53行定义的，而g_nvme_pci_drv.driver.id_table则是在L38行定义的
  spdk_nvme_probe(NULL, NULL, probe_cb, attach_cb, NULL) 枚举传输 ID 指示的总线，并在需要时将用户空间 NVMe 驱动程序附加到找到的每个设备。 此函数不是线程安全的，一次只能从一个线程调用，而没有其他线程正在主动使用任何 NVMe 设备。 如果从辅助进程调用，则只会探测已附加到主进程中的用户空间驱动程序的设备。 如果多次调用，则只会报告尚未附加到 SPDK NVMe 驱动程序的设备。 要停止使用控制器并释放其关联资源，请使用 attach_cb() 函数中的 spdk_nvme_ctrlr 实例调用 spdk_nvme_detach(), 发现ssd, 参考: https://zhuanlan.zhihu.com/p/555930316, https://www.cnblogs.com/vlhn/p/7727016.html, proby__by_class_code, Class Code (SPDK_PCI_CLASS_NVME=0x010802)扯上了关系。 全局变量g_nvme_pci_drv就是在L53行定义的，而g_nvme_pci_drv.driver.id_table则是在L38行定义的, 查看内存: lshw -short -C memory, lspci|grep -i non, nvme-cli
    spdk_nvme_trid_populate_transport
    spdk_nvme_probe_async
    nvme_init_controllers... spdk_pci_enumerate -> rte_pci_probe -> pci_probe_all_drivers -> rte_pci_probe_one_driver -> rte_pci_match
      if (id_table->class_id != pci_dev->id.class_id...
  hello_world -> hello_world(void)
    spdk_nvme_ctrlr_alloc_io_qpair 分配一个nvme queue pair作为该IO Channel的实际对象, 分配一个 I/O qpair，我们可以使用它来向控制器上的命名空间提交读/写请求。 NVMe 控制器通常支持每个控制器的许多 qpairs。 为控制器分配的任何 I/O qpair 都可以将 I/O 提交到该控制器上的任何命名空间。
SPDK NVMe 驱动程序不为 qpair 访问提供同步 - 应用程序必须确保只有一个线程将 I/O 提交给 qpair，并且同一线程还必须检查该 qpair 上的完成。 这通过使所有 I/O 操作完全无锁来实现极其高效的 I/O 处理, 分配一个 I/O 队列对（提交和完成队列）。 默认情况下，此函数还执行新创建的 qpair 所需的任何连接活动。 为避免这种行为，用户应将 opts 结构中的 create_only 标志设置为 true。 每个队列对一次只能在一个线程中使用（用户必须强制执行互斥）
    spdk_nvme_ctrlr_map_cmb LBA start  number of LBAs, 映射先前保留的控制器内存缓冲区，以便它的数据对 CPU 可见。 此操作并不总是可行的, 从控制器中申请内存, 设备内存(DRAM), Controller Memory Buffer(CMB)，可用于保存SQ和CQ队列元素，也可用于App buf(借助spdk_nvme_ctrlr_map_cmb函数)，从而节省了部分DMA操作(从主存拷贝数据到控制器)，降低了PCIe switch的路由开销, 如果ctrlr中的buf不够则从主机内存中去分配
    spdk_zmalloc 根据给定的 dma_flg 分配 dma/可共享内存。 它是一个具有给定大小、对齐方式和套接字 ID 的内存缓冲区。 此外，缓冲区将被清零
    snprintf(sequence.buf, 0x1000, "%s", "Hello world!\n") 将字符串打印到buf上(4KB)
    spdk_nvme_ns_cmd_write -> spdk_nvme_ns_cmd_write(struct spdk_nvme_ns *ns
    _nvme_ns_cmd_rw -> _nvme_ns_cmd_rw(struct spdk_nvme_ns *ns... SPDK_NVME_OPC_WRITE
    nvme_qpair_submit_request
    write_complete
    spdk_nvme_qpair_process_completions
  spdk_vmd_fini

example
打印所有nvme设备： build/examples/identify.c

app more example

发现nvme设备(spdk_nvme_probe)
发现SSD设备的时候，从SPDK进入到DPDK中，函数调用栈为
00 hello_world.c
01 -> main()
02 --> spdk_nvme_probe()
03 ---> spdk_nvme_probe_async()
04 ----> nvme_probe_internal()
05 -----> nvme_transport_ctrlr_scan()
06 -----> nvme_pcie_ctrlr_scan()  
07 ------> spdk_pci_enumerate()		
08 ------> spdk_pci_enumerate()
09 ------> rte_bus_probe()				             			    | SPDK |
   =========================================================================
10 -------> mlx5_common_dev_probe()                                 | DPDK |
11 --------> drivers_probe()
12 ---------> mlx5_bus_match()
13 ----------> mlx5_dev_pci_match()

probe流程详解:
struct spdk_nvme_probe_ctx {
	struct spdk_nvme_transport_id		trid;
	void					*cb_ctx;
	spdk_nvme_probe_cb			probe_cb;
	spdk_nvme_attach_cb			attach_cb;
	spdk_nvme_remove_cb			remove_cb;
	TAILQ_HEAD(, spdk_nvme_ctrlr)		init_ctrlrs;
};

rc = spdk_nvme_probe(NULL, NULL, probe_cb, attach_cb, NULL);  		20.02版本的调用	if (spdk_nvme_probe(NULL, probe_ctx, probe_cb, attach_cb, remove_cb)) 		20.10中已删除	probe_cb	
	spdk_nvme_trid_populate_transport 初始化trid  设置pcie类型SPDK_NVME_TRANSPORT_NAME_PCIE
	probe_ctx = spdk_nvme_probe_async(trid, cb_ctx, probe_cb, attach_cb, remove_cb);  开始进行probe_ctx的初始化
				nvme_driver_init  初始化g_spdk_nvme_driver的全局变量，其中包括锁，driver队列，热插拔fd的connect获取，uuid的初始化  只允许一个进程去做，加锁，避免引起混乱
				probe_ctx = calloc(1, sizeof(*probe_ctx));  创建上下文 类型为：struct spdk_nvme_probe_ctx 
				nvme_probe_ctx_init(probe_ctx, trid, cb_ctx, probe_cb, attach_cb, remove_cb);  变量的赋值，初始化probe_ctx->init_ctrlrs的队列，用来存放需要初始化的nvme 控制器
				rc = nvme_probe_internal(probe_ctx, false); 
					nvme_transport_ctrlr_scan(probe_ctx, direct_connect)； 
					nvme_pcie_ctrlr_scan
						enum_ctx.probe_ctx = probe_ctx;
						bool enum_ctx.has_pci_addr ？ 判断probe_ctx->trid.traddr中内容是否为空， 不为空，说明有特殊指定pci，则调用spdk_pci_device_attach，为空说明没有特殊指定pci  则调用spdk_pci_enumerate
						spdk_pci_enumerate(spdk_pci_nvme_get_driver(), pcie_nvme_enum_cb, &enum_ctx); 本example传入的参数为NULL，则直接执行enumerate流程
							cleanup_pci_devices  清除pci设备，状态dev->internal.removed为true的，从g_pci_devices中移除，在热插拔链表g_pci_hotplugged_devices中遍历，移除该队列并插入到g_pci_devices的尾部
							TAILQ_FOREACH(dev, &g_pci_devices, internal.tailq) {  开始遍历g_pci_devices，过程中需要加g_pci_mutex锁，防止列表信息变化。 实际上此时g_pci_devices为空，直接退出
							}
							scan_pci_bus					重新把所有的bus上的设备扫一遍，主要是pci bus
							driver->cb_fn = enum_cb;
							driver->cb_arg = enum_ctx;      把pcie_nvme_enum_cb和arg传入作为nvme driver的回调
							rte_bus_probe					调用pci_probe 把pci设备进行绑定
								----------------------------------------------------------------------------------------------------------一个个匹配，所以以下流程对于满足条件的pci设备会走多次
								rte_pci_probe_one_driver	device和驱动匹配上，如果不是匹配的，则退出。
								rte_pci_map_device			进行pci地址映射，以在用户空间访问
								ret = dr->probe(dr, dev);	调用pci_device_init函数，作为driver的probe。  pci_env_init函数中进行的probe函数的指定
									pci_device_init			
										rc = driver->cb_fn(driver->cb_arg, dev);  主要进行addr等基础信息的赋值传递，同时执行一开始传入的回调函数
										pcie_nvme_enum_cb						
											nvme_get_ctrlr_by_trid_unsafe		去g_nvme_attached_ctrlrs和g_spdk_nvme_driver->shared_attached_ctrlrs两个链表中搜索ctrlr，用来进行判断是否已创建
											nvme_ctrlr_probe(&trid, enum_ctx->probe_ctx, pci_dev);  有一个条件，用户传入过pci，则只创建传入的pci的ctrlr，否则全部创建
												spdk_nvme_ctrlr_get_default_ctrlr_opts				获取默认的ctrlr的opts参数
												probe_ctx->probe_cb(probe_ctx->cb_ctx, trid, &opts) 调用传入的probe_cb，打印了"Attaching to %s\n", trid->traddr
												ctrlr = nvme_transport_ctrlr_construct(trid, &opts, devhandle);  开始创建ctrlr
													spdk_pci_device_claim(pci_dev)										先claim pci设备，保证唯一性
													pctrlr = spdk_zmalloc(sizeof(struct nvme_pcie_ctrlr), 64, NULL,		创建nvme的ctrlr
																			SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_SHARE);
													rc = nvme_ctrlr_construct(&pctrlr->ctrlr);			初始化spdk_nvme_ctrlr的所有信息，初始化ctrlr->ctrlr_lock、active_procs等资源
													rc = nvme_pcie_ctrlr_allocate_bars(pctrlr);
															rc = spdk_pci_device_map_bar(pctrlr->devhandle, 0, &addr, &phys_addr, &size);  把之前映射好的地址读取过来，保存在参数中
															pctrlr->regs = (volatile struct spdk_nvme_registers *)addr;						关联pctrlr的regs和实际的pci的addr
															pctrlr->regs_size = size;														赋值寄存器的size
															nvme_pcie_ctrlr_map_cmb(pctrlr);												把addr、phys_addr、size、offset进行传递
													
													spdk_pci_device_cfg_read16(pci_dev, &cmd_reg, 4);				读取中断管理的fd，写入404，使中断失效
													cmd_reg |= 0x404;
													spdk_pci_device_cfg_write16(pci_dev, cmd_reg, 4);
													
													nvme_ctrlr_get_cap(&pctrlr->ctrlr, &cap)
													nvme_ctrlr_get_vs(&pctrlr->ctrlr, &vs)
													nvme_ctrlr_init_cap(&pctrlr->ctrlr, &cap, &vs);					通过寄存器读取的方式获取cap和vs信息，初始化cap。主要信息是page_size、io_queue_size、io_queue_requests
													rc = nvme_pcie_ctrlr_construct_admin_qpair(&pctrlr->ctrlr, pctrlr->ctrlr.opts.admin_queue_size);         创建管理队列qpair
															pqpair = spdk_zmalloc(sizeof(*pqpair), 64, NULL, SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_SHARE);	使用的是大页内存
															rc = nvme_qpair_init(ctrlr->adminq, 0, /* qpair ID */ ctrlr, SPDK_NVME_QPRIO_URGENT, num_entries);  初始化qpair的下属队列等，0号qpair就是管理队列
															nvme_pcie_qpair_construct(ctrlr->adminq, NULL);				创建其他qpair所需的信息max_completions_cap、tracker等，tracker是一次性分配该qpair的所有数量，因为内存对齐和边界的要求。
																														然后将tracker装入到qpair中的tracker数组指针中进行保存，并插入free_tr中进行记录
															nvme_pcie_qpair_reset(qpair);								qpair中的队列信息清零
													rc = nvme_ctrlr_add_process(&pctrlr->ctrlr, pci_dev);				创建进程信息，用于多进程下的ctrlr管理
													
													ctrlr->remove_cb = probe_ctx->remove_cb;							remove_cb的传入
													ctrlr->cb_ctx = probe_ctx->cb_ctx;
													nvme_qpair_set_state(ctrlr->adminq, NVME_QPAIR_ENABLED);			设置qpair的状态
													TAILQ_INSERT_TAIL(&probe_ctx->init_ctrlrs, ctrlr, tailq);			插入控制器到probe_ctx->init_ctrlrs中，用于后续的状态初始化
										TAILQ_INSERT_TAIL(&g_pci_hotplugged_devices, dev, internal.tailq);				插入到热插拔的队列中
								--------------------------------------------------------------------------------------------------------------------------------
							cleanup_pci_devices();		把新probe的控制器给放入g_pci_devices中管理（此前为空）
						
	nvme_init_controllers(probe_ctx);		
		rc = spdk_nvme_probe_poll_async(probe_ctx);
			------------------------------------------------------------TAILQ_FOREACH_SAFE(ctrlr, &probe_ctx->init_ctrlrs, tailq, ctrlr_tmp) {      对每一个在init_ctrlrs队列中的ctrlr
			nvme_ctrlr_poll_internal(ctrlr, probe_ctx);		对每个在probe_ctx->init_ctrlrs的ctrlr执行，直到该队列为空，设置g_spdk_nvme_driver->initialized = true;返回0
				rc = nvme_ctrlr_process_init(ctrlr);		配置ctrlr的寄存器状态 cc.en，identify 、 construct namespace、 identify namespace 等，直到状态为NVME_CTRLR_STATE_READY才算成功
				TAILQ_REMOVE(&probe_ctx->init_ctrlrs, ctrlr, tailq);								移除初始化队列
				TAILQ_INSERT_TAIL(&g_spdk_nvme_driver->shared_attached_ctrlrs, ctrlr, tailq);		插入到g_spdk_nvme_driver->shared_attached_ctrlrs队列
				nvme_ctrlr_proc_get_ref(ctrlr);														移除inactive的proc，给当前的proc进行active_proc->ref++
				probe_ctx->attach_cb(probe_ctx->cb_ctx, &ctrlr->trid, ctrlr, &ctrlr->opts);			如果attach_cb有效，则进行attach_cb流程
			------------------------------------------------------------	循环执行
------------------------------------------------------------------------------------------------  到此为止example的spdk_nvme_probe流程就结束了 

probe代码详解, 参考文档: https://www.cnblogs.com/vlhn/p/7727016.html
03 - DPDK中环境抽象层（EAL: Environment Abstraction Layer）的函数rte_pci_match()是发现SSD设备的关键。
04 - DPDK的EAL在DPDK架构中所处的位置


from xb:
文档
向nvme设备提交io: doc/nvme_spec.md

git add xx
git commit --amend
git push origin xb
https://github.com/ssbandjl/spdk/blob/xb/doc/nvme_spec.md



bdev_malloc_create

examples/blob/hello_world/hello_blob.c

块设备：
创建daos bdev块设备
daos cont create --pool=test-pool --label=test-cont --type=POSIX
rpc.py bdev_daos_create daosdev0 test-pool test-cont 64 4096  # 块大小为4096， 共64块, 256KB

./scripts/rpc.py bdev_daos_create daosdev0 test-pool test-cont 64 4096  -> from spdk.rpc.client -> 4K * 64 = 256KB -> |4K|4K|...|
SPDK_RPC_REGISTER("bdev_daos_create", rpc_bdev_daos_create, SPDK_RPC_RUNTIME)
rpc_bdev_daos_create -> module/bdev/daos/bdev_daos_rpc.c
  create_bdev_daos
    block_size % 512 512对齐
    daos->disk.fn_table = &daos_fn_table -> static const struct spdk_bdev_fn_table daos_fn_table -> 提交IO请求 -> bdev_daos_submit_request -> ... -> dfs_write ->  src/client/dfs/dfs.c -> daos_array_write
    bdev_get_daos_engine
      daos_init()
    bdev_daos_io_channel_create_cb
      spdk_call_unaffinitized(_bdev_daos_io_channel_create_cb, ch)
        rte_thread_get_affinity(&orig_cpuset)
        spdk_unaffinitize_thread() 移除cpu亲和性
          CPU_ZERO(&new_cpuset)
        _bdev_daos_io_channel_create_cb
          bdev_get_daos_engine
          daos_pool_connect
          daos_cont_open
          dfs_mount(ch->pool, ch->cont, O_RDWR, &ch->dfs)
          dfs_open
          daos_eq_create
        rte_thread_set_affinity(&orig_cpuset) 将cpu亲和性设回去
      ch->poller = SPDK_POLLER_REGISTER(bdev_daos_channel_poll, ch, 0)
    bdev_daos_io_channel_destroy_cb
      spdk_poller_unregister
      daos_eq_destroy
      dfs_release
      dfs_umount
      daos_cont_close
      daos_pool_disconnect
      bdev_daos_put_engine
    spdk_io_device_register(daos, bdev_daos_io_channel_create_cb, bdev_daos_io_channel_destroy_cb -> lib/thread/thread.c
      thread = spdk_get_thread() 此外，在注册 io 设备之前将 UT 更改为 set_thread()
      dev->create_cb = create_cb = bdev_daos_io_channel_create_cb
      dev->destroy_cb = destroy_cb
      tmp = RB_INSERT(io_device_tree, &g_io_devices, dev) -> io_device_tree_RB_INSERT(&g_io_devices, dev) -> 将IO设备对象插入红黑树(g_io_devices全局红黑树头) -> 利用宏生成红黑树插入函数: #define RB_GENERATE_INSERT -> name##_RB_INSERT
    spdk_bdev_register -> lib/bdev/bdev.c -> spdk_bdev_register(struct spdk_bdev *bdev)
      bdev_register(bdev) -> lib/bdev/bdev.c -> bdev_register(struct spdk_bdev *bdev)
        spdk_bdev_get_memory_domains 获取给定 bdev 使用的 SPDK 内存域。 如果 bdev 报告它使用内存域，这意味着它可以使用位于这些内存域中的数据缓冲区。 用户可以调用此函数并将 domains 设置为 NULL 并将 array_size 设置为 0 以获取 bdev 使用的内存域数
        spdk_bdev_is_md_separate 查询元数据是否与块数据交织或与块数据分离
        bdev_alloc_io_stat
        bdev_name_add
        spdk_bdev_get_buf_align
        spdk_io_device_register 将不透明的 io_device 上下文注册为 I/O 设备。 I/O设备注册后，可以使用spdk_get_io_channel()函数返回I/O通道, 许多 bdev 模块创建自己的 bdev 结构，其中 spdk_bdev 作为第一个成员。 bdev.c 当前使用 spdk_bdev 指针作为其 io_device 句柄，强制 bdev 模块选择不同的东西。将其更改为使用 spdk_bdev 指针 + 1 个字节作为其 io_device 句柄。 实际句柄并不重要——它只需要是独一无二的。 这将简化 bdev 模块开发
        TAILQ_INSERT_TAIL(&g_bdev_mgr.bdevs, bdev, internal.link) 上链表
      bdev_open 注册 bdev 时，bdev 模块会在通知 bdev 寄存器之前对其进行检查。检查可能是异步的，例如 当 bdev 模块必须在新的 bdev 上执行 I/O 时。这会导致竞争条件，其中 bdev 可能会在检查未完成时被破坏。 然后，一旦所有模块都发出检查已完成的信号，`bdev_register_finished` 将对释放的 bdev 指针进行无效访问。要解决此问题，请推迟取消注册，直到通过打开 bdev 上的描述符完成检查
      bdev_examine
      spdk_bdev_wait_for_examine(bdev_register_finished, desc) 当所有 bdev 完成检查过程时报告。 注册的 cb_fn 只会被调用一次。 需要再次调用此函数以接收有关检查过程的进一步报告
    *bdev = &(daos->disk)


docker run -it -d --privileged --cap-add=ALL --name spdk eeb6ee3f44bd
docker run -it -d --privileged --cap-add=ALL --name spdk -v /root/project/stor/spdk/xb/spdk:/home/xb/project/stor/spdk/xb/spdk centos:centos7.9.2009
docker start spdk
docker exec -u root -it spdk bash -c 'cd /home/xb/project/stor/spdk/xb/spdk;exec "${SHELL:-sh}"'
docker exec -it spdk /bin/bash

大页管理:
/usr/bin/hugeadm --pool-pages-min DEFAULT:2G -vvv

编译:
./configure --prefix="/opt/daos/prereq/debug/spdk" --enable-debug --disable-tests --disable-unit-tests --disable-apps --without-vhost --without-crypto --without-pmdk --without-rbd --with-rdma --without-iscsi-initiator --without-isal --without-vtune --with-shared --enable-examples
  configure -> common.sh -> CONFIG -> CONFIG.sh

autorun.sh -> autobuild.sh -> meson build-tmp

dpdk debug
dpdkbuild/Makefile
ifeq ($(CONFIG_DEBUG),y)
DPDK_OPTS += --buildtype=debug

dpdk/lib/meson.build -> 
default_cflags += ['-g']
default_cflags += ['-O0']

sed正则表达式:
sed -r 's/CONFIG_([[:alnum:]_]+)=(.*)/CONFIG[\1]=\2/g' $rootdir/CONFIG > $rootdir/CONFIG.sh

nvme-cli, 
https://spdk.io/doc/nvme.html#nvme_cuse
modprobe cuse, lsmod|grep cuse

export HUGE_EVEN_ALLOC=yes   # 如果设置为“是”，大页面将均匀分布在所有“系统的 NUMA 节点（有效地忽略 HUGENODE 中设置的任何内容）”。 使用内核默认的大页面大小
sudo ./scripts/setup.sh
sudo ./build/bin/nvmf_tgt -m [21,22,23,24]
app/nvmf_tgt/nvmf_main.c -> nvmf_tgt -> main
spdk_app_start(&opts, nvmf_tgt_started, NULL) -> nvmf_tgt_started(void *arg1)
  spdk_memzone_dump
  fflush
...

参数说明:
scripts/rpc.py
def nvmf_create_transport

使用提升的权限启动 nvmf_tgt 应用程序。 启动目标后，nvmf_create_transport rpc 可用于初始化给定的传输。 下面是一个示例，其中目标启动并配置了两种不同的传输。 RDMA 传输配置有 8192 字节的 I/O 单元大小、最大 I/O 大小 131072 和 8192 字节的封装数据大小。 TCP 传输配置了 16384 字节的 I/O 单元大小，每个控制器最多 8 个 qpairs，以及 8192 字节的封装数据大小
gdb build/bin/nvmf_tgt
./scripts/rpc.py nvmf_create_transport -t RDMA -u 8192 -i 131072 -c 8192
scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192

#define SPDK_DEFAULT_RPC_ADDR "/var/tmp/spdk.sock"
./scripts/rpc.py nvmf_create_transport -t RDMA -u 8192 -p 4 -c 0
sudo ./scripts/rpc.py nvmf_create_transport -t TCP -u 2097152 -i 2097152
nvmf_create_transport -> lib/nvmf/nvmf_rpc.c -> rpc_nvmf_create_transport(struct spdk_jsonrpc_request *request
  spdk_nvmf_get_tgt(ctx->tgt_name) -> tgt_name ?
    TAILQ_FOREACH(tgt, &g_nvmf_tgts, link)
  spdk_nvmf_transport_opts_init
    nvmf_get_transport_ops
      TAILQ_FOREACH(ops, &g_spdk_nvmf_transport_ops
    nvmf_transport_opts_copy
  spdk_nvmf_tgt_get_transport
    TAILQ_FOREACH(transport, &tgt->transports
  spdk_nvmf_transport_create_async nvmf_rpc_create_transport_done
    nvmf_transport_create
      nvmf_get_transport_ops
      nvmf_transport_opts_copy
      spdk_iobuf_get_opts
      spdk_min
      ctx->cb_fn = cb_fn -> 设置完成回调 -> nvmf_rpc_create_transport_done
      spdk_thread_send_msg(spdk_get_thread(), _nvmf_transport_create_done, ctx) -> msg->fn = fn -> tls_thread
        msg = spdk_mempool_get(g_spdk_msg_mempool)
          rte_mempool_get -> 从内存池中获取一个对象。 该函数调用多消费者或单消费者版本，具体取决于创建内存池时指定的默认行为（请参阅标志）。 如果启用缓存，则将首先从缓存中检索对象，然后从公共池中检索对象。 请注意，当本地缓存和公共池为空时，即使其他 lcore 的缓存已满，它也会返回 -ENOENT, 放回去: spdk_mempool_put
          
      ctx->ops->create_async(&ctx->opts, nvmf_transport_create_async_done, ctx) -> 异步

export C_INCLUDE_PATH=$C_INCLUDE_PATH:/usr/include/openssl11
cp /usr/include/openssl11/openssl/kdf.h /usr/include/openssl/

soft rdma, 软件模拟rdma, 
lsmod |grep rxe
modprobe nvme_rdma
./build/bin/nvmf_tgt

./scripts/rpc.py nvmf_create_transport -t RDMA -u 8192
rdma.c:2484:create_ib_device:

4.通过rpc创建导出bdev
创建内存测试盘
# scripts/rpc.py bdev_malloc_create -b Malloc0 512 512
# scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
# scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 Malloc0
# scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t rdma -a 192.168.80.100 -s 4420

创建nvme盘
# scripts/rpc.py bdev_nvme_attach_controller -b Nvme0 -t PCIe -a 0000:01:00.0
# scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode2 -a -s SPDK00000000000002 -d SPDK_Controller1
# scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode2 Nvme0n1
# scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode2 -t rdma -a 192.168.80.100 -s 4420

客户端
1.load module
# modprobe nvme-rdma

2.discovery
nvme discover -t rdma -a 175.17.53.73 -s 4420
# nvme discover -t rdma -a 192.168.80.100 -s 4420
3.connect
连接cnode1
# nvme connect -t rdma -n "nqn.2016-06.io.spdk:cnode1" -a 192.168.80.100 -s 4420
连接cnode2
# nvme connect -t rdma -n "nqn.2016-06.io.spdk:cnode2" -a 192.168.80.100 -s 4420
# lsblk
4.disconnect
# nvme disconnect -n "nqn.2016-06.io.spdk:cnode1"
# nvme disconnect -n "nqn.2016-06.io.spdk:cnode2"


nvmf_rdma_create
lib/nvmf/rdma.c -> create_ib_device

nvmf_main.c:47
spdk_app_start -> spdk_app_start(struct spdk_app_opts *opts_user
  app_setup_env
  spdk_reactors_init
  spdk_reactors_start
  _reactor_run
  spdk_thread_poll -> thread_poll
  msg_queue_run_batch -> msg->fn(msg->arg)
  _nvmf_transport_create_done -> ctx->cb_fn -> _ctx->ops->create -> .create = nvmf_rdma_create,
  nvmf_rdma_create
    rtransport->event_channel = rdma_create_event_channel()
    rtransport->data_wr_pool = spdk_mempool_create
    rdma_get_devices
    create_ib_device
      ibv_query_device
      nvmf_rdma_is_rxe_device
      TAILQ_INSERT_TAIL(&rtransport->devices, device, link)
      ibv_alloc_pd
      spdk_rdma_create_mem_map
      "Create IB device xxx"
	  rc = generate_poll_fds(rtransport);
    nvmf_rdma_accept

./scripts/rpc.py --verbose=DEBUG nvmf_create_transport -t RDMA
python3 -m pdb scripts/rpc.py nvmf_create_transport -t RDMA
python/spdk/rpc/nvmf.py -> nvmf_create_transport -> def nvmf_create_transport
client.call('nvmf_create_transport', params) -> def call(self, method, params={}) -> flush -> self.sock.sendall(reqstr.encode("utf-8"))
class JSONRPCClient(object) -> __init__ -> self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) -> unix domain socket -> 

__main__
args.client = rpc.client.JSONRPCClient
log_set_level


./build/bin/nvmf_tgt --json bdev_nvme.json   # bdev_nvme.json内容请见附件  # 另开一个终端窗口
./scripts/rpc.py log_set_level "DEBUG"      # 这里最低只能设置"ERROR"
./scripts/rpc.py log_get_level               # 显示当前log等级
./scripts/rpc.py log_set_print_level "DEBUG"  # 设置当前log打印等级
./scripts/rpc.py log_get_print_level         # 显示当前log打印等级

支持的logflag:
./build/bin/nvmf_tgt -h
-L, --logflag <flag>    enable log flag (all, accel, accel_ioat, aio, app_config, app_rpc, bdev, bdev_concat, bdev_daos, bdev_ftl, bdev_malloc, bdev_null, bdev_nvme, bdev_raid, bdev_raid0, bdev_raid1, blob, blob_esnap, blob_rw, blobfs, blobfs_bdev, blobfs_bdev_rpc, blobfs_rw, ftl_core, ftl_init, gpt_parse, ioat, json_util, log, log_rpc, lvol, lvol_rpc, nbd, notify_rpc, nvme, nvme_vfio, nvmf, nvmf_tcp, opal, rdma, reactor, rpc, rpc_client, sock, sock_posix, thread, trace, vbdev_delay, vbdev_gpt, vbdev_lvol, vbdev_opal, vbdev_passthru, vbdev_split, vbdev_zone_block, vfio_pci, vfio_user, virtio, virtio_blk, virtio_dev, virtio_pci, virtio_user, virtio_vfio_user, vmd)

ls -alh /sys/class/nvme
scripts/rpc.py bdev_nvme_attach_controller -b Nvme0 -t PCIe -a 0000:e6:00.0

nvme discover /dev/nvme-fabrics -t rdma -a 192.168.1.110 -s 4420
lsmod |grep nvme
lsmod |grep rdma
modprobe -r nvme_rdma nvme_fabrics
lsmod |grep mlx

./scripts/rpc.py log_set_level "DEBUG" 
./scripts/rpc.py log_set_print_level "DEBUG"

scripts/rpc.py nvmf_create_transport -t RDMA
scripts/rpc.py nvmf_get_transports

scripts/rpc.py bdev_malloc_create -b Malloc0 512 512
scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 Malloc0
scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t rdma -a 175.16.53.73 -s 4420
scripts/rpc.py bdev_nvme_attach_controller -b Nvme0 -t PCIe -a 0000:e6:00.0
scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode2 -a -s SPDK00000000000002 -d SPDK_Controller1
scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode2 Nvme0n1
scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode2 -t rdma -a 175.16.53.73 -s 4420\

scripts/rpc.py bdev_malloc_create -b Malloc0 512 512
scripts/rpc.py nvmf_create_subsystem nqn.2014-08.org.nvmexpress:uuid:a8dce057-b5a2-492e-8da3-9cf328f401c7 -a -s SPDK00000000000001 -d SPDK_Controller1
scripts/rpc.py nvmf_subsystem_add_ns nqn.2014-08.org.nvmexpress:uuid:a8dce057-b5a2-492e-8da3-9cf328f401c7 Malloc0
scripts/rpc.py nvmf_subsystem_add_listener nqn.2014-08.org.nvmexpress:uuid:a8dce057-b5a2-492e-8da3-9cf328f401c7 -t rdma -a 175.16.53.73 -s 4420

scripts/rpc.py bdev_nvme_attach_controller -b Nvme0 -t PCIe -a 0000:e6:00.0
scripts/rpc.py nvmf_create_subsystem nqn.2014-08.org.nvmexpress:uuid:a8dce057-b5a2-492e-8da3-9cf328f401c7 -a -s SPDK00000000000002 -d SPDK_Controller1
scripts/rpc.py nvmf_subsystem_add_ns nqn.2014-08.org.nvmexpress:uuid:a8dce057-b5a2-492e-8da3-9cf328f401c7 Nvme0n1
scripts/rpc.py nvmf_subsystem_add_listener nqn.2014-08.org.nvmexpress:uuid:a8dce057-b5a2-492e-8da3-9cf328f401c7 -t rdma -a 175.16.53.73 -s 4420



modinfo nvme-rdma
nvme list-subsys /dev/nvme0n1


nvme discover --transport=rdma --traddr=192.168.1.3 --hostnqn=host1-rogue-nqn
cat /etc/nvme/discovery.conf



content in test_rdma.conf
`[Nvme]
TransportID "trtype:PCIe traddr:0000:02:00.0" Nvme0

[Transport]


Type RDMA

[Subsystem1]
NQN nqn.2016-06.io.spdk:cnode1
Listen RDMA 192.168.1.110:4420
AllowAnyHost Yes
Host nqn.2016-06.io.spdk:init
SN SPDK00000000000001
MaxNamespaces 20
Namespace Nvme0n1 1`

app/nvmf_tgt/nvmf_tgt -m 15 -c test_rdma.conf

modprobe -r nvme_rdma nvme_fabrics

客户都安执行:
[root@node215 ~]# modinfo nvme-rdma
filename:       /lib/modules/5.10.38-21.01.el7.x86_64/extra/mlnx-nvme/host/nvme-rdma.ko
license:        GPL v2
srcversion:     8817A28E0A60AA47E80648C
depends:        nvme-core,mlx_compat,rdma_cm,ib_core,nvme-fabrics  #看依赖

MLNX_OFED 默认安装什么都不做的虚拟 nvme-rdma 内核模块。
尝试使用 --with-nvmf 安装它：
./mlnxofedinstall --with-nvmf
这应该安装正确的 nvme-rdma 驱动程序。
您可以检查其他 MLNX_OFED 安装选项：
./mlnxofedinstall --help

modprobe nvme_fabrics --force-modversion

modprobe -r nvme-rdma nvme-fabrics nvme nvme-core 然后 modprobe nvme-rdma 或者只是重新启动主机以重新加载所有模块

find / -name mlnxofedinstall
mlnxofedinstall -h

./scripts/rpc.py nvmf_get_stats

modinfo nvmet 和 modinfo nvmet-rdma 的输出——它们的描述中不应有虚拟词, 或dmesg中查看错误



vhost, qemu, io_path
dpdk/examples/vdpa/main.c
rte_vhost_driver_start
vhost_user_start_client
vhost_user_add_connection
fdset_add(&vhost_user.fdset, fd, vhost_user_read_cb
vhost_user_read_cb
vhost_user_msg_handler
dev->notify_ops->new_device(dev->vid)
static const struct rte_vhost_device_ops g_spdk_vhost_ops
.new_device =  start_device
start_device(int vid)
spdk_thread_send_msg(vdev->thread, vhost_user_session_start, vsession) -> vhost_user_session_start(void *arg1)
backend->start_session(vdev, vsession, NULL)
static const struct spdk_vhost_user_dev_backend vhost_blk_user_device_backend
.start_session =  vhost_blk_start -> vhost_blk_start(struct spdk_vhost_dev *vdev 以前 spdk_vhost_dev_backend 持有 vhost_blk 和 vhost_scsi 功能的回调，以及由 vhost_user 后端调用的回调。 此补丁将这些回调分为两个结构：spdk_vhost_dev_backend - 由 vhost_blk 和 vhost_scsi 实现，spdk_vhost_user_dev_backend - 仅由 vhost_user 后端实现，回调特定于该传输的会话管理
  vdev_worker -> vdev_worker(void *arg)



参考流程, https://rootw.github.io/2018/05/SPDK-ioanalyze/
vdev_worker()
    \-process_vq()
        |-spdk_vhost_vq_avail_ring_get()
        \-process_blk_request()
            |-blk_iovs_setup()
            \-spdk_bdev_readv()/spdk_bdev_writev()
                \-spdk_bdev_io_submit()
                    \-bdev->fn_table->submit_request()


DPDK预备知识, 简介: https://mp.weixin.qq.com/s?__biz=MzIyNDU0ODk4OA==&mid=2247483746&idx=1&sn=1913b4bc84e93f3a46738b0e19c045b0&scene=19&key=0af7c27fedbcdd2b966410742eb13bf6af8e65619c30834a90a1a4d74b98613d5f15bc52d6badf7839c6dbc40048b39cffe215b67c97b303edf85fc71c9867828a252095a245ac4ba837a0f826fabdec3e02b2c39f19b287403cd90d50add9309be5cf26e315ca09e30e63112571f8a281fcba8392f8bd85311f155c9eabebf0&ascene=7&uin=NjA2MTUyMDIw&devicetype=Windows+10+x64&version=6309021a&lang=zh_CN&session_us=gh_e0b6b1390d76&countrycode=CN&exportkey=n_ChQIAhIQZMzD%2FYd%2BBaclr%2BU0BgDTphLkAQIE97dBBAEAAAAAABmbMIjFKSIAAAAOpnltbLcz9gKNyK89dVj0xzbpP5qJP91wBjcqFMCWnduSZ%2FOIi1pIvsvTtX2xo2UQhqX9DCSpbWXT%2Fm8jDKEcQVqSW6D7G3Z%2Fshq7agQMW%2FigiULvIsk%2BL2wmfNiTXV3k9G4cZJXjvRldK%2Fwi1EG9YNpvrQ%2FWxMkPTUtrpXmmd%2BxmZmVSjGCS2ZF9Ibe3GFMqN8%2Bv0%2FKCQb9ydCMp9xKnKKFR%2BXQ5af30OpctKZm%2BoBNULjbM3%2FX7cw34jjLylO2s2zWPyZWaVPDsnHgnKQ%3D%3D&acctmode=0&pass_ticket=%2B2fKFckRRXpy%2B4qNFfX6EiGulJ0Kj5fByeaAl97LrJFxsqIODZxD9OJFDdcnOoCGK6L1bcvrzi66DtH5PKX6oQ%3D%3D&wx_header=1&fontgear=2


DPDK支持多线程编程，以lcore表示一个CPU。DPDK启动时，会自动检测系统中的CPU信息，并根据用户指定的-c参数（以hex值表示cpu mask信息）运行线程。为了方便线程间的通信和管理，DPDK将这些线程分为一个Master线程和其他Slave线程。Master线程是用户运行程序的主进程，Slave线程用来运行用户自定义的程序。通过这个方式，省去了用户自己创建和管理线程的步骤。下图是SPDK用这个模型跑的一个例子（见examples/perf）
examples/nvme/perf/perf.c -> main(int argc, char **argv)
spdk_env_init -> spdk_env_init(const struct spdk_env_opts *opts)
  rte_eal_init(g_eal_cmdline_argcount, dpdk_args)
    pthread_create eal_thread_loop
      rte_eal_trace_thread_lcore_ready
    rte_eal_cpu_init
    rte_eal_remote_launch
      eal_thread_wake_worker

PMD，即Poll Mode Driver, 
要编写PCIe驱动，必须能够访问BAR空间。对于用户态的程序来说，是不能直接访问BAR空间的，DPDK使用UIO来达到这个目的。UIO还有一个特点就是让用户态的驱动也可以使用中断（Linux系统中，中断只能通过内核线程处理，UIO也只是通过内核函数设置一个计数器，当用户态的程序读这个计数器的时候就知道有没有新的中断了，DPDK据此特别启用了一个线程来管理中断）。用户可以通过uio_pci_generic申明一个PCIe设备作为UIO注册到PCI Bus上。uio_pci_generic默认编译到内核中，跟前一期讲的内核NVMe驱动是同等类型（pci_driver），可以通过modprobe加载。但是uio_pci_generic没有匹配特别的device，需要用户手动配置。SPDK有一个setup脚本，这个脚本会把设备与内核的NVMe驱动unbind，再绑定到uio_pci_generic上。
当把nvme设备bind到uio_pci_generic上后，PCI bus就会去调用它的probe函数。uio_pci_generic的probe函数如下，注册了UIO设备
bind到uio_pci_generic的设备会出现在/dev目录下，此后，就可以通过/dev/uio#操作设备了, 
查看驱动: ls -alh /sys/bus/pci/drivers/uio_pci_generic/
查看uio设备: ls -alh /dev/uio0

rte_eal_pci_probe

下面从这两个方面分析SPDK NVMe驱动。为了方面理解，我们以SPDK的一个例子spdk/examples/nvme/identify来说明。这个例子能够打印绑定UIO的NVMe设备信息。如下图，在Main()函数中，首先调用了rte_eal_init()，然后调用spdk_nvme_probe()，并且传递了probe_cb和attach_cb两个callback函数（由用户指定）。probe_cb中判断这个设备是否加载在UIO上，如果不是就报错。attach_cb则把SPDK定义的controller对象传给print_controller()处理，print_controller提取这个controller的信息打印出来
examples/nvme/identify/identify.c

spdk, nvme, 接口, 函数, nvme命令
include/spdk/nvme.h



(gdb) bt
#0  spdk_nvmf_tgt_create (opts=0x7fffffffd4d0) at nvmf.c:269
#1  0x00000000004c45b4 in nvmf_tgt_create_target () at nvmf_tgt.c:311
#2  0x00000000004c495b in nvmf_tgt_advance_state () at nvmf_tgt.c:418
#3  0x00000000004c4b34 in nvmf_subsystem_init () at nvmf_tgt.c:492
#4  0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#5  0x0000000000561da2 in bdev_initialize_complete (cb_arg=0x0, rc=0) at bdev.c:18
#6  0x0000000000567629 in bdev_init_complete (rc=0) at bdev.c:1953
#7  0x00000000005676a2 in bdev_module_action_complete () at bdev.c:1995
#8  0x0000000000567b19 in spdk_bdev_initialize (cb_fn=0x561d89 <bdev_initialize_complete>, cb_arg=0x0) at bdev.c:2126
#9  0x0000000000561db9 in bdev_subsystem_initialize () at bdev.c:24
#10 0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#11 0x000000000057f014 in accel_subsystem_initialize () at accel.c:20
#12 0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#13 0x000000000058fad3 in iobuf_subsystem_initialize () at iobuf.c:57
#14 0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#15 0x000000000058d167 in sock_subsystem_init () at sock.c:13
#16 0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#17 0x0000000000588587 in vmd_subsystem_init () at vmd.c:63
#18 0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#19 0x0000000000555bfb in scheduler_subsystem_init () at scheduler.c:23
#20 0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#21 0x000000000059172c in spdk_subsystem_init (cb_fn=0x5562f1 <app_start_rpc>, cb_arg=0x0) at subsystem.c:199
#22 0x0000000000556c39 in bootstrap_fn (arg1=0x0) at app.c:498
#23 0x0000000000598404 in msg_queue_run_batch (thread=0xd10150, max_msgs=8) at thread.c:832
#24 0x0000000000598d57 in thread_poll (thread=0xd10150, max_msgs=0, now=74054886509296) at thread.c:1054
#25 0x0000000000599057 in spdk_thread_poll (thread=0xd10150, max_msgs=0, now=74054886509296) at thread.c:1147
#26 0x000000000055b162 in _reactor_run (reactor=0xd0fd00) at reactor.c:914
#27 0x000000000055b251 in reactor_run (arg=0xd0fd00) at reactor.c:952
#28 0x000000000055b6c7 in spdk_reactors_start () at reactor.c:1068
#29 0x0000000000557b65 in spdk_app_start (opts_user=0x7fffffffdee0, start_fn=0x407755 <nvmf_tgt_started>, arg1=0x0) at app.c:808
#30 0x000000000040785d in main (argc=1, argv=0x7fffffffe0a8) at nvmf_main.c:47


nvmf_add_discovery_subsystem 创建 discovery NVM subsystem， 这个subsystem一般设置为给所有的host可见。其主要用于实现相应的log discovery命令，告诉host端有多少NVM subsystem在线。当然这个实际会存储在g_spdk_nvmf_tgt中的变量discovery_log_page中 (如表1所示)，并且会做相应的更新
  spdk_nvmf_subsystem_create
  spdk_nvmf_subsystem_set_allow_any_host


nvmf_rdma_connect
  spdk_nvmf_tgt_new_qpair

nvme_rdma_poll_group


spdk_nvmf_request_exec_fabrics
spdk_nvmf_request_exec
  nvmf_ctrlr_process_fabrics_cmd