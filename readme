spdk_tgt源码分析, SPDK NVMe-oF target代码分析（转载+改编）: https://www.cnblogs.com/whl320124/articles/10450250.html
initiator_spdk_tgt, spdk-nvmf指南: https://blog.csdn.net/zxpoiu/article/details/116003444
搭建远端存储，深度解读SPDK NVMe-oF target: https://mp.weixin.qq.com/s?__biz=MzI3NDA4ODY4MA==&mid=2653335091&idx=1&sn=81344e9239956c5045cee444073ec24b&chksm=f0cb59b4c7bcd0a21d2179b09a4e242457b7bca877120512cbea0f5ca83bf4ecc8777ce3f43f&scene=21#wechat_redirect
使用SPDK lib搭建自己的NVMe-oF Target应用: https://blog.csdn.net/weixin_60043341/article/details/126505064
SPDK线程模型解析: https://blog.csdn.net/weixin_37097605/article/details/101488760
spdk api文档: https://spdk.io/doc/jsonrpc.html
jsonrpc.md

log日志, SPDK Trace Log用法简介: https://zhuanlan.zhihu.com/p/556631633
提交io到nvme设备: https://mp.weixin.qq.com/s/HnrBHknlKtGpLprCmJQxIw

submodul_and_spdk_obj: https://portal.qiniu.com/kodo/bucket/resource-v2?bucketName=sxb-obj

git submodule update --init --recursive

常用命令:
查看线程状态: ./scripts/rpc.py -s /var/tmp/spdk_xb.sock thread_get_stats
lscpu
cat /proc/cpuinfo 查看cpu核数(逻辑) lcore,core_id对应


spdk_tgt, 及 SPDK Vhost target提供了VM中virtio-blk/scsi半虚拟化IO请求的加速器，相比较原来的QEMU virtio-blk方案以及kernel vhost-scsi方案极大的提高了性能，VM内置的内核半虚拟化驱动即可使用

spdk

shell巧用数组:
sed -r 's/CONFIG_([[:alnum:]_]+)=(.*)/CONFIG[\1]=\2/g' $rootdir/CONFIG > $rootdir/CONFIG.sh
declare -A CONFIG
source $rootdir/CONFIG.sh

配置：将长选项中的 ~ 文字视为用户的 $HOME


编译, 安装依赖, 安装日志: spdk_install_dep_log
git clone https://github.com/spdk/spdk
cd spdk
git submodule update --init
yum install -y vim git

# 安装依赖
sudo scripts/pkgdep.sh
sudo scripts/pkgdep.sh --all

./configure # ./configure --enable-lto --enable-werror
make

# 捕获异常
trap 'set +e; trap - ERR; echo "Error!"; exit 1;' ERR


io路径
nvme_initiator -> nvmf_tgt
...
nvme_rdma_queue_rq
...
ibv_post_send
-------------- nvme_initiator ----------------
ibv_poll_cq
...


-------------- nvmf_tgt(spdk_tgt) ----------------





io路径
hello_world.c
参考源码分析: https://blog.csdn.net/wade1010/article/details/128782710
demo: 从hello_world程序来剖析SDPK NVMe用户态驱动，对NVMe盘的初始化、管理和读写操作
examples/nvme/hello_world/hello_world.c -> main
spdk_env_opts_init(&opts)
main
  parse_args
  spdk_env_opts_init -> pts->reactor_mask = optarg -> -m 0xFFFFFFFF
  spdk_env_init
    build_eal_cmdline
    rte_eal_init
    spdk_env_dpdk_post_init
    dpdk_args = calloc(g_eal_cmdline_argcount, sizeof(char *))  二级指针分配
    rte_eal_init(g_eal_cmdline_argcount, dpdk_args)
    spdk_env_dpdk_post_init
      pci_env_init
        TAILQ_FOREACH
        register_rte_driver(driver)
        _pci_env_init()
      mem_map_init
        spdk_mem_map_alloc
        rte_mem_event_callback_register("spdk", memory_hotplug_cb, NULL)
        rte_memseg_contig_walk(memory_iter_cb, NULL)
      vtophys_init
        vtophys_notify
        vtophys_check_contiguous_entries
        spdk_mem_map_alloc
  spdk_vmd_init
    spdk_pci_enumerate(spdk_pci_vmd_get_driver(), vmd_enum_cb, &g_vmd_container)
  spdk_nvme_probe(NULL, NULL, probe_cb, attach_cb, NULL)
    spdk_nvme_trid_populate_transport
    spdk_nvme_probe_async
    nvme_init_controllers
  hello_world
    spdk_nvme_ctrlr_alloc_io_qpair
    spdk_nvme_ctrlr_map_cmb LBA start  number of LBAs 
    spdk_nvme_ns_cmd_write
    write_complete
    spdk_nvme_qpair_process_completions
  spdk_vmd_fini

example
打印所有nvme设备： build/examples/identify.c

app more example




from xb:
文档
向nvme设备提交io: doc/nvme_spec.md

git add xx
git commit --amend
git push origin xb
https://github.com/ssbandjl/spdk/blob/xb/doc/nvme_spec.md




examples/blob/hello_world/hello_blob.c





docker run -it -d --privileged --cap-add=ALL --name spdk eeb6ee3f44bd
docker run -it -d --privileged --cap-add=ALL --name spdk -v /root/project/stor/spdk/xb/spdk:/home/xb/project/stor/spdk/xb/spdk centos:centos7.9.2009
docker start spdk
docker exec -u root -it spdk bash -c 'cd /home/xb/project/stor/spdk/xb/spdk;exec "${SHELL:-sh}"'
docker exec -it spdk /bin/bash



daos块设备, https://docs.daos.io/v2.3/user/blockdev/
POSIX 容器可以通过 NVMe-oF 协议导出块设备。 这需要在第三方节点（例如专用节点或在客户端节点或 DAOS 存储节点上的特定核心上运行）上设置单独的 SPDK 服务，将 DAOS 容器导出为 NVMe 目标。 本节介绍如何配置 SPDK DAOS bdev 并通过 NVMe-oF 协议访问它。 它假定已经配置了 DAOS 系统。

git clone https://github.com/spdk/spdk.git
git submodule update --init
./configure --with-daos
make -j 16


sudo HUGE_EVEN_ALLOC=yes scripts/setup.sh
sudo ./build/bin/nvmf_tgt -m [21,22,23,24]
$ sudo ./build/bin/nvmf_tgt -m [21,22,23,24]
[2023-04-21 09:09:40.791150] Starting SPDK v23.05-pre git sha1 26b9be752 / DPDK 22.11.1 initialization...
[2023-04-21 09:09:40.791194] [ DPDK EAL parameters: nvmf --no-shconf -l 21,22,23,24 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid747434 ]
TELEMETRY: No legacy callbacks, legacy socket not created
[2023-04-21 09:09:40.830768] app.c: 738:spdk_app_start: *NOTICE*: Total cores available: 4
[2023-04-21 09:09:40.859580] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 22
[2023-04-21 09:09:40.859716] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 23
[2023-04-21 09:09:40.859843] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 24
[2023-04-21 09:09:40.859844] reactor.c: 937:reactor_run: *NOTICE*: Reactor started on core 21
[2023-04-21 09:09:40.878692] accel_sw.c: 601:sw_accel_module_init: *NOTICE*: Accel framework software module initialized.

POOL_UUID="${POOL:-pool_label}"
CONT_UUID="${CONT:-const_label}"
DISK_UUID="${UUID:-`uuidgen`}"
NR_DISKS="${1:-1}"
BIND_IP="${TARGET_IP:-172.31.91.61}"




for i in $(seq 1 "$NR_DISKS"); do
    sudo ./scripts/rpc.py bdev_daos_create disk$i ${POOL_UUID} ${CONT_UUID} 1048576 4096 --uuid ${DISK_UUID}
    subsystem=nqn.2016-06.io.spdk$i:cnode$i
    sudo scripts/rpc.py nvmf_create_subsystem $subsystem -a -s SPDK0000000000000$i -d SPDK_Virtual_Controller_$i
    sudo scripts/rpc.py nvmf_subsystem_add_ns $subsystem  disk$i
    sudo scripts/rpc.py nvmf_subsystem_add_listener $subsystem -t tcp -a ${BIND_IP} -s 4420
done

/opt/xxx/script/tgt/rpc.py nvmf_subsystem_add_listener nqn.2014-08.org.nvmexpress.discovery -t rdma -a 172.17.29.217 -s 4420

块设备：
创建daos bdev块设备
daos cont create --pool=test-pool --label=test-cont --type=POSIX
rpc.py bdev_daos_create daosdev0 test-pool test-cont 64 4096  # 块大小为4096， 共64块, 256KB


spdk块设备主流程
nvmf_tgt
nvmf_create_transport
bdev_daos_create
nvmf_create_subsystem
nvmf_subsystem_add_ns
nvmf_subsystem_add_listener


test_daos
cc -o /dev/null -x c -march=native -I/usr/local/include -L/usr/local/lib -I/opt/daos/include -L/opt/daos/lib64 -lgurt -ldaos -ldaos_common -ldfs test_daos_dfs.c

--with-shared

--with-daos=--with-daos=/path/to/daos/install/dir

打开另一个终端进行配置过程。 配置是通过 scripts/rpc.py 脚本完成的，之后可以将其转储到 json 文件中，稍后可以将其传递给 nvmf_tgt。 创建由 DAOS DFS 备份的几个磁盘的最短方法是使用以下脚本（称为 export_disk.sh）


sudo nvme connect-all -t tcp -a 172.31.91.61 -s 4420
$ sudo umount /testfs
$ sudo mkfs.ext4 -F -O mmp /dev/nvme1n1
$ sudo e2mmpstatus /dev/nvme1n1
e2mmpstatus: it is safe to mount '/dev/nvme1n1', MMP is clean
$ sudo mount /dev/nvme1n1 /testfs
$ sudo nvme disconnect-all


./scripts/setup.sh
./scripts/setup.sh reset

qa:
spdk找不到 ncurses.h, spdk_top.c:52:21: fatal error: ncurses.h: No such file or directory
yum install ncurses-devel

编译spdk:
cd build/external/debug/spdk
export CFLAGS='-O0 -g'
./configure --help
yum install ncurses-devel ncurses
./configure --prefix="/opt/daos/prereq/debug/spdk" --enable-debug --disable-tests --disable-unit-tests --disable-apps --without-vhost --without-crypto --without-pmdk --without-rbd --with-rdma --without-iscsi-initiator --without-isal --without-vtune --with-shared --enable-examples

Running commands in /home/xb/project/stor/daos/origin/docker/daos
command:cp -r cache/spdk /home/xb/project/stor/daos/origin/docker/daos/build/external/debug/spdk
Running commands in /home/xb/project/stor/daos/origin/docker/daos/build/external/debug/spdk
command:git checkout v21.07

编译选项:
reqs.define('spdk',
            retriever=retriever,
            commands=['./configure --prefix="$SPDK_PREFIX"'                \
                      ' --disable-tests --disable-unit-tests '             \
                      ' --disable-apps --without-vhost '                   \
                      ' --without-crypto --without-pmdk --without-rbd '    \
                      ' --with-rdma --without-iscsi-initiator '            \
                      ' --without-isal --without-vtune --with-shared',
                      'make $JOBS_OPT', 'make install',
                      'cp -r -P dpdk/build/lib/* "$SPDK_PREFIX/lib"',
                      'mkdir -p "$SPDK_PREFIX/include/dpdk"',
                      'cp -r -P dpdk/build/include/* '                     \
                      '"$SPDK_PREFIX/include/dpdk"',
                      'mkdir -p "$SPDK_PREFIX/share/spdk"',
                      'cp -r include scripts "$SPDK_PREFIX/share/spdk"'],
            headers=['spdk/nvme.h', 'dpdk/rte_eal.h'],
            extra_include_path=['/usr/include/dpdk',
                                '$SPDK_PREFIX/include/dpdk',
                                # debian dpdk rpm puts rte_config.h here
                                '/usr/include/x86_64-linux-gnu/dpdk'],
            patch_rpath=['lib'])


ips="182.200.53.61 182.200.53.62"
for ip in $ips; do 
  rsync -rapvu /opt/daos root@$ip:/opt/
done


command:cp -r -P dpdk/build/lib/* "/opt/daos/prereq/debug/spdk/lib"
command:mkdir -p "/opt/daos/prereq/debug/spdk/include/dpdk"
command:cp -r -P dpdk/build/include/* "/opt/daos/prereq/debug/spdk/include/dpdk"
command:mkdir -p "/opt/daos/prereq/debug/spdk/share/spdk"
command:cp -r include scripts "/opt/daos/prereq/debug/spdk/share/spdk"
Running commands in /home/xb/project/stor/daos/origin/docker/daos
command:patchelf --set-rpath '$ORIGIN:/opt/daos/prereq/debug/spdk/lib' /opt/daos/prereq/debug/spdk/lib/librte_bus_pci.so
Running commands in /home/xb/project/stor/daos/origin/docker/daos
command:patchelf --set-rpath '$ORIGIN:/opt/daos/prereq/debug/spdk/lib' /opt/daos/prereq/debug/spdk/lib/librte_bus_vdev.so
Running commands in /home/xb/project/stor/daos/origin/docker/daos
command:patchelf --set-rpath '$ORIGIN:/opt/daos/prereq/debug/spdk/lib' /opt/daos/prereq/debug/spdk/lib/librte_cmdline.so


spdk

编译, 安装依赖, 安装日志: spdk_install_dep_log
git clone https://github.com/spdk/spdk
cd spdk
git submodule update --init
yum install -y vim git

sudo scripts/pkgdep.sh
sudo scripts/pkgdep.sh --all

./configure
make

# 捕获异常
trap 'set +e; trap - ERR; echo "Error!"; exit 1;' ERR



参考源码分析: https://blog.csdn.net/wade1010/article/details/128782710, https://blog.csdn.net/weixin_44879249/article/details/110920401
demo: 从hello_world程序来剖析SDPK NVMe用户态驱动，对NVMe盘的初始化、管理和读写操作
export LD_LIBRARY_PATH=/opt/daos/prereq/debug/spdk/lib:$LD_LIBRARY_PATH
cd build/examples
gdb hello_world

examples/nvme/hello_world/hello_world.c -> main
spdk_env_opts_init(&opts)
main
  parse_args
  spdk_env_opts_init
  spdk_env_init -> spdk_env_init(const struct spdk_env_opts *opts)
    if (g_external_init == false) g_external_init=true
    build_eal_cmdline -> build_eal_cmdline(const struct spdk_env_opts *opts)
      push_arg -> EAL参数如下:
      -c 0x1
      --log-level=lib.eal:6
      --log-level=lib.cryptodev:5
      --log-level=user1:6
      --iova-mode=pa
      --base-virtaddr=0x200000000000
      --match-allocations
      --file-prefix=spdk0
      --proc-type=auto
    rte_eal_init -> rte_eal_init(int argc, char **argv) 初始化环境抽象层 (EAL)。 此函数将仅在 MAIN lcore 上执行，并尽快在应用程序的 main() 函数中执行。 它将 WORKER lcores 置于 WAIT 状态, 代码流程: https://blog.csdn.net/armlinuxww/article/details/90288725, https://zhuanlan.zhihu.com/p/439119807
      rte_eal_get_configuration
      eal_get_internal_configuration
      rte_eal_cpu_init
      eal_parse_args
      rte_config_init
      rte_bus_scan
      is_iommu_enabled
      rte_eal_iova_mode
      rte_eal_memory_init ...
    spdk_env_dpdk_post_init -> spdk_env_dpdk_post_init(bool legacy_mem)
      pci_env_init() -> pci_env_init(void)
        TAILQ_FOREACH(driver, &g_pci_drivers, tailq)
          register_rte_driver -> register_rte_driver(struct spdk_pci_driver *driver) 将 spdk_pci_driver 转换为 rte_pci_driver 并将其注册到 dpdk
        _pci_env_init -> _pci_env_init(void)
          scan_pci_bus -> scan_pci_bus(bool delay_init)
            rte_bus_scan
            spdk_get_ticks...
          spdk_process_is_primary 热移除
      mem_map_init -> mem_map_init(bool legacy_mem)
        spdk_mem_map_alloc
        rte_mem_event_callback_register
        rte_memseg_contig_walk
      vtophys_init -> vtophys_init(void)
        vtophys_iommu_init
    dpdk_args = calloc(g_eal_cmdline_argcount, sizeof(char *))  二级指针分配
    rte_eal_init(g_eal_cmdline_argcount, dpdk_args)
    spdk_env_dpdk_post_init
      pci_env_init
        TAILQ_FOREACH
        register_rte_driver(driver)
        _pci_env_init()
      mem_map_init
        spdk_mem_map_alloc
        rte_mem_event_callback_register("spdk", memory_hotplug_cb, NULL)
        rte_memseg_contig_walk(memory_iter_cb, NULL)
      vtophys_init
        vtophys_notify
        vtophys_check_contiguous_entries
        spdk_mem_map_alloc
  spdk_vmd_init
    spdk_pci_enumerate(spdk_pci_vmd_get_driver(), vmd_enum_cb, &g_vmd_container)
  spdk_nvme_probe(NULL, NULL, probe_cb, attach_cb, NULL) 枚举传输 ID 指示的总线，并在需要时将用户空间 NVMe 驱动程序附加到找到的每个设备。 此函数不是线程安全的，一次只能从一个线程调用，而没有其他线程正在主动使用任何 NVMe 设备。 如果从辅助进程调用，则只会探测已附加到主进程中的用户空间驱动程序的设备。 如果多次调用，则只会报告尚未附加到 SPDK NVMe 驱动程序的设备。 要停止使用控制器并释放其关联资源，请使用 attach_cb() 函数中的 spdk_nvme_ctrlr 实例调用 spdk_nvme_detach(), 发现ssd, 参考: https://zhuanlan.zhihu.com/p/555930316, https://www.cnblogs.com/vlhn/p/7727016.html, proby__by_class_code, Class Code (SPDK_PCI_CLASS_NVME=0x010802)扯上了关系。 全局变量g_nvme_pci_drv就是在L53行定义的，而g_nvme_pci_drv.driver.id_table则是在L38行定义的
  spdk_nvme_probe(NULL, NULL, probe_cb, attach_cb, NULL) 枚举传输 ID 指示的总线，并在需要时将用户空间 NVMe 驱动程序附加到找到的每个设备。 此函数不是线程安全的，一次只能从一个线程调用，而没有其他线程正在主动使用任何 NVMe 设备。 如果从辅助进程调用，则只会探测已附加到主进程中的用户空间驱动程序的设备。 如果多次调用，则只会报告尚未附加到 SPDK NVMe 驱动程序的设备。 要停止使用控制器并释放其关联资源，请使用 attach_cb() 函数中的 spdk_nvme_ctrlr 实例调用 spdk_nvme_detach(), 发现ssd, 参考: https://zhuanlan.zhihu.com/p/555930316, https://www.cnblogs.com/vlhn/p/7727016.html, proby__by_class_code, Class Code (SPDK_PCI_CLASS_NVME=0x010802)扯上了关系。 全局变量g_nvme_pci_drv就是在L53行定义的，而g_nvme_pci_drv.driver.id_table则是在L38行定义的, 查看内存: lshw -short -C memory, lspci|grep -i non, nvme-cli
    spdk_nvme_trid_populate_transport
    spdk_nvme_probe_async
    nvme_init_controllers... spdk_pci_enumerate -> rte_pci_probe -> pci_probe_all_drivers -> rte_pci_probe_one_driver -> rte_pci_match
      if (id_table->class_id != pci_dev->id.class_id...
  hello_world -> hello_world(void)
    spdk_nvme_ctrlr_alloc_io_qpair 分配一个nvme queue pair作为该IO Channel的实际对象, 分配一个 I/O qpair，我们可以使用它来向控制器上的命名空间提交读/写请求。 NVMe 控制器通常支持每个控制器的许多 qpairs。 为控制器分配的任何 I/O qpair 都可以将 I/O 提交到该控制器上的任何命名空间。
SPDK NVMe 驱动程序不为 qpair 访问提供同步 - 应用程序必须确保只有一个线程将 I/O 提交给 qpair，并且同一线程还必须检查该 qpair 上的完成。 这通过使所有 I/O 操作完全无锁来实现极其高效的 I/O 处理, 分配一个 I/O 队列对（提交和完成队列）。 默认情况下，此函数还执行新创建的 qpair 所需的任何连接活动。 为避免这种行为，用户应将 opts 结构中的 create_only 标志设置为 true。 每个队列对一次只能在一个线程中使用（用户必须强制执行互斥）
    spdk_nvme_ctrlr_map_cmb LBA start  number of LBAs, 映射先前保留的控制器内存缓冲区，以便它的数据对 CPU 可见。 此操作并不总是可行的, 从控制器中申请内存, 设备内存(DRAM), Controller Memory Buffer(CMB)，可用于保存SQ和CQ队列元素，也可用于App buf(借助spdk_nvme_ctrlr_map_cmb函数)，从而节省了部分DMA操作(从主存拷贝数据到控制器)，降低了PCIe switch的路由开销, 如果ctrlr中的buf不够则从主机内存中去分配
    spdk_zmalloc 根据给定的 dma_flg 分配 dma/可共享内存。 它是一个具有给定大小、对齐方式和套接字 ID 的内存缓冲区。 此外，缓冲区将被清零
    snprintf(sequence.buf, 0x1000, "%s", "Hello world!\n") 将字符串打印到buf上(4KB)
    spdk_nvme_ns_cmd_write -> spdk_nvme_ns_cmd_write(struct spdk_nvme_ns *ns
    _nvme_ns_cmd_rw -> _nvme_ns_cmd_rw(struct spdk_nvme_ns *ns... SPDK_NVME_OPC_WRITE
    nvme_qpair_submit_request
    write_complete
    spdk_nvme_qpair_process_completions
  spdk_vmd_fini

example
打印所有nvme设备： build/examples/identify.c

app more example

发现nvme设备(spdk_nvme_probe)
发现SSD设备的时候，从SPDK进入到DPDK中，函数调用栈为
00 hello_world.c
01 -> main()
02 --> spdk_nvme_probe()
03 ---> spdk_nvme_probe_async()
04 ----> nvme_probe_internal()
05 -----> nvme_transport_ctrlr_scan()
06 -----> nvme_pcie_ctrlr_scan()  
07 ------> spdk_pci_enumerate()		
08 ------> spdk_pci_enumerate()
09 ------> rte_bus_probe()				             			    | SPDK |
   =========================================================================
10 -------> mlx5_common_dev_probe()                                 | DPDK |
11 --------> drivers_probe()
12 ---------> mlx5_bus_match()
13 ----------> mlx5_dev_pci_match()

probe流程详解:
struct spdk_nvme_probe_ctx {
  struct spdk_nvme_transport_id		trid;
  void					*cb_ctx;
  spdk_nvme_probe_cb			probe_cb;
  spdk_nvme_attach_cb			attach_cb;
  spdk_nvme_remove_cb			remove_cb;
  TAILQ_HEAD(, spdk_nvme_ctrlr)		init_ctrlrs;
};

rc = spdk_nvme_probe(NULL, NULL, probe_cb, attach_cb, NULL);  		20.02版本的调用	if (spdk_nvme_probe(NULL, probe_ctx, probe_cb, attach_cb, remove_cb)) 		20.10中已删除	probe_cb	
  spdk_nvme_trid_populate_transport 初始化trid  设置pcie类型SPDK_NVME_TRANSPORT_NAME_PCIE
  probe_ctx = spdk_nvme_probe_async(trid, cb_ctx, probe_cb, attach_cb, remove_cb);  开始进行probe_ctx的初始化
        nvme_driver_init  初始化g_spdk_nvme_driver的全局变量，其中包括锁，driver队列，热插拔fd的connect获取，uuid的初始化  只允许一个进程去做，加锁，避免引起混乱
        probe_ctx = calloc(1, sizeof(*probe_ctx));  创建上下文 类型为：struct spdk_nvme_probe_ctx 
        nvme_probe_ctx_init(probe_ctx, trid, cb_ctx, probe_cb, attach_cb, remove_cb);  变量的赋值，初始化probe_ctx->init_ctrlrs的队列，用来存放需要初始化的nvme 控制器
        rc = nvme_probe_internal(probe_ctx, false); 
          nvme_transport_ctrlr_scan(probe_ctx, direct_connect)； 
          nvme_pcie_ctrlr_scan
            enum_ctx.probe_ctx = probe_ctx;
            bool enum_ctx.has_pci_addr ？ 判断probe_ctx->trid.traddr中内容是否为空， 不为空，说明有特殊指定pci，则调用spdk_pci_device_attach，为空说明没有特殊指定pci  则调用spdk_pci_enumerate
            spdk_pci_enumerate(spdk_pci_nvme_get_driver(), pcie_nvme_enum_cb, &enum_ctx); 本example传入的参数为NULL，则直接执行enumerate流程
              cleanup_pci_devices  清除pci设备，状态dev->internal.removed为true的，从g_pci_devices中移除，在热插拔链表g_pci_hotplugged_devices中遍历，移除该队列并插入到g_pci_devices的尾部
              TAILQ_FOREACH(dev, &g_pci_devices, internal.tailq) {  开始遍历g_pci_devices，过程中需要加g_pci_mutex锁，防止列表信息变化。 实际上此时g_pci_devices为空，直接退出
              }
              scan_pci_bus					重新把所有的bus上的设备扫一遍，主要是pci bus
              driver->cb_fn = enum_cb;
              driver->cb_arg = enum_ctx;      把pcie_nvme_enum_cb和arg传入作为nvme driver的回调
              rte_bus_probe					调用pci_probe 把pci设备进行绑定
                ----------------------------------------------------------------------------------------------------------一个个匹配，所以以下流程对于满足条件的pci设备会走多次
                rte_pci_probe_one_driver	device和驱动匹配上，如果不是匹配的，则退出。
                rte_pci_map_device			进行pci地址映射，以在用户空间访问
                ret = dr->probe(dr, dev);	调用pci_device_init函数，作为driver的probe。  pci_env_init函数中进行的probe函数的指定
                  pci_device_init			
                    rc = driver->cb_fn(driver->cb_arg, dev);  主要进行addr等基础信息的赋值传递，同时执行一开始传入的回调函数
                    pcie_nvme_enum_cb						
                      nvme_get_ctrlr_by_trid_unsafe		去g_nvme_attached_ctrlrs和g_spdk_nvme_driver->shared_attached_ctrlrs两个链表中搜索ctrlr，用来进行判断是否已创建
                      nvme_ctrlr_probe(&trid, enum_ctx->probe_ctx, pci_dev);  有一个条件，用户传入过pci，则只创建传入的pci的ctrlr，否则全部创建
                        spdk_nvme_ctrlr_get_default_ctrlr_opts				获取默认的ctrlr的opts参数
                        probe_ctx->probe_cb(probe_ctx->cb_ctx, trid, &opts) 调用传入的probe_cb，打印了"Attaching to %s\n", trid->traddr
                        ctrlr = nvme_transport_ctrlr_construct(trid, &opts, devhandle);  开始创建ctrlr
                          spdk_pci_device_claim(pci_dev)										先claim pci设备，保证唯一性
                          pctrlr = spdk_zmalloc(sizeof(struct nvme_pcie_ctrlr), 64, NULL,		创建nvme的ctrlr, 按64B对齐
                                      SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_SHARE);
                          rc = nvme_ctrlr_construct(&pctrlr->ctrlr);			初始化spdk_nvme_ctrlr的所有信息，初始化ctrlr->ctrlr_lock、active_procs等资源
                          rc = nvme_pcie_ctrlr_allocate_bars(pctrlr);
                              rc = spdk_pci_device_map_bar(pctrlr->devhandle, 0, &addr, &phys_addr, &size);  把之前映射好的地址读取过来，保存在参数中
                              pctrlr->regs = (volatile struct spdk_nvme_registers *)addr;						关联pctrlr的regs和实际的pci的addr
                              pctrlr->regs_size = size;														赋值寄存器的size
                              nvme_pcie_ctrlr_map_cmb(pctrlr);												把addr、phys_addr、size、offset进行传递
                          spdk_pci_device_cfg_read16(pci_dev, &cmd_reg, 4);				读取中断管理的fd，写入404，使中断失效
                          cmd_reg |= 0x404;
                          spdk_pci_device_cfg_write16(pci_dev, cmd_reg, 4);
                          nvme_ctrlr_get_cap(&pctrlr->ctrlr, &cap)
                            nvme_transport_ctrlr_get_reg_8
                                transport->ops.ctrlr_get_reg_8(ctrlr, offset, value)
                                    spdk_mmio_read_8(nvme_pcie_reg_addr(ctrlr, offset))
                          nvme_ctrlr_get_vs(&pctrlr->ctrlr, &vs)
                          nvme_ctrlr_init_cap(&pctrlr->ctrlr, &cap, &vs);					通过寄存器读取的方式获取cap和vs信息，初始化cap。主要信息是page_size、io_queue_size、io_queue_requests
                          rc = nvme_pcie_ctrlr_construct_admin_qpair(&pctrlr->ctrlr, pctrlr->ctrlr.opts.admin_queue_size);         创建管理队列qpair
                              pqpair = spdk_zmalloc(sizeof(*pqpair), 64, NULL, SPDK_ENV_SOCKET_ID_ANY, SPDK_MALLOC_SHARE);	使用的是大页内存
                              rc = nvme_qpair_init(ctrlr->adminq, 0, /* qpair ID */ ctrlr, SPDK_NVME_QPRIO_URGENT, num_entries);  初始化qpair的下属队列等，0号qpair就是管理队列
                                  STAILQ_INIT(&qpair->free_req)
                                  req = (void *)((uintptr_t)qpair->req_buf + i * req_size_padded)
                                  STAILQ_INSERT_HEAD(&qpair->free_req, req, stailq)
                              nvme_pcie_qpair_construct(ctrlr->adminq, NULL);				创建其他qpair所需的信息max_completions_cap、tracker等，tracker是一次性分配该qpair的所有数量，因为内存对齐和边界的要求。然后将tracker装入到qpair中的tracker数组指针中进行保存，并插入free_tr中进行记录
                              nvme_pcie_qpair_reset(qpair);								qpair中的队列信息清零
                          rc = nvme_ctrlr_add_process(&pctrlr->ctrlr, pci_dev);				创建进程信息，用于多进程下的ctrlr管理
                          ctrlr->remove_cb = probe_ctx->remove_cb;							remove_cb的传入
                          ctrlr->cb_ctx = probe_ctx->cb_ctx;
                          nvme_qpair_set_state(ctrlr->adminq, NVME_QPAIR_ENABLED);			设置qpair的状态
                          TAILQ_INSERT_TAIL(&probe_ctx->init_ctrlrs, ctrlr, tailq);			插入控制器到probe_ctx->init_ctrlrs中，用于后续的状态初始化
                    TAILQ_INSERT_TAIL(&g_pci_hotplugged_devices, dev, internal.tailq);				插入到热插拔的队列中
                --------------------------------------------------------------------------------------------------------------------------------
              cleanup_pci_devices();		把新probe的控制器给放入g_pci_devices中管理（此前为空）
  nvme_init_controllers(probe_ctx);		
    rc = spdk_nvme_probe_poll_async(probe_ctx);
      ------------------------------------------------------------TAILQ_FOREACH_SAFE(ctrlr, &probe_ctx->init_ctrlrs, tailq, ctrlr_tmp) {      对每一个在init_ctrlrs队列中的ctrlr
      nvme_ctrlr_poll_internal(ctrlr, probe_ctx);		对每个在probe_ctx->init_ctrlrs的ctrlr执行，直到该队列为空，设置g_spdk_nvme_driver->initialized = true;返回0
        rc = nvme_ctrlr_process_init(ctrlr);		配置ctrlr的寄存器状态 cc.en，identify 、 construct namespace、 identify namespace 等，直到状态为NVME_CTRLR_STATE_READY才算成功
        TAILQ_REMOVE(&probe_ctx->init_ctrlrs, ctrlr, tailq);								移除初始化队列
        TAILQ_INSERT_TAIL(&g_spdk_nvme_driver->shared_attached_ctrlrs, ctrlr, tailq);		插入到g_spdk_nvme_driver->shared_attached_ctrlrs队列
        nvme_ctrlr_proc_get_ref(ctrlr);														移除inactive的proc，给当前的proc进行active_proc->ref++
        probe_ctx->attach_cb(probe_ctx->cb_ctx, &ctrlr->trid, ctrlr, &ctrlr->opts);			如果attach_cb有效，则进行attach_cb流程
      ------------------------------------------------------------	循环执行
------------------------------------------------------------------------------------------------  到此为止example的spdk_nvme_probe流程就结束了 

probe代码详解, 参考文档: https://www.cnblogs.com/vlhn/p/7727016.html
03 - DPDK中环境抽象层（EAL: Environment Abstraction Layer）的函数rte_pci_match()是发现SSD设备的关键。
04 - DPDK的EAL在DPDK架构中所处的位置


git add xx
git commit --amend
git push origin xb
https://github.com/ssbandjl/spdk/blob/xb/doc/nvme_spec.md



块设备：
创建daos bdev块设备
daos cont create --pool=test-pool --label=test-cont --type=POSIX
rpc.py bdev_daos_create daosdev0 test-pool test-cont 64 4096  # 块大小为4096， 共64块, 256KB

./scripts/rpc.py bdev_daos_create daosdev0 test-pool test-cont 64 4096  -> from spdk.rpc.client -> 4K * 64 = 256KB -> |4K|4K|...|
SPDK_RPC_REGISTER("bdev_daos_create", rpc_bdev_daos_create, SPDK_RPC_RUNTIME)
rpc_bdev_daos_create -> module/bdev/daos/bdev_daos_rpc.c
  create_bdev_daos
    block_size % 512 512对齐
    daos->disk.fn_table = &daos_fn_table -> static const struct spdk_bdev_fn_table daos_fn_table -> 提交IO请求 -> bdev_daos_submit_request -> ... -> dfs_write ->  src/client/dfs/dfs.c -> daos_array_write
    bdev_get_daos_engine
      daos_init()
    bdev_daos_io_channel_create_cb
      spdk_call_unaffinitized(_bdev_daos_io_channel_create_cb, ch)
        rte_thread_get_affinity(&orig_cpuset)
        spdk_unaffinitize_thread() 移除cpu亲和性
          CPU_ZERO(&new_cpuset)
        _bdev_daos_io_channel_create_cb
          bdev_get_daos_engine
          daos_pool_connect
          daos_cont_open
          dfs_mount(ch->pool, ch->cont, O_RDWR, &ch->dfs)
          dfs_open
          daos_eq_create
        rte_thread_set_affinity(&orig_cpuset) 将cpu亲和性设回去
      ch->poller = SPDK_POLLER_REGISTER(bdev_daos_channel_poll, ch, 0)
    bdev_daos_io_channel_destroy_cb
      spdk_poller_unregister
      daos_eq_destroy
      dfs_release
      dfs_umount
      daos_cont_close
      daos_pool_disconnect
      bdev_daos_put_engine
    spdk_io_device_register(daos, bdev_daos_io_channel_create_cb, bdev_daos_io_channel_destroy_cb -> lib/thread/thread.c
      thread = spdk_get_thread() 此外，在注册 io 设备之前将 UT 更改为 set_thread()
      dev->create_cb = create_cb = bdev_daos_io_channel_create_cb
      dev->destroy_cb = destroy_cb
      tmp = RB_INSERT(io_device_tree, &g_io_devices, dev) -> io_device_tree_RB_INSERT(&g_io_devices, dev) -> 将IO设备对象插入红黑树(g_io_devices全局红黑树头) -> 利用宏生成红黑树插入函数: #define RB_GENERATE_INSERT -> name##_RB_INSERT
    spdk_bdev_register -> lib/bdev/bdev.c -> spdk_bdev_register(struct spdk_bdev *bdev)
      bdev_register(bdev) -> lib/bdev/bdev.c -> bdev_register(struct spdk_bdev *bdev)
        spdk_bdev_get_memory_domains 获取给定 bdev 使用的 SPDK 内存域。 如果 bdev 报告它使用内存域，这意味着它可以使用位于这些内存域中的数据缓冲区。 用户可以调用此函数并将 domains 设置为 NULL 并将 array_size 设置为 0 以获取 bdev 使用的内存域数
        spdk_bdev_is_md_separate 查询元数据是否与块数据交织或与块数据分离
        bdev_alloc_io_stat -> io statics
            struct spdk_bdev_io_stat *stat
            spdk_bdev_reset_io_stat(stat, SPDK_BDEV_RESET_STAT_ALL)
        bdev_name_add
        spdk_bdev_get_buf_align
        spdk_io_device_register 将不透明的 io_device 上下文注册为 I/O 设备。 I/O设备注册后，可以使用spdk_get_io_channel()函数返回I/O通道, 许多 bdev 模块创建自己的 bdev 结构，其中 spdk_bdev 作为第一个成员。 bdev.c 当前使用 spdk_bdev 指针作为其 io_device 句柄，强制 bdev 模块选择不同的东西。将其更改为使用 spdk_bdev 指针 + 1 个字节作为其 io_device 句柄。 实际句柄并不重要——它只需要是独一无二的。 这将简化 bdev 模块开发
        TAILQ_INSERT_TAIL(&g_bdev_mgr.bdevs, bdev, internal.link) 上链表
      bdev_open 注册 bdev 时，bdev 模块会在通知 bdev 寄存器之前对其进行检查。检查可能是异步的，例如 当 bdev 模块必须在新的 bdev 上执行 I/O 时。这会导致竞争条件，其中 bdev 可能会在检查未完成时被破坏。 然后，一旦所有模块都发出检查已完成的信号，`bdev_register_finished` 将对释放的 bdev 指针进行无效访问。要解决此问题，请推迟取消注册，直到通过打开 bdev 上的描述符完成检查
      bdev_examine
        TAILQ_FOREACH(module, &g_bdev_mgr.bdev_modules, internal.tailq)
            module->examine_config(bdev)
        module->examine_disk(bdev)
      spdk_bdev_wait_for_examine(bdev_register_finished, desc) 当所有 bdev 完成检查过程时报告。 注册的 cb_fn 只会被调用一次。 需要再次调用此函数以接收有关检查过程的进一步报告
    *bdev = &(daos->disk)


docker run -it -d --privileged --cap-add=ALL --name spdk eeb6ee3f44bd
docker run -it -d --privileged --cap-add=ALL --name spdk -v /root/project/stor/spdk/xb/spdk:/home/xb/project/stor/spdk/xb/spdk centos:centos7.9.2009
docker start spdk
docker exec -u root -it spdk bash -c 'cd /home/xb/project/stor/spdk/xb/spdk;exec "${SHELL:-sh}"'
docker exec -it spdk /bin/bash

大页管理:
/usr/bin/hugeadm --pool-pages-min DEFAULT:2G -vvv

编译:
./configure --prefix="/opt/daos/prereq/debug/spdk" --enable-debug --disable-tests --disable-unit-tests --disable-apps --without-vhost --without-crypto --without-pmdk --without-rbd --with-rdma --without-iscsi-initiator --without-isal --without-vtune --with-shared --enable-examples
  configure -> common.sh -> CONFIG -> CONFIG.sh

autorun.sh -> autobuild.sh -> meson build-tmp

dpdk debug
dpdkbuild/Makefile
ifeq ($(CONFIG_DEBUG),y)
DPDK_OPTS += --buildtype=debug

dpdk/lib/meson.build -> 
default_cflags += ['-g']
default_cflags += ['-O0']

sed正则表达式:
sed -r 's/CONFIG_([[:alnum:]_]+)=(.*)/CONFIG[\1]=\2/g' $rootdir/CONFIG > $rootdir/CONFIG.sh

nvme-cli, 
https://spdk.io/doc/nvme.html#nvme_cuse
modprobe cuse, lsmod|grep cuse

export HUGE_EVEN_ALLOC=yes   # 如果设置为“是”，大页面将均匀分布在所有“系统的 NUMA 节点（有效地忽略 HUGENODE 中设置的任何内容）”。 使用内核默认的大页面大小
sudo ./scripts/setup.sh
sudo ./build/bin/nvmf_tgt -m [21,22,23,24]
app/nvmf_tgt/nvmf_main.c -> nvmf_tgt -> main
spdk_app_start(&opts, nvmf_tgt_started, NULL) -> nvmf_tgt_started(void *arg1)
    if (getenv("MEMZONE_DUMP") != NULL)
        spdk_memzone_dump(stdout)
        fflush(stdout)
...


创建传输层:
参数: scripts/rpc.py -> def nvmf_create_transport(args) 
sudo ./scripts/rpc.py nvmf_create_transport -t TCP -u 2097152 -i 2097152  # -u: IO单元字节数, -i: 最大IO字节

使用提升的权限启动 nvmf_tgt 应用程序。 启动目标后，nvmf_create_transport rpc 可用于初始化给定的传输。 下面是一个示例，其中目标启动并配置了两种不同的传输。 RDMA 传输配置有 8192 字节的 I/O 单元大小、最大 I/O 大小 131072 和 8192 字节的封装数据大小。 TCP 传输配置了 16384 字节的 I/O 单元大小，每个控制器最多 8 个 qpairs，以及 8192 字节的封装数据大小
gdb build/bin/nvmf_tgt
./scripts/rpc.py nvmf_create_transport -t RDMA -u 8192 -i 131072 -c 8192
scripts/rpc.py nvmf_create_transport -t TCP -u 16384 -m 8 -c 8192

#define SPDK_DEFAULT_RPC_ADDR "/var/tmp/spdk.sock"
./scripts/rpc.py nvmf_create_transport -t RDMA -u 8192 -p 4 -c 0
sudo ./scripts/rpc.py nvmf_create_transport -t TCP -u 2097152 -i 2097152
nvmf_create_transport -> lib/nvmf/nvmf_rpc.c -> rpc_nvmf_create_transport(struct spdk_jsonrpc_request *request -> 创建传输层
  spdk_nvmf_get_tgt(ctx->tgt_name) -> tgt_name ?
    TAILQ_FOREACH(tgt, &g_nvmf_tgts, link)
  spdk_nvmf_transport_opts_init
    nvmf_get_transport_ops
      TAILQ_FOREACH(ops, &g_spdk_nvmf_transport_ops
    nvmf_transport_opts_copy
  spdk_nvmf_tgt_get_transport
    TAILQ_FOREACH(transport, &tgt->transports
  spdk_nvmf_transport_create_async nvmf_rpc_create_transport_done
    nvmf_transport_create
      nvmf_get_transport_ops
      nvmf_transport_opts_copy
      spdk_iobuf_get_opts
      spdk_min
      ctx->cb_fn = cb_fn -> 设置完成回调 -> nvmf_rpc_create_transport_done
      spdk_thread_send_msg(spdk_get_thread(), _nvmf_transport_create_done, ctx) -> msg->fn = fn -> tls_thread -> 优先执行同步创建
        msg = spdk_mempool_get(g_spdk_msg_mempool)
          rte_mempool_get -> 从内存池中获取一个对象。 该函数调用多消费者或单消费者版本，具体取决于创建内存池时指定的默认行为（请参阅标志）。 如果启用缓存，则将首先从缓存中检索对象，然后从公共池中检索对象。 请注意，当本地缓存和公共池为空时，即使其他 lcore 的缓存已满，它也会返回 -ENOENT, 放回去: spdk_mempool_put
        spdk_ring_enqueue -> spdk_ring_dequeue -> msg->fn(msg->arg)
          rte_ring_enqueue_bulk
            rte_ring_enqueue_bulk_elem
        thread_send_msg_notification(thread)
      return 0
      ctx->ops->create_async(&ctx->opts, nvmf_transport_create_async_done, ctx) -> 其次才是异步

export C_INCLUDE_PATH=$C_INCLUDE_PATH:/usr/include/openssl11
cp /usr/include/openssl11/openssl/kdf.h /usr/include/openssl/

soft rdma, 软件模拟rdma, 
lsmod |grep rxe
modprobe nvme_rdma
./build/bin/nvmf_tgt

./scripts/rpc.py nvmf_create_transport -t RDMA -u 8192
rdma.c:2484:create_ib_device:

4.通过rpc创建导出bdev
创建内存测试盘
# scripts/rpc.py bdev_malloc_create -b Malloc0 512 512 #块大小=512, 块数=512, 1GB=512 * 2048
# scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
# scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 Malloc0
# scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t rdma -a 192.168.80.100 -s 4420

创建nvme盘
# scripts/rpc.py bdev_nvme_attach_controller -b Nvme0 -t PCIe -a 0000:01:00.0
# scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode2 -a -s SPDK00000000000002 -d SPDK_Controller1
# scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode2 Nvme0n1
# scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode2 -t rdma -a 192.168.80.100 -s 4420

客户端
1.load module
# modprobe nvme-rdma

2.discovery
nvme discover -t rdma -a 175.17.53.73 -s 4420
# nvme discover -t rdma -a 192.168.80.100 -s 4420
3.connect
连接cnode1
# nvme connect -t rdma -n "nqn.2016-06.io.spdk:cnode1" -a 192.168.80.100 -s 4420
连接cnode2
# nvme connect -t rdma -n "nqn.2016-06.io.spdk:cnode2" -a 192.168.80.100 -s 4420
# lsblk
4.disconnect
# nvme disconnect -n "nqn.2016-06.io.spdk:cnode1"
# nvme disconnect -n "nqn.2016-06.io.spdk:cnode2"


nvmf_rdma_create
lib/nvmf/rdma.c -> create_ib_device

gdb调试 spdk_nvme_tgt
app/nvmf_tgt/nvmf_main.c:47
spdk_app_opts_init -> opts=0x7fffffffdee0, opts_size=224
  #define SET_FIELD(field, value) -> 临时定义宏
  SET_FIELD(enable_coredump, true) -> 设置默认选项
  SET_FIELD(shm_id, -1);
  SET_FIELD(mem_size, SPDK_APP_DPDK_DEFAULT_MEM_SIZE);
  SET_FIELD(main_core, SPDK_APP_DPDK_DEFAULT_MAIN_CORE);
  SET_FIELD(mem_channel, SPDK_APP_DPDK_DEFAULT_MEM_CHANNEL);
  SET_FIELD(base_virtaddr, SPDK_APP_DPDK_DEFAULT_BASE_VIRTADDR);
  SET_FIELD(print_level, SPDK_APP_DEFAULT_LOG_PRINT_LEVEL);
  SET_FIELD(rpc_addr, SPDK_DEFAULT_RPC_ADDR); -> /var/tmp/spdk.sock
  SET_FIELD(num_entries, SPDK_APP_DEFAULT_NUM_TRACE_ENTRIES);
  SET_FIELD(delay_subsystem_init, false);
  SET_FIELD(disable_signal_handlers, false)
  SET_FIELD(rpc_allowlist, NULL);
gdb -> p opts
spdk_app_parse_args -> 解析参数
    -m -> opts->reactor_mask = optarg
spdk_app_start(&opts, nvmf_tgt_started, NULL) -> spdk_app_start(struct spdk_app_opts *opts_user -> g_start_fn = start_fn
  app_copy_opts
  spdk_log_set_print_level
  app_setup_env
  calculate_mempool_size -> app：添加 msg_mempool_size 自动计算 修复 #811 目前，无论 SPDK 应用程序运行在多少个核心上，消息内存池大小始终配置为相同的默认值。 在某些情况下，默认值 262143 是不够的，特别是在启动大量 SPDK 线程的情况下。 如果分配给 SPDK 的核心超过 63 个，则此补丁添加了 msg_mempool_size 的计算。 在这种情况下，msg_mempool_size 的大小计算为 core_count * 4096。对于最多 63 个核心，始终使用值 262143。 用户还可以使用 --msg-mempool-size 选项指定消息内存池的大小。 如果由用户指定，它始终会覆盖默认或自动计算的大小
  spdk_log_open
  spdk_reactors_init(size_t msg_mempool_size) -> 初始化 reactor, spdk线程模型: https://zhuanlan.zhihu.com/p/560861776
    g_spdk_event_mempool = spdk_mempool_create(mempool_name
      rte_mempool_create
    spdk_env_get_last_core
      SPDK_ENV_FOREACH_CORE -> 遍历cpu
    posix_memalign
    g_core_infos = calloc
    spdk_thread_lib_init_ext(reactor_thread_op, reactor_thread_op_supported -> g_thread_op_fn -> g_thread_op_supported_fn -> 初始化线程库。 必须在分配任何线程之前调用一次 thread_op_fn 和 thread_op_type_supported_fn 必须同时指定或不指定 -> nvmeof_tgt: https://blog.csdn.net/weixin_60043341/article/details/126505064, 将g_new_thread_fn赋值为reactor_thread_op，从而实现后续以spdk_create_thread创建的逻辑层面的thread都和具体的reactor相关联, 使用SPDK lib搭建自己的NVMe-oF Target应用: https://mp.weixin.qq.com/s/niKa3wnlRuz4LJ47mJBJvQ
      _thread_lib_init
        g_spdk_msg_mempool = spdk_mempool_create -> 创建消息池msgpool
    SPDK_ENV_FOREACH_CORE reactor_construct -> 构造reactor
      reactor->events = spdk_ring_create
      if (reactor_interrupt_init(reactor) -> 默认中断模式 -> 中断：在thd和reactor中应用fd_group，每个reactor和每个线程分配一个fd组。 同时，每个线程被视为一个中断源，注册到其相应的反应器中。 reacotr 的egrp函数是唯一等待事件的阻塞点
        spdk_fd_group_create
        reactor->resched_fd = eventfd
        SPDK_FD_GROUP_ADD(reactor->fgrp, reactor->resched_fd, reactor_schedule_thread_event
          epoll_ctl(epfd
        SPDK_FD_GROUP_ADD reactor->events_fd event_queue_run_batch
      spdk_interrupt_mode_is_enabled
    reactor = spdk_reactor_get(current_core)
    g_scheduling_reactor = reactor
  spdk_cpuset_set_cpu
  spdk_thread_create("app_thread", &tmp_cpumask) -> 创建一个新的 SPDK 线程对象。 请注意，通过 spdk_thread_create() 创建的第一个线程将被指定为应用程序线程。 其他 SPDK 库可能会限制某些 API 只能在此应用程序线程的上下文中调用
    spdk_interrupt_mode_is_enabled -> 默认禁用中断
    g_thread_op_fn(thread, SPDK_THREAD_OP_NEW) -> reactor_thread_op
      _reactor_schedule_thread
        spdk_cpuset_zero
        spdk_cpuset_set_cpu
        spdk_cpuset_xor -> 异或, eactor：避免在intr中将线程调度到reactor，目前，spdk_thread无法在处于中断模式的reactor上执行，但spdk_thread的中断未启用。 所以避免调度 spdk_thread 就可以了
          dst->cpus[i] ^= src->cpus[i]
        spdk_cpuset_copy -> copy dst <- src
        spdk_cpuset_and -> dst->cpus[i] &= src->cpus[i]
        spdk_cpuset_get_cpu(cpumask, core) -> return (set->cpus[cpu / 8] >> (cpu % 8)) & 1U -> 位运算
        evt = spdk_event_allocate(core, _schedule_thread, lw_thread, NULL)
          event = spdk_mempool_get(g_spdk_event_mempool)
          event->fn = fn
        lw_thread->tsc_start = spdk_get_ticks() -> lib/event: 将线程的运行时间添加到framework_get_reactors RPC的输出中，收集每个SPDK线程的运行时间并将其添加到framework_get_reactors RPC的输出中
        spdk_event_call(evt)
          rc = spdk_ring_enqueue(reactor->events, (void **)&event, 1, NULL) -> 消息事件入队
          rc = write(reactor->events_fd, &notify, sizeof(notify)) -> 默认不通知
  spdk_thread_send_msg(spdk_thread_get_app_thread(), bootstrap_fn, NULL)
    msg->fn = fn
    spdk_ring_enqueue(thread->messages, (void **)&msg, 1, NULL) -> 消息其实是通过spdk_ring_enqueue()放入了ring-buffer中的。在后续的poller挂在的函数中对ring-buffer中的消息进行处理, 代码分析: https://zhuanlan.zhihu.com/p/423779832
    thread_send_msg_notification(thread)
  spdk_reactors_start
    reactor_run(reactor)
      while (1)
      _reactor_run(reactor)
        event_queue_run_batch(reactor)
          count = spdk_ring_dequeue(reactor->events, events, SPDK_EVENT_BATCH_SIZE)
           event->fn(event->arg1, event->arg2) -> _schedule_thread(void *arg1, void *arg2)

        spdk_thread_poll -> thread_poll
          msg_queue_run_batch -> msg->fn(msg->arg) ->  -> nvmf_subsystem_init
          _nvmf_transport_create_done -> ctx->cb_fn -> _ctx->ops->create -> .create = nvmf_rdma_create,
          nvmf_rdma_create
            TAILQ_INIT(&rtransport->devices);
            TAILQ_INIT(&rtransport->ports);
            TAILQ_INIT(&rtransport->poll_groups);
            TAILQ_INIT(&rtransport->retry_ports);
            ...
            rtransport->rdma_opts.num_cqe = DEFAULT_NVMF_RDMA_CQ_SIZE -> rdma参数
            ...
            SPDK_INFOLOG(rdma, "*** RDMA Transport Init ***\n" -> 打印参数
            rtransport->event_channel = rdma_create_event_channel()
            fcntl(rtransport->event_channel->fd, F_SETFL, flag | O_NONBLOCK)
            rtransport->data_wr_pool = spdk_mempool_create
              spdk_zmalloc
              spdk_mem_map_alloc
            rdma_get_devices
            create_ib_device
              ibv_query_device
              nvmf_rdma_is_rxe_device
              TAILQ_INSERT_TAIL(&rtransport->devices, device, link)
              ibv_alloc_pd
              device->map = spdk_rdma_create_mem_map
              "Create IB device xxx"
            rc = generate_poll_fds(rtransport);
            SPDK_POLLER_REGISTER nvmf_rdma_accept -> thread_execute_timed_poller -> poller->fn(poller->arg)
          thread_execute_poller(thread, poller) -> 扇出,执行poller或定时器poller
            poller->fn(poller->arg) -> nvmf_poll_group_poll <- poller_register <- spdk_poller_register
              TAILQ_FOREACH nvmf_transport_poll_group_poll -> nvmf_rdma_poll_group_poll
                TAILQ_FOREACH_SAFE nvmf_rdma_poller_poll
                  reaped = ibv_poll_cq(rpoller->cq, 32, wc) -> reaped(收割IO) -> nvmf/rdma：为每个线程而不是每个连接创建一个 cq，这在目标扩展到多个连接时大大提高了效率。 现在，可以在 O(1) 内轮询给定线程处理的所有连接，而之前是 O(n)，其中 n 是连接数
                  switch (rdma_wr->type) 
                    case RDMA_WR_TYPE_RECV -> 假如收到host的写请求
                      STAILQ_INSERT_HEAD(&rqpair->resources->incoming_queue, rdma_recv, link) -> 将接收到的请求放入队列
                      nvmf_rdma_qpair_process_pending
                        incoming_queue -> free_queue
                        nvmf_rdma_request_process(rtransport, rdma_req) -> 处理rdma请求, enum spdk_nvmf_rdma_request_state -> rdma请求状态机
    spdk_env_thread_wait_all()


./scripts/rpc.py --verbose=DEBUG nvmf_create_transport -t RDMA
python3 -m pdb scripts/rpc.py nvmf_create_transport -t RDMA
python/spdk/rpc/nvmf.py -> nvmf_create_transport -> def nvmf_create_transport
client.call('nvmf_create_transport', params) -> def call(self, method, params={}) -> flush -> self.sock.sendall(reqstr.encode("utf-8"))
class JSONRPCClient(object) -> __init__ -> self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) -> unix domain socket -> 

__main__
args.client = rpc.client.JSONRPCClient
log_set_level


./build/bin/nvmf_tgt --json bdev_nvme.json   # bdev_nvme.json内容请见附件  # 另开一个终端窗口
./scripts/rpc.py log_set_level "DEBUG"      # 这里最低只能设置"ERROR"
./scripts/rpc.py log_get_level               # 显示当前log等级
./scripts/rpc.py log_set_print_level "DEBUG"  # 设置当前log打印等级
./scripts/rpc.py log_get_print_level         # 显示当前log打印等级

支持的logflag:
./build/bin/nvmf_tgt -h
-L, --logflag <flag>    enable log flag (all, accel, accel_ioat, aio, app_config, app_rpc, bdev, bdev_concat, bdev_daos, bdev_ftl, bdev_malloc, bdev_null, bdev_nvme, bdev_raid, bdev_raid0, bdev_raid1, blob, blob_esnap, blob_rw, blobfs, blobfs_bdev, blobfs_bdev_rpc, blobfs_rw, ftl_core, ftl_init, gpt_parse, ioat, json_util, log, log_rpc, lvol, lvol_rpc, nbd, notify_rpc, nvme, nvme_vfio, nvmf, nvmf_tcp, opal, rdma, reactor, rpc, rpc_client, sock, sock_posix, thread, trace, vbdev_delay, vbdev_gpt, vbdev_lvol, vbdev_opal, vbdev_passthru, vbdev_split, vbdev_zone_block, vfio_pci, vfio_user, virtio, virtio_blk, virtio_dev, virtio_pci, virtio_user, virtio_vfio_user, vmd)

ls -alh /sys/class/nvme
scripts/rpc.py bdev_nvme_attach_controller -b Nvme0 -t PCIe -a 0000:e6:00.0

nvme discover /dev/nvme-fabrics -t rdma -a 192.168.1.110 -s 4420
lsmod |grep nvme
lsmod |grep rdma
modprobe -r nvme_rdma nvme_fabrics
lsmod |grep mlx

./scripts/rpc.py log_set_level "DEBUG" 
./scripts/rpc.py log_set_print_level "DEBUG"

scripts/rpc.py nvmf_create_transport -t RDMA
scripts/rpc.py nvmf_get_transports

scripts/rpc.py bdev_malloc_create -b Malloc0 512 512
scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 Malloc0
scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t rdma -a 175.16.53.73 -s 4420
scripts/rpc.py bdev_nvme_attach_controller -b Nvme0 -t PCIe -a 0000:e6:00.0
scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode2 -a -s SPDK00000000000002 -d SPDK_Controller1
scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode2 Nvme0n1
scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode2 -t rdma -a 175.16.53.73 -s 4420\

scripts/rpc.py bdev_malloc_create -b Malloc0 512 512
scripts/rpc.py nvmf_create_subsystem nqn.2014-08.org.nvmexpress:uuid:a8dce057-b5a2-492e-8da3-9cf328f401c7 -a -s SPDK00000000000001 -d SPDK_Controller1
scripts/rpc.py nvmf_subsystem_add_ns nqn.2014-08.org.nvmexpress:uuid:a8dce057-b5a2-492e-8da3-9cf328f401c7 Malloc0
scripts/rpc.py nvmf_subsystem_add_listener nqn.2014-08.org.nvmexpress:uuid:a8dce057-b5a2-492e-8da3-9cf328f401c7 -t rdma -a 175.16.53.73 -s 4420

scripts/rpc.py bdev_nvme_attach_controller -b Nvme0 -t PCIe -a 0000:e6:00.0
scripts/rpc.py nvmf_create_subsystem nqn.2014-08.org.nvmexpress:uuid:a8dce057-b5a2-492e-8da3-9cf328f401c7 -a -s SPDK00000000000002 -d SPDK_Controller1
scripts/rpc.py nvmf_subsystem_add_ns nqn.2014-08.org.nvmexpress:uuid:a8dce057-b5a2-492e-8da3-9cf328f401c7 Nvme0n1
scripts/rpc.py nvmf_subsystem_add_listener nqn.2014-08.org.nvmexpress:uuid:a8dce057-b5a2-492e-8da3-9cf328f401c7 -t rdma -a 175.16.53.73 -s 4420



modinfo nvme-rdma
nvme list-subsys /dev/nvme0n1


nvme discover --transport=rdma --traddr=192.168.1.3 --hostnqn=host1-rogue-nqn
cat /etc/nvme/discovery.conf



content in test_rdma.conf
`[Nvme]
TransportID "trtype:PCIe traddr:0000:02:00.0" Nvme0

[Transport]


Type RDMA

[Subsystem1]
NQN nqn.2016-06.io.spdk:cnode1
Listen RDMA 192.168.1.110:4420
AllowAnyHost Yes
Host nqn.2016-06.io.spdk:init
SN SPDK00000000000001
MaxNamespaces 20
Namespace Nvme0n1 1`

app/nvmf_tgt/nvmf_tgt -m 15 -c test_rdma.conf

modprobe -r nvme_rdma nvme_fabrics

客户都安执行:
[root@node215 ~]# modinfo nvme-rdma
filename:       /lib/modules/5.10.38-21.01.el7.x86_64/extra/mlnx-nvme/host/nvme-rdma.ko
license:        GPL v2
srcversion:     8817A28E0A60AA47E80648C
depends:        nvme-core,mlx_compat,rdma_cm,ib_core,nvme-fabrics  #看依赖

MLNX_OFED 默认安装什么都不做的虚拟 nvme-rdma 内核模块。
尝试使用 --with-nvmf 安装它：
./mlnxofedinstall --with-nvmf
这应该安装正确的 nvme-rdma 驱动程序。
您可以检查其他 MLNX_OFED 安装选项：
./mlnxofedinstall --help

modprobe nvme_fabrics --force-modversion

modprobe -r nvme-rdma nvme-fabrics nvme nvme-core 然后 modprobe nvme-rdma 或者只是重新启动主机以重新加载所有模块

find / -name mlnxofedinstall
mlnxofedinstall -h

./scripts/rpc.py nvmf_get_stats

modinfo nvmet 和 modinfo nvmet-rdma 的输出——它们的描述中不应有虚拟词, 或dmesg中查看错误



vhost, qemu, io_path
dpdk/examples/vdpa/main.c -> main
start_vdpa
    rte_vhost_driver_start
        vhost_user_start_server
            bind, listen
            fdset_add(&vhost_user.fdset, fd, vhost_user_server_new_connection, NULL, vsocket)
                fd = accept(fd, NULL, NULL)
                vhost_user_add_connection
        or vhost_user_start_client
            vhost_user_connect_nonblock
            vhost_user_add_connection
                fdset_add(&vhost_user.fdset, fd, vhost_user_read_cb
                    vhost_user_msg_handler(conn->vid, connfd)
                        read_vhost_message(dev, fd, &ctx)
                        vhost_user_check_and_alloc_queue_pair
                        switch (request)
                        msg_result = (*dev->extern_ops.pre_msg_handle)(dev->vid, &ctx)
                        msg_result = (*dev->extern_ops.post_msg_handle)(dev->vid, &ctx)
                        send_vhost_reply(dev, fd, &ctx)
dev->notify_ops->new_device(dev->vid)
static const struct rte_vhost_device_ops g_spdk_vhost_ops
.new_device =  start_device <- case VHOST_USER_SET_VRING_KICK
    start_device(int vid)
        spdk_thread_send_msg(vdev->thread, vhost_user_session_start, vsession) -> vhost_user_session_start(void *arg1)
            backend->start_session(vdev, vsession, NULL)
static const struct spdk_vhost_user_dev_backend vhost_blk_user_device_backend
.start_session =  vhost_blk_start -> vhost_blk_start(struct spdk_vhost_dev *vdev 以前 spdk_vhost_dev_backend 持有 vhost_blk 和 vhost_scsi 功能的回调，以及由 vhost_user 后端调用的回调。 此补丁将这些回调分为两个结构：spdk_vhost_dev_backend - 由 vhost_blk 和 vhost_scsi 实现，spdk_vhost_user_dev_backend - 仅由 vhost_user 后端实现，回调特定于该传输的会话管理
    bvsession->io_channel = vhost_blk_get_io_channel(vdev)
        spdk_bdev_get_io_channel(bvdev->bdev_desc)
    vdev_worker -> vdev_worker(void *arg) -> struct spdk_vhost_virtqueue *vq = arg -> _vdev_vq_worker(vq)
        struct spdk_vhost_session *vsession = vq->vsession
        process_packed_vq(bvsession, vq)
        or process_vq
    or no_bdev_vdev_worker -> _no_bdev_vdev_vq_worker
        no_bdev_process_packed_vq
            task_idx = vhost_vring_packed_desc_get_buffer_id(vq, req_idx, &num_descs)
            unlikely vhost_vq_packed_ring_enqueue
            blk_task_init(task)
                blk_task->iovcnt = SPDK_COUNTOF(blk_task->iovs) -> #define SPDK_COUNTOF(arr) (sizeof(arr) / sizeof((arr)[0])) -> array
                blk_task->payload_size = 0
            blk_iovs_packed_queue_setup
                vhost_vq_get_desc_packed(vsession, vq, req_idx, &desc, &desc_table, &desc_table_size)
                    *desc =  &virtqueue->vring.desc_packed[req_idx]
                    if (vhost_vring_packed_desc_is_indirect(*desc))
                        vhost_packed_desc_indirect_to_desc_table
                            *desc_table = vhost_gpa_to_vva(vsession, addr, len)
                                vva = (void *)rte_vhost_va_from_guest_pa(vsession->mem, addr, &newlen)
                blk_iovs_packed_desc_setup(vsession, vq, req_idx, desc_table, desc_table_size, iovs, iovs_cnt, length)
                    struct vring_packed_desc *desc
                    desc = &vq->vring.desc_packed[req_idx]
                    vhost_vring_packed_desc_to_iov(vsession, iovs, &cnt, desc)
                        vhost_vring_desc_payload_to_iov(vsession, iov, iov_index, desc->addr, desc->len) -> desc -> payload
                            do
                                vva = (uintptr_t)rte_vhost_va_from_guest_pa(vsession->mem, payload, &len) -> Convert guest physical address to host virtual address
                                iov[*iov_index].iov_base = (void *)vva; -> map vva to iovs, when read/write iovs, eq to read/write vva
                                iov[*iov_index].iov_len = len;
                                remaining -= len;
                    out_cnt += vhost_vring_packed_desc_is_wr(desc)
                    vhost_vring_packed_desc_get_next(&desc, &req_idx, vq, desc_table, desc_table_size) -> Get subsequent descriptor from vq or desc table
                        (*req_idx)++
                        *desc = &desc_table[*req_idx]
                    *length = len -> calc task payload_size from desc len
                    *iovs_cnt = cnt
            vhost_vq_packed_ring_enqueue
                struct vring_packed_desc *desc = &virtqueue->vring.desc_packed[virtqueue->last_used_idx]
                desc->id = buffer_id
                rte_vhost_set_last_inflight_io_packed(vsession->vid, virtqueue->vring_idx, inflight_head)
                    inflight_info->desc[last].next = inflight_info->free_head
                desc->flags |= VRING_DESC_F_AVAIL_USED
                or desc->flags &= ~VRING_DESC_F_AVAIL_USED
                rte_vhost_clr_inflight_desc_packed -> lib/vhost：将 virtio 相关功能移至 rte_vhost，目前只有 rte_vhost_user 使用 virtio 相关功能。它由 vhost_scsi 使用，并将由 vhost_user_blk virtio 传输使用
                    inflight_info->desc[head].inflight = 0
                    inflight_info->old_used_wrap_counter = inflight_info->used_wrap_counter
                vhost_log_used_vring_elem(vsession, virtqueue, virtqueue->last_used_idx)
                    rte_vhost_log_used_vring(vsession->vid, virtqueue->vring_idx, offset, len)
                        vhost_log_used_vring(dev, vq, offset, len) -> __vhost_log_write
                virtqueue->packed.used_phase = !virtqueue->packed.used_phase
                virtqueue->used_req_cnt++
        or no_bdev_process_vq
        vhost_session_vq_used_signal
    spdk_poller_register_interrupt(bvsession->requestq_poller, vhost_blk_poller_set_interrupt_mode, bvsession)
        poller->set_intr_cb_fn = cb_fn
        poller->set_intr_cb_arg = cb_arg
        if (poller->thread->in_interrupt) 
            poller->set_intr_cb_fn(poller, poller->set_intr_cb_arg, true)
                vhost_blk_poller_set_interrupt_mode
                    vhost_user_session_set_interrupt_mode
                        if (interrupt_mode)
                            write(q->vring.kickfd, &num_events, sizeof(num_events))
                            vsession->interrupt_mode = true
                        else vsession->interrupt_mode = false
.new_connection = new_connection,
    posix_memalign((void **)&vsession, SPDK_CACHE_LINE_SIZE, sizeof(*vsession) + user_dev->user_backend->session_ctx_size)
    vsession->vdev = vdev;
    vsession->vid = vid;
    vsession->id = user_dev->vsessions_num++;
    vsession->name = spdk_sprintf_alloc("%ss%u", vdev->name, vsession->vid);
    TAILQ_INSERT_TAIL(&user_dev->vsessions, vsession, tailq)
    vhost_session_install_rte_compat_hooks(vsession)
        rte_vhost_extern_callback_register(vsession->vid, &g_spdk_extern_vhost_ops, NULL)
.destroy_connection = destroy_connection,



struct rte_vhost_user_extern_ops g_spdk_extern_vhost_ops = {
	.pre_msg_handle = extern_vhost_pre_msg_handler,

	.post_msg_handle = extern_vhost_post_msg_handler,
        vsession = vhost_session_find_by_vid(vid)
            TAILQ_FOREACH(vsession, &user_dev->vsessions, tailq)
        if (msg->request == VHOST_USER_SET_MEM_TABLE)
            vhost_register_memtable_if_required(vsession, vid)
        switch (msg->request)
        case VHOST_USER_SET_FEATURES:
		    rc = vhost_get_negotiated_features(vid, &vsession->negotiated_features)
        case VHOST_USER_SET_VRING_CALL
            rc = set_device_vq_callfd(vsession, qid)
        case VHOST_USER_SET_VRING_KICK
            enable_device_vq(vsession, qid)
                rte_vhost_get_vhost_ring_inflight(vsession->vid, qid, &q->vring_inflight)
                rte_vhost_get_vring_base(vsession->vid, qid, &q->last_avail_idx, &q->last_used_idx)
                backend->alloc_vq_tasks(vsession, qid)
                if (packed_ring)
                    rte_vhost_get_vring_base_from_inflight
                    q->packed.avail_phase = q->last_avail_idx >> 15
                    q->last_avail_idx = q->last_avail_idx & 0x7FFF;
                    q->packed.used_phase = q->last_used_idx >> 15;
                    q->last_used_idx = q->last_used_idx & 0x7FFF;
                else backend->register_vq_interrupt(vsession, q)
            if (!vsession->started)
                g_spdk_vhost_ops.new_device(vid)
};


参考流程, https://rootw.github.io/2018/05/SPDK-ioanalyze/
vdev_worker()
    \-process_vq()
        |-spdk_vhost_vq_avail_ring_get()
        \-process_blk_request()
            |-blk_iovs_setup()
            \-spdk_bdev_readv()/spdk_bdev_writev()
                \-spdk_bdev_io_submit()
                    \-bdev->fn_table->submit_request()


DPDK预备知识, 简介: https://mp.weixin.qq.com/s?__biz=MzIyNDU0ODk4OA==&mid=2247483746&idx=1&sn=1913b4bc84e93f3a46738b0e19c045b0&scene=19&key=0af7c27fedbcdd2b966410742eb13bf6af8e65619c30834a90a1a4d74b98613d5f15bc52d6badf7839c6dbc40048b39cffe215b67c97b303edf85fc71c9867828a252095a245ac4ba837a0f826fabdec3e02b2c39f19b287403cd90d50add9309be5cf26e315ca09e30e63112571f8a281fcba8392f8bd85311f155c9eabebf0&ascene=7&uin=NjA2MTUyMDIw&devicetype=Windows+10+x64&version=6309021a&lang=zh_CN&session_us=gh_e0b6b1390d76&countrycode=CN&exportkey=n_ChQIAhIQZMzD%2FYd%2BBaclr%2BU0BgDTphLkAQIE97dBBAEAAAAAABmbMIjFKSIAAAAOpnltbLcz9gKNyK89dVj0xzbpP5qJP91wBjcqFMCWnduSZ%2FOIi1pIvsvTtX2xo2UQhqX9DCSpbWXT%2Fm8jDKEcQVqSW6D7G3Z%2Fshq7agQMW%2FigiULvIsk%2BL2wmfNiTXV3k9G4cZJXjvRldK%2Fwi1EG9YNpvrQ%2FWxMkPTUtrpXmmd%2BxmZmVSjGCS2ZF9Ibe3GFMqN8%2Bv0%2FKCQb9ydCMp9xKnKKFR%2BXQ5af30OpctKZm%2BoBNULjbM3%2FX7cw34jjLylO2s2zWPyZWaVPDsnHgnKQ%3D%3D&acctmode=0&pass_ticket=%2B2fKFckRRXpy%2B4qNFfX6EiGulJ0Kj5fByeaAl97LrJFxsqIODZxD9OJFDdcnOoCGK6L1bcvrzi66DtH5PKX6oQ%3D%3D&wx_header=1&fontgear=2


DPDK支持多线程编程，以lcore表示一个CPU。DPDK启动时，会自动检测系统中的CPU信息，并根据用户指定的-c参数（以hex值表示cpu mask信息）运行线程。为了方便线程间的通信和管理，DPDK将这些线程分为一个Master线程和其他Slave线程。Master线程是用户运行程序的主进程，Slave线程用来运行用户自定义的程序。通过这个方式，省去了用户自己创建和管理线程的步骤。下图是SPDK用这个模型跑的一个例子（见examples/perf）
examples/nvme/perf/perf.c -> main(int argc, char **argv)
spdk_env_init -> spdk_env_init(const struct spdk_env_opts *opts)
  rte_eal_init(g_eal_cmdline_argcount, dpdk_args)
    pthread_create eal_thread_loop
      rte_eal_trace_thread_lcore_ready
    rte_eal_cpu_init
    rte_eal_remote_launch
      eal_thread_wake_worker

PMD，即Poll Mode Driver, 
要编写PCIe驱动，必须能够访问BAR空间。对于用户态的程序来说，是不能直接访问BAR空间的，DPDK使用UIO来达到这个目的。UIO还有一个特点就是让用户态的驱动也可以使用中断（Linux系统中，中断只能通过内核线程处理，UIO也只是通过内核函数设置一个计数器，当用户态的程序读这个计数器的时候就知道有没有新的中断了，DPDK据此特别启用了一个线程来管理中断）。用户可以通过uio_pci_generic申明一个PCIe设备作为UIO注册到PCI Bus上。uio_pci_generic默认编译到内核中，跟前一期讲的内核NVMe驱动是同等类型（pci_driver），可以通过modprobe加载。但是uio_pci_generic没有匹配特别的device，需要用户手动配置。SPDK有一个setup脚本，这个脚本会把设备与内核的NVMe驱动unbind，再绑定到uio_pci_generic上。
当把nvme设备bind到uio_pci_generic上后，PCI bus就会去调用它的probe函数。uio_pci_generic的probe函数如下，注册了UIO设备
bind到uio_pci_generic的设备会出现在/dev目录下，此后，就可以通过/dev/uio#操作设备了, 
查看驱动: ls -alh /sys/bus/pci/drivers/uio_pci_generic/
查看uio设备: ls -alh /dev/uio0

rte_eal_pci_probe

下面从这两个方面分析SPDK NVMe驱动。为了方面理解，我们以SPDK的一个例子spdk/examples/nvme/identify来说明。这个例子能够打印绑定UIO的NVMe设备信息。如下图，在Main()函数中，首先调用了rte_eal_init()，然后调用spdk_nvme_probe()，并且传递了probe_cb和attach_cb两个callback函数（由用户指定）。probe_cb中判断这个设备是否加载在UIO上，如果不是就报错。attach_cb则把SPDK定义的controller对象传给print_controller()处理，print_controller提取这个controller的信息打印出来
examples/nvme/identify/identify.c

spdk, nvme, 接口, 函数, nvme命令
include/spdk/nvme.h


tgt启动堆栈:
#0  spdk_nvmf_tgt_create (opts=0x7fffffffd4d0) at nvmf.c:269
#1  0x00000000004c45b4 in nvmf_tgt_create_target () at nvmf_tgt.c:311  -> 创建tgt
#2  0x00000000004c495b in nvmf_tgt_advance_state () at nvmf_tgt.c:418 -> NVMF_TGT_INIT_CREATE_TARGET
#3  0x00000000004c4b34 in nvmf_subsystem_init () at nvmf_tgt.c:492
#4  0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#5  0x0000000000561da2 in bdev_initialize_complete (cb_arg=0x0, rc=0) at bdev.c:18
#6  0x0000000000567629 in bdev_init_complete (rc=0) at bdev.c:1953
#7  0x00000000005676a2 in bdev_module_action_complete () at bdev.c:1995
#8  0x0000000000567b19 in spdk_bdev_initialize (cb_fn=0x561d89 <bdev_initialize_complete>, cb_arg=0x0) at bdev.c:2126
#9  0x0000000000561db9 in bdev_subsystem_initialize () at bdev.c:24
#10 0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#11 0x000000000057f014 in accel_subsystem_initialize () at accel.c:20
#12 0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#13 0x000000000058fad3 in iobuf_subsystem_initialize () at iobuf.c:57
#14 0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#15 0x000000000058d167 in sock_subsystem_init () at sock.c:13
#16 0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#17 0x0000000000588587 in vmd_subsystem_init () at vmd.c:63
#18 0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#19 0x0000000000555bfb in scheduler_subsystem_init () at scheduler.c:23
#20 0x00000000005915b6 in spdk_subsystem_init_next (rc=0) at subsystem.c:166
#21 0x000000000059172c in spdk_subsystem_init (cb_fn=0x5562f1 <app_start_rpc>, cb_arg=0x0) at subsystem.c:199
#22 0x0000000000556c39 in bootstrap_fn (arg1=0x0) at app.c:498
#23 0x0000000000598404 in msg_queue_run_batch (thread=0xd10150, max_msgs=8) at thread.c:832
#24 0x0000000000598d57 in thread_poll (thread=0xd10150, max_msgs=0, now=74054886509296) at thread.c:1054
#25 0x0000000000599057 in spdk_thread_poll (thread=0xd10150, max_msgs=0, now=74054886509296) at thread.c:1147
#26 0x000000000055b162 in _reactor_run (reactor=0xd0fd00) at reactor.c:914
#27 0x000000000055b251 in reactor_run (arg=0xd0fd00) at reactor.c:952
#28 0x000000000055b6c7 in spdk_reactors_start () at reactor.c:1068
#29 0x0000000000557b65 in spdk_app_start (opts_user=0x7fffffffdee0, start_fn=0x407755 <nvmf_tgt_started>, arg1=0x0) at app.c:808
#30 0x000000000040785d in main (argc=1, argv=0x7fffffffe0a8) at nvmf_main.c:47


nvmf_add_discovery_subsystem 创建 discovery NVM subsystem， 这个subsystem一般设置为给所有的host可见。其主要用于实现相应的log discovery命令，告诉host端有多少NVM subsystem在线。当然这个实际会存储在g_spdk_nvmf_tgt中的变量discovery_log_page中 (如表1所示)，并且会做相应的更新
  spdk_nvmf_subsystem_create
  spdk_nvmf_subsystem_set_allow_any_host


nvmf_rdma_connect
  spdk_nvmf_tgt_new_qpair




spdk_nvmf_request_exec_fabrics
spdk_nvmf_request_exec
  nvmf_ctrlr_process_fabrics_cmd
    nvmf_ctrlr_cmd_connect
      spdk_nvmf_tgt_find_subsystem
      ...
      nvmf_ctrlr_create
    status = SPDK_NVMF_REQUEST_EXEC_STATUS_ASYNCHRONOUS
    _nvmf_request_complete -> 不执行
        nvmf_transport_req_complete(req)
            req->qpair->transport->ops->req_complete(req)
        nvmf_qpair_request_cleanup
            qpair->state_cb(qpair->state_cb_arg, 0)



spdkh
rpc_nvmf_put_blockservice2transport
  spdk_nvmf_get_tgt
  nvmf_transport_set_bs_network
    spdk_nvmf_transport_bs_state -> spdk_nvmf_transport


nvme connect 


 nvmf_rdma_accept (ctx=0xd9a440) at rdma.c:3780
#1  0x000000000059d531 in thread_execute_timed_poller (thread=0xd296d0, poller=0xda4bb0, now=1663292075260985) at thread.c:1007
#2  0x000000000059d848 in thread_poll (thread=0xd296d0, max_msgs=0, now=1663292075260985) at thread.c:1097
#3  0x000000000059da5b in spdk_thread_poll (thread=0xd296d0, max_msgs=0, now=1663292075260985) at thread.c:1156
#4  0x000000000055f7de in _reactor_run (reactor=0xd29140) at reactor.c:914
#5  0x000000000055f8cd in reactor_run (arg=0xd29140) at reactor.c:952
#6  0x000000000055fd4b in spdk_reactors_start () at reactor.c:1068
#7  0x000000000055c20a in spdk_app_start (opts_user=0x7fffffffde70, start_fn=0x407c15 <nvmf_tgt_started>, arg1=0x0) at app.c:808
#8  0x0000000000407d1d in main (argc=3, argv=0x7fffffffe038) at nvmf_main.c:47
.create = nvmf_rdma_create
nvmf_rdma_create(struct spdk_nvmf_transport_opts *opts)
nvmf_rdma_accept
  nvmf_rdma_retry_listen_port -> nvmf/rdma：IB设备热插拔后重新创建资源和监听器，在修改bonding、IB设备的从属设备时，IB设备可能会被拔出和热插拔。 此补丁将在 IB 设备恢复后尝试重新创建 ibv 设备上下文、轮询器和侦听器
    nvmf_rdma_rescan_devices
      ibv_get_device_list -> 1
      ibv_open_device -> 1
      nvmf_rdma_find_ib_device
      rdma_get_devices
      nvmf_rdma_check_devices_context
      generate_poll_fds
    nvmf_rdma_listen
      rdma_create_id
      rdma_bind_addr
      rdma_listen
  count = nfds = poll(rtransport->poll_fds, rtransport->npoll_fds, 0)
  nvmf_process_cm_event
    while (1)
    rdma_get_cm_event
    ...
    case RDMA_CM_EVENT_CONNECT_REQUEST
      nvmf_rdma_connect(struct spdk_nvmf_transport *transport, struct rdma_cm_event *event)
        spdk_nvmf_tgt_new_qpair(transport->tgt, &rqpair->qpair)
          spdk_thread_send_msg(group->thread, _nvmf_poll_group_add, ctx) -> reactor
            spdk_nvmf_poll_group_add
              nvmf_transport_poll_group_add -> poll_group_add -> nvmf_rdma_poll_group_add
                nvmf_rdma_qpair_initialize
                  qp_init_attr.cap.max_send_wr	= (uint32_t)rqpair->max_queue_depth * 2 -> 设置qp属性
                  ...
                  nvmf_rdma_resize_cq
                  spdk_rdma_qp_create(rqpair->cm_id, &qp_init_attr)
                    struct ibv_qp_init_attr attr
                    .qp_type = IBV_QPT_RC
                    rdma_create_qp(cm_id, qp_attr->pd, &attr)
                  nvmf_rdma_resources_create -> 创建qp
                    rdma_recv->rdma_wr.type = RDMA_WR_TYPE_RECV
                    spdk_rdma_get_translation
                    spdk_rdma_memory_translation_get_lkey
                    spdk_rdma_qp_queue_recv_wrs -> 非共享接收队列
                      rdma_queue_recv_wrs -> rdma：为每个 qpair 添加统计信息 这些统计信息可以帮助估计工作请求批处理的效率并显示繁忙/空闲轮询比率。 发送：动词提供者的门铃更新统计信息在每次 ibv_post_send 调用时递增，对于 mlx5_dv 则在每次 ibv_wr_complete 调用时递增。 Recv：当调用 ibv_post_recv 或 ibv_post_srq_recv 函数时，两个提供程序的门铃更新统计信息都会更新。 初始化时的每个 qpair 接受一个指向共享统计信息（nvmf/nvme 轮询组）的可选指针。 如果未提供指向统计信息的指针，则 qpair 分配其自己的结构。 这样做是为了支持 NVME RDMA 启动器不使用轮询组的情况，因此我们可以避免检查 qpair 在 IO 路径中是否有统计信息
                    rdma_req->rsp.rdma_wr.type = RDMA_WR_TYPE_SEND
                    rdma_req->rsp.wr.opcode = IBV_WR_SEND
                    rdma_req->data.rdma_wr.type = RDMA_WR_TYPE_DATA
                    spdk_rdma_qp_flush_recv_wrs -> 提交所有的接收工作请求
                      ibv_post_recv
                RB_INSERT(qpairs_tree, &poller->qpairs, rqpair)
                nvmf_rdma_event_accept(rqpair->cm_id, rqpair)
                nvmf_rdma_update_ibv_state(rqpair)
              nvmf_qpair_set_state(qpair, SPDK_NVMF_QPAIR_ACTIVE)  
    ...
  nvmf_process_ib_events -> 随后的事件是ib异步事件


json, api: def nvmf_create_transport(args)



rdma_connect


nvmf_rdma_handle_cm_event_addr_change
nvmf_rdma_listen



tgt绑核: 将 CPU 核心分配给 NVMe over Fabrics 目标，SPDK 使用 DPDK 环境抽象层来访问硬件资源，例如大内存页面和 CPU 核心。 DPDK EAL 提供了将线程分配给特定内核的函数。 为确保 SPDK NVMe-oF 目标具有最佳性能，请将 NIC 和 NVMe 设备配置为位于同一 NUMA 节点上。-m core mask 选项指定允许 SPDK 执行工作项的 CPU 内核的位掩码。 例如，允许 SPDK 使用核心 24、25、26 和 27：
build/bin/nvmf_tgt -m 0xF000000



生成配置文件: ./scripts/gen_nvme.sh


nvmf_poll_groups_create
    spdk_thread_send_msg(thread, nvmf_tgt_create_poll_group, NULL)

nvmf_tgt_create_poll_group
group->poller = SPDK_POLLER_REGISTER(nvmf_poll_group_poll, group, 0);
nvmf_poll_group_add_transport
    nvmf_transport_poll_group_create
        tgroup = transport->ops->poll_group_create(transport, group)
poll_group_poll
nvmf_rdma_poll_group_poll
nvmf_rdma_poll_group_add | nvmf_rdma_poller_poll
  nvmf_rdma_event_accept
  reaped = ibv_poll_cq(rpoller->cq, 32, wc)
  spdk_nvmf_qpair_disconnect
nvmf_rdma_update_ibv_state
  ibv_query_qp




SPDK 加速框架介绍: https://www.sdnlab.com/25328.html
static struct accel_mlx5_module g_accel_mlx5 = {
  .module = {
    .module_init		= accel_mlx5_init,
accel_mlx5_init
spdk_io_device_register(&g_accel_mlx5, accel_mlx5_create_cb, accel_mlx5_destroy_cb,
accel_mlx5_create_cb
accel_mlx5_qp_create
mlx5_qp_init_2_rts






struct ibv_qp_attr
  uint8_t			timeout;
  uint8_t			retry_cnt;
  uint8_t			rnr_retry;


poll_group_add
nvmf_rdma_poll_group_add
nvmf_rdma_qpair_initialize
spdk_rdma_qp_create
rdma_create_qp



(gdb) bt
#0  0x00007ffff7533820 in ibv_modify_qp () from /lib64/libibverbs.so.1
#1  0x00007ffff7309307 in ucma_init_conn_qp () from /lib64/librdmacm.so.1
#2  0x00007ffff7309bcc in rdma_create_qp_ex () from /lib64/librdmacm.so.1
#3  0x00007ffff7309d3e in rdma_create_qp () from /lib64/librdmacm.so.1
#4  0x0000000000559c61 in spdk_rdma_qp_create (cm_id=0xd9ad20, qp_attr=0x7fffffffdd60) at rdma_verbs.c:47
#5  0x0000000000507ebd in nvmf_rdma_qpair_initialize (qpair=0xdc4280) at rdma.c:1025
#6  0x0000000000510afb in nvmf_rdma_poll_group_add (group=0xda5da0, qpair=0xdc4280) at rdma.c:4168
#7  0x00000000004f4b3e in nvmf_transport_poll_group_add (group=0xda5da0, qpair=0xdc4280) at transport.c:692
#8  0x00000000004e8ed1 in spdk_nvmf_poll_group_add (group=0xd2a060, qpair=0xdc4280) at nvmf.c:1066
#9  0x00000000004e8bb3 in _nvmf_poll_group_add (_ctx=0xd9af80) at nvmf.c:982
#10 0x000000000059cc53 in msg_queue_run_batch (thread=0xd83a90, max_msgs=8) at thread.c:841
#11 0x000000000059d5a6 in thread_poll (thread=0xd83a90, max_msgs=0, now=202236035489009) at thread.c:1063
#12 0x000000000059d8a6 in spdk_thread_poll (thread=0xd83a90, max_msgs=0, now=202236035489009) at thread.c:1156
#13 0x000000000055f629 in _reactor_run (reactor=0xd2a140) at reactor.c:914
#14 0x000000000055f718 in reactor_run (arg=0xd2a140) at reactor.c:952
#15 0x000000000055fb96 in spdk_reactors_start () at reactor.c:1068
#16 0x000000000055c055 in spdk_app_start (opts_user=0x7fffffffe3f0, start_fn=0x407ba5 <nvmf_tgt_started>, arg1=0x0) at app.c:808
#17 0x0000000000407cad in main (argc=3, argv=0x7fffffffe5b8) at nvmf_main.c:47

scp root@qcentos:/root/project/stor/spdk/spdk/build/bin/nvmf_tgt .
scp nvmf_tgt root@s63:/root/pkg/
gdb --args ./nvmf_tgt -L all
[2023-07-11 11:06:07.627500] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-11 11:06:07.740754] rdma.c:2594:nvmf_rdma_create: *INFO*: *** RDMA Transport Init ***
  Transport opts:  max_ioq_depth=128, max_io_size=131072,
  max_io_qpairs_per_ctrlr=127, io_unit_size=8192,
  in_capsule_data_size=4096, max_aq_depth=128,
  num_shared_buffers=4095, num_cqe=4096, max_srq_depth=4096, no_srq=0,  acceptor_backlog=100, no_wr_batching=0 abort_timeout_sec=1



  struct ibv_qp_attr qp_attr;
  struct ibv_qp_init_attr init_attr;
  ibv_query_qp(rqpair->rdma_qp->qp, &qp_attr, IBV_QP_STATE, &init_attr);
  SPDK_ERRLOG("SSSS qp timeout:%d, retry_cnt:%d, rnr_retry:%d\n",  qp_attr.timeout, qp_attr.retry_cnt, qp_attr.rnr_retry);


[2023-07-11 17:10:21.941507] nvme_qpair.c: 220:nvme_admin_qpair_print_command: *NOTICE*: FABRIC CONNECT qid:63 cid:12288 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-11 17:10:21.941515] ctrlr.c: 745:_nvmf_ctrlr_connect: *DEBUG*: recfmt 0x0 qid 63 sqsize 127


// 主动断开QP
char *file_name = "/tmp/control";
FILE *fp = NULL;
char control_str[1024] = {'\0'};

if (access(file_name, F_OK) == 0) {
  fp = fopen(file_name, "r");
  if (fp) {
    if(fgets(control_str, 1024, fp)) {
      SPDK_ERRLOG("SSSS content:%s\n", control_str);
      if (strncmp(control_str, "1", 1) == 0) {
        SPDK_ERRLOG("SSSS disconnect qp opcode:%d\n", wc[i].opcode);
        spdk_nvmf_qpair_disconnect(&rqpair->qpair, NULL, NULL);
        continue;
      }
    };
  };
};
if(fp)
  fclose(fp);

[root@s63 ~]# date;echo 1 > /tmp/control
[root@s63 ~]# date;echo 0 > /tmp/control



git tag|sort -n
dpdk build error, gcc9.4 编译 dpdk20.11.3 avx512 flag引发的编译错误: https://blog.csdn.net/bostonrayalen/article/details/127771812
acl_avx512_on
net_crc_sse42_cpu_support
net_crc_sse42_cc_support


FAILED: lib/librte_hash.a.p/hash_rte_thash.c.o 
gcc 版本不宜过高
rm -rf /bin/gcc
cp /bin/gcc_bak /bin/gcc
gcc version 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC) 



export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/h3c/lib
#echo 4096 >/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages; cat /proc/meminfo|grep -i huge
cd /root/pkg/tgt/sbin
./tgtd -L all
ps aux|grep tgt

/opt/h3c/script/tgt
rsync -rlpva tgt root@s62:/opt/h3c/script/


spdk常用命令, 创建rdma传输层
/opt/h3c/script/tgt/rpc.py nvmf_create_transport -t RDMA -u 1048576 -m 8 -c 4096 -i 1048576 -r; /opt/h3c/script/tgt/rpc.py nvmf_get_transports

# 从大页内存创建bdev
/opt/h3c/script/tgt/rpc.py bdev_malloc_create -b Malloc0 512 2048; /opt/h3c/script/tgt/rpc.py bdev_get_bdevs

# 创建子系统
/opt/h3c/script/tgt/rpc.py nvmf_create_subsystem nqn.2022-06.io.spdk:cnode216 -m 512 -r -a -s SPDK00000000000001 -d SPDK_Controll; /opt/h3c/script/tgt/rpc.py nvmf_get_subsystems

# 将ns添加到subsystem: 命名空间 -> NVME子系统
/opt/h3c/script/tgt/rpc.py nvmf_subsystem_add_ns nqn.2022-06.io.spdk:cnode216 Malloc0; /opt/h3c/script/tgt/rpc.py nvmf_get_subsystems

# 为nvme子系统添加监听地址
/opt/h3c/script/tgt/rpc.py nvmf_subsystem_add_listener nqn.2022-06.io.spdk:cnode216 -t rdma -a 172.17.29.64  -s 4420; /opt/h3c/script/tgt/rpc.py nvmf_subsystem_get_listeners 'nqn.2022-06.io.spdk:cnode216'

# 查询nvme子系统状态(详情)
/opt/h3c/script/tgt/rpc.py nvmf_get_stats


# 放行块服务网段
/opt/h3c/script/tgt/rpc.py nvmf_put_blockservice2transport -t rdma -b 172.17.29.0/24


rpm -qli nvme_xxx.rpm
# host -> tgt, 发现tgt
nvme discover -t rdma -a 172.17.29.64 -s 4420

# 连接tgt
nvme connect -t rdma -n "nqn.2022-06.io.spdk:cnode216" -a  172.17.29.64 -s 4420

# 断开
nvme disconnect-all
# 两种disconnect的方式，一种是指定盘符，一种是指定subnqn
nvme disconnect  -d /dev/nvme0
nvme disconnect -n "nqn.2022-06.io.spdk:cnode216"




/opt/h3c/script/tgt/rpc.py nvmf_create_subsystem nqn.2022-06.io.spdk:cnode216 -m 512 -r -a -s SPDK00000000000001 -d SPDK_Controll  # /opt/h3c/script/tgt/rpc.py nvmf_subsystem_add_ns nqn.2022-06.io.spdk:cnode216 Malloc0
/opt/h3c/script/tgt/rpc.py nvmf_subsystem_add_listener nqn.2022-06.io.spdk:cnode216 -t rdma -a 0.0.0.0 -s 4420 # /opt/h3c/script/tgt/rpc.py nvmf_subsystem_add_listener nqn.2022-06.io.spdk:cnode216 -t rdma -a 172.17.29.217  -s 4420
/opt/h3c/script/tgt/rpc.py bdev_dbd_create pool lun_test1 512 -u 497a9e48-9663-4382-aada-4df94cbc1289 --name test # 创建内存盘: /opt/h3c/script/tgt/rpc.py bdev_malloc_create -b Malloc0 512 2048          # bdev_malloc_delete
/opt/h3c/script/tgt/rpc.py nvmf_subsystem_add_ns nqn.2022-06.io.spdk:cnode216 test
/opt/h3c/script/tgt/rpc.py nvmf_put_blockservice2transport -t rdma -b 172.17.29.0/24

查询传输层: /opt/bk_h3c/script/tgt/rpc.py nvmf_get_transports
查询子系统: /opt/bk_h3c/script/tgt/rpc.py nvmf_get_subsystems
查询子系统状态: /opt/bk_h3c/script/tgt/rpc.py nvmf_get_stats|jq .
查询监听: /opt/bk_h3c/script/tgt/rpc.py nvmf_subsystem_get_listeners 'nqn.2014-08.org.nvmexpress.discovery'
查看bdev: /opt/bk_h3c/script/tgt/rpc.py bdev_get_bdevs


nvme:
nvme discover -t rdma -a 172.17.29.217 -s 4420


nvme_of_target: https://github.com/spdk/spdk/issues/2082, 
# 创建一个大小是64M，每个块大小是512的malloc bdev，命名为Malloc0
scripts/rpc.py construct_malloc_bdev -b Malloc0 64 512

#创建一个TCP transport
scripts/rpc.py nvmf_create_transport -t TCP -p 4

#创建一个 NVMe-oF subsystem
scripts/rpc.py nvmf_subsystem_create nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001

#在这个subsystem的中添加Malloc0，作为相应的subsystem
scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 Malloc0

#在命名为nqn.2016-06.io.spdk:cnode1的subsystem 上创建一个监听socket，其中IP 地址是192.168.0.128 IP，TCP端口是4420
scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t tcp -a 192.168.0.128 -s 4420

host:
modprobe nvme_tcp
nvme connect -t tcp -n "nqn.2016-06.io.spdk:cnode1" -a 192.168.0.128 -s 4420


创建内存测试盘
scripts/rpc.py bdev_malloc_create -b Malloc0 512 2048 #1GB

#scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode1 -a -s SPDK00000000000001 -d SPDK_Controller1
# scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode1 Malloc0
# scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode1 -t rdma -a 192.168.80.100 -s 4420

创建nvme盘

# scripts/rpc.py bdev_nvme_attach_controller -b Nvme0 -t PCIe -a 0000:01:00.0
# scripts/rpc.py nvmf_create_subsystem nqn.2016-06.io.spdk:cnode2 -a -s SPDK00000000000002 -d SPDK_Controller1
# scripts/rpc.py nvmf_subsystem_add_ns nqn.2016-06.io.spdk:cnode2 Nvme0n1
# scripts/rpc.py nvmf_subsystem_add_listener nqn.2016-06.io.spdk:cnode2 -t rdma -a 192.168.80.100 -s 4420

rdma驱动:
[root@localhost MLNX_OFED_LINUX-5.7-1.0.2.0-rhel7.9-x86_64]# ./mlnxofedinstall --without-depcheck --add-kernel-support --kmp --force


/opt/h3c/etc/tgt/tgt_skip_recovery.flag 
/home/lh/perf/debug 






/opt/h3c/script/tgt/rpc.py nvmf_create_transport -t RDMA -u 1048576 -m 8 -c 4096 -i 1048576 -r; /opt/h3c/script/tgt/rpc.py nvmf_get_transports
/opt/h3c/script/tgt/rpc.py bdev_malloc_create -b Malloc0 400 512 -u 41a7f127-38ea-4390-b5c6-ab0fed80f5d3; /opt/h3c/script/tgt/rpc.py bdev_get_bdevs
/opt/h3c/script/tgt/rpc.py nvmf_create_subsystem nqn.2022-06.io.spdk:cnode216 -m 512 -r -a -s SPDK00000000000001 -d SPDK_Controll; /opt/h3c/script/tgt/rpc.py nvmf_get_subsystems
/opt/h3c/script/tgt/rpc.py nvmf_subsystem_add_ns nqn.2022-06.io.spdk:cnode216 Malloc0; /opt/h3c/script/tgt/rpc.py nvmf_get_subsystems
/opt/h3c/script/tgt/rpc.py nvmf_subsystem_add_listener nqn.2022-06.io.spdk:cnode216 -t rdma -a 0.0.0.0  -s 4420; /opt/h3c/script/tgt/rpc.py nvmf_subsystem_get_listeners 'nqn.2022-06.io.spdk:cnode216'
/opt/h3c/script/tgt/rpc.py nvmf_put_blockservice2transport -t rdma -b 172.17.29.0/24

# 创建内存块 总大小(M),每块大小(字节)
/opt/h3c/script/tgt/rpc.py nvmf_create_transport -t RDMA --no-srq; /opt/h3c/script/tgt/rpc.py nvmf_get_transports
/opt/h3c/script/tgt/rpc.py bdev_malloc_create -b Malloc0 1024 512 -u 41a7f127-38ea-4390-b5c6-ab0fed80f5d3; /opt/h3c/script/tgt/rpc.py bdev_get_bdevs
/opt/h3c/script/tgt/rpc.py nvmf_create_subsystem nqn.2022-06.io.spdk:cnode216 -m 512 -r -a -s SPDK00000000000001 -d SPDK_Controll; /opt/h3c/script/tgt/rpc.py nvmf_get_subsystems
/opt/h3c/script/tgt/rpc.py nvmf_subsystem_add_ns nqn.2022-06.io.spdk:cnode216 Malloc0; /opt/h3c/script/tgt/rpc.py nvmf_get_subsystems
/opt/h3c/script/tgt/rpc.py nvmf_subsystem_add_listener nqn.2022-06.io.spdk:cnode216 -t rdma -a 172.17.29.63 -s 4420; /opt/h3c/script/tgt/rpc.py nvmf_subsystem_get_listeners 'nqn.2022-06.io.spdk:cnode216'


常用命令:
nvme list-subsys
nvme connect -t rdma -n "nqn.2022-06.io.spdk:cnode216" -a  172.17.29.66 -s 4420 -c2 -k3
cat /sys/module/nvme_core/parameters/multipath
modprobe nvme-rdma nvme-fabrics nvme nvme-core
modprobe nvme-core multipath=N
ln -s /opt/h3c/script/tgt/rpc.py /usr/bin/tgtmgr

ipmitool lan print 1

nvmf_tgt依赖动态库:
[root@s63 spdk]# ldd ./build/bin/nvmf_tgt 
        linux-vdso.so.1 =>  (0x00007ffce09f9000)
        libssl.so.1.1 => /lib64/libssl.so.1.1 (0x00007f8529515000)
        libnuma.so.1 => /lib64/libnuma.so.1 (0x00007f8529309000)
        libdl.so.2 => /lib64/libdl.so.2 (0x00007f8529105000)
        libibverbs.so.1 => /lib64/libibverbs.so.1 (0x00007f8528ee6000)
        librdmacm.so.1 => /lib64/librdmacm.so.1 (0x00007f8528ccc000)
        librt.so.1 => /lib64/librt.so.1 (0x00007f8528ac4000)
        libuuid.so.1 => /lib64/libuuid.so.1 (0x00007f85288bf000)
        libcrypto.so.1.1 => /lib64/libcrypto.so.1.1 (0x00007f85283db000)
        libm.so.6 => /lib64/libm.so.6 (0x00007f85280d9000)
        libaio.so.1 => /lib64/libaio.so.1 (0x00007f8527ed7000)
        libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f8527cbb000)
        libc.so.6 => /lib64/libc.so.6 (0x00007f85278ed000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f85297a5000)
        libz.so.1 => /lib64/libz.so.1 (0x00007f85276d7000)
        libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007f85274c1000)
        libnl-route-3.so.200 => /lib64/libnl-route-3.so.200 (0x00007f8527254000)
        libnl-3.so.200 => /lib64/libnl-3.so.200 (0x00007f8527033000)


spdk_nvme_print_command
nvme_admin_qpair_print_command
  case SPDK_NVME_OPC_FABRIC
  SPDK_NOTICELOG("%s %s qid:%d cid:%d %s



nvmf_ctrlr_cmd_connect
nvmf_qpair_access_allowed
spdk_nvmf_subsystem_listener_allowed


#0  nvme_admin_qpair_print_command (qid=0, cmd=0x2000070df000) at nvme_qpair.c:202
#1  0x00000000005333b8 in spdk_nvme_print_command (qid=0, cmd=0x2000070df000) at nvme_qpair.c:267
#2  0x00000000004d52be in spdk_nvmf_request_exec (req=0x2000070d4000) at ctrlr.c:4556
#3  0x000000000050b800 in nvmf_rdma_request_process (rtransport=0xd9a440, rdma_req=0x2000070d4000) at rdma.c:2239
#4  0x0000000000511f29 in nvmf_rdma_poller_poll (rtransport=0xd9a440, rpoller=0xda4e70) at rdma.c:4623
#5  0x0000000000512333 in nvmf_rdma_poll_group_poll (group=0xda4da0) at rdma.c:4767
#6  0x00000000004f4d04 in nvmf_transport_poll_group_poll (group=0xda4da0) at transport.c:715
#7  0x00000000004e6a93 in nvmf_poll_group_poll (ctx=0xd29060) at nvmf.c:70
#8  0x000000000059d238 in thread_execute_poller (thread=0xd82a90, poller=0xd82e20) at thread.c:946
#9  0x000000000059d7bb in thread_poll (thread=0xd82a90, max_msgs=0, now=496007760725265) at thread.c:1072
#10 0x000000000059da5b in spdk_thread_poll (thread=0xd82a90, max_msgs=0, now=496007760725265) at thread.c:1156
#11 0x000000000055f7de in _reactor_run (reactor=0xd29140) at reactor.c:914
#12 0x000000000055f8cd in reactor_run (arg=0xd29140) at reactor.c:952
#13 0x000000000055fd4b in spdk_reactors_start () at reactor.c:1068
#14 0x000000000055c20a in spdk_app_start (opts_user=0x7fffffffde70, start_fn=0x407c15 <nvmf_tgt_started>, arg1=0x0) at app.c:808
#15 0x0000000000407d1d in main (argc=3, argv=0x7fffffffe038) at nvmf_main.c:47



nvmf_rdma_qpair_process_pending
  STAILQ_FOREACH_SAFE nvmf_rdma_request_process
    spdk_nvmf_request_exec
      nvmf_ctrlr_process_admin_cmd
        case SPDK_NVME_OPC_GET_LOG_PAGE
        nvmf_ctrlr_get_log_page
          spdk_nvmf_qpair_get_listen_trid
            .qpair_get_listen_trid = nvmf_rdma_qpair_get_listen_trid
              spdk_nvme_trid_populate_transport
          nvmf_get_discovery_log_page
            nvmf_generate_discovery_log
              spdk_nvmf_subsystem_get_first
              spdk_nvmf_subsystem_get_next
                RB_NEXT(subsystem_tree, &tgt->subsystems, subsystem) -> 创建subsystem的时候将数据插入红黑树: RB_INSERT -> nvmf：使用 RB 树来跟踪 tgt 子系统 目前我们使用数组，这对于许多子系统来说计算成本很高，因为查找需要对子系统 nqns 进行 O(n) 字符串比较。 它的容量并不昂贵，因为它只是一个指针数组。 因此，将其从指针数组切换为 RB_HEAD，这样我们就可以将查找次数减少到 O(log n) 字符串比较。 请注意，我们仍然会为每个传输轮询组分配 spdk_nvmf_subsystem_poll_groups 数组，因为我们不想在 IO 路径中产生 RB_FIND 的额外成本
              nvmf_transport_listener_discover
                nvmf_rdma_discover
                  entry->trtype = SPDK_NVMF_TRTYPE_RDMA
                  ...
                  entry->tsas.rdma.rdma_qptype = SPDK_NVMF_RDMA_QPTYPE_RELIABLE_CONNECTED
                  entry->tsas.rdma.rdma_cms = SPDK_NVMF_RDMA_CMS_RDMA_CM





#0  nvmf_ctrlr_get_log_page (req=0x2000070d4560) at ctrlr.c:2517
#1  0x00000000004d2dbc in nvmf_ctrlr_process_admin_cmd (req=0x2000070d4560) at ctrlr.c:3592
#2  0x00000000004d5365 in spdk_nvmf_request_exec (req=0x2000070d4560) at ctrlr.c:4566
#3  0x000000000050b800 in nvmf_rdma_request_process (rtransport=0xd9a440, rdma_req=0x2000070d4560) at rdma.c:2239
#4  0x000000000050ea12 in nvmf_rdma_qpair_process_pending (rtransport=0xd9a440, rqpair=0xda5ec0, drain=false) at rdma.c:3276
#5  0x00000000005120ab in nvmf_rdma_poller_poll (rtransport=0xd9a440, rpoller=0xda4e70) at rdma.c:4685
#6  0x0000000000512333 in nvmf_rdma_poll_group_poll (group=0xda4da0) at rdma.c:4767
#7  0x00000000004f4d04 in nvmf_transport_poll_group_poll (group=0xda4da0) at transport.c:715
#8  0x00000000004e6a93 in nvmf_poll_group_poll (ctx=0xd29060) at nvmf.c:70
#9  0x000000000059d238 in thread_execute_poller (thread=0xd82a90, poller=0xd82e20) at thread.c:946
#10 0x000000000059d7bb in thread_poll (thread=0xd82a90, max_msgs=0, now=514611841303047) at thread.c:1072
#11 0x000000000059da5b in spdk_thread_poll (thread=0xd82a90, max_msgs=0, now=514611841303047) at thread.c:1156
#12 0x000000000055f7de in _reactor_run (reactor=0xd29140) at reactor.c:914
#13 0x000000000055f8cd in reactor_run (arg=0xd29140) at reactor.c:952
#14 0x000000000055fd4b in spdk_reactors_start () at reactor.c:1068
#15 0x000000000055c20a in spdk_app_start (opts_user=0x7fffffffde70, start_fn=0x407c15 <nvmf_tgt_started>, arg1=0x0) at app.c:808
#16 0x0000000000407d1d in main (argc=3, argv=0x7fffffffe038) at nvmf_main.c:47



spdk_nvmf_subsystem_create
  RB_INSERT(subsystem_tree, &tgt->subsystems, subsystem)



spdk_nvme_async_event_completion


pip3 install configshell_fb
查看存储配置: ./scripts/spdkcli.py ls


#0  nvmf_rdma_create (opts=0xd26ee8) at rdma.c:2553
#1  0x00000000004f3ad9 in _nvmf_transport_create_done (ctx=0xd26ee0) at transport.c:220
#2  0x000000000059ce08 in msg_queue_run_batch (thread=0xd296d0, max_msgs=8) at thread.c:841
#3  0x000000000059d75b in thread_poll (thread=0xd296d0, max_msgs=0, now=1082734976806585) at thread.c:1063
#4  0x000000000059da5b in spdk_thread_poll (thread=0xd296d0, max_msgs=0, now=1082734976806585) at thread.c:1156
#5  0x000000000055f7de in _reactor_run (reactor=0xd29140) at reactor.c:914
#6  0x000000000055f8cd in reactor_run (arg=0xd29140) at reactor.c:952
#7  0x000000000055fd4b in spdk_reactors_start () at reactor.c:1068
#8  0x000000000055c20a in spdk_app_start (opts_user=0x7fffffffde70, start_fn=0x407c15 <nvmf_tgt_started>, arg1=0x0) at app.c:808
#9  0x0000000000407d1d in main (argc=3, argv=0x7fffffffe038) at nvmf_main.c:47



创建内存块设备 -> bdev_malloc_create -> rpc_bdev_malloc_create
create_malloc_disk -> dev/malloc：使用 options 结构体创建 malloc bdev 定义一个 options 结构体 malloc_bdev_opts，并直接将其用于 bdev_malloc_create RPC。 为此，bdev_malloc.h 包含 bdev_module.h 而不是 bdev.h 来定义 struct spdk_uuid，并且 struct malloc_bdev_opts 具有 struct spdk_uuid 的实例。 一起清理文件包含。 此外，使用 spdk_uuid_copy() 将 uuid 从 malloc_bdev_opts 复制到 malloc 磁盘而不是 = 运算符，并删除重复的大小检查。 这些有助于添加更多参数进行创建
    struct malloc_disk *mdisk
    opts->block_size % 512
    opts->physical_block_size % 512
    opts->md_size
    mdisk = calloc(1, sizeof(*mdisk)) -> 使用env库执行之前直接调用DPDK的所有内存分配
    mdisk->malloc_buf = spdk_zmalloc(opts->num_blocks * block_size, 2 * 1024 * 1024, NULL,SPDK_ENV_LCORE_ID_ANY, SPDK_MALLOC_DMA) -> hugepage -> bdev/malloc：支持交叉和分离元数据 malloc bdev 模块在此补丁中支持交错和分离元数据。 与 NULL bdev 模块不同，opts->block_size 是数据块大小，如果 opts->md_interleave 为 true，则块大小在内部计算为 opts->block_size 和 opts->md_size 之和，否则为 opts->block_size 。 这样会更直观。 此外，opts->md_size 仅接受 0、8、16、32、64 或 128 中的一个。以下补丁将支持保护信息 (T10 DIF/DIX)
    mdisk->malloc_md_buf = spdk_zmalloc(opts->num_blocks * opts->md_size, 2 * 1024 * 1024, NULL, SPDK_ENV_LCORE_ID_ANY, SPDK_MALLOC_DMA) -> disk meta data
    mdisk->disk.name = strdup(opts->name)
    if (opts->dif_type != SPDK_DIF_DISABLE)
        malloc_disk_setup_pi(mdisk)
    if (opts->optimal_io_boundary) -> bdev/malloc：使用选项结构创建 malloc bdev 定义选项结构 malloc_bdev_opts，并将其直接用于 bdev_malloc_create RPC。为此，bdev_malloc.h 包含 bdev_module.h 而不是 bdev.h 以定义 struct spdk_uuid，而 struct malloc_bdev_opts 具有 struct spdk_uuid 的实例。一起清理文件包含。此外，使用 spdk_uuid_copy() 将 uuid 从 malloc_bdev_opts 复制到 malloc 磁盘，而不是 = 运算符，并删除重复的大小检查。这些有助于添加更多创建参数
    mdisk->disk.fn_table = &malloc_fn_table;
    mdisk->disk.module = &malloc_if;
    spdk_bdev_register(&mdisk->disk)
    TAILQ_INSERT_TAIL(&g_malloc_disks, mdisk, link)


static const struct spdk_bdev_fn_table malloc_fn_table = {
	.destruct			= bdev_malloc_destruct,
	.submit_request			= bdev_malloc_submit_request,
	.io_type_supported		= bdev_malloc_io_type_supported,
	.get_io_channel			= bdev_malloc_get_io_channel,
    spdk_get_io_channel(&g_malloc_disks)
	.write_config_json		= bdev_malloc_write_json_config,
	.get_memory_domains		= bdev_malloc_get_memory_domains,
        spdk_memory_domain_get_first
        spdk_memory_domain_get_next
	.accel_sequence_supported	= bdev_malloc_accel_sequence_supported,
};

static struct spdk_bdev_module malloc_if = {
	.name = "malloc",
	.module_init = bdev_malloc_initialize,
        spdk_io_device_register(&g_malloc_disks, malloc_create_channel_cb, malloc_destroy_channel_cb, sizeof(struct malloc_channel), "bdev_malloc")
	.module_fini = bdev_malloc_deinitialize,
	.get_ctx_size = bdev_malloc_get_ctx_size,
};


malloc_create_channel_cb
    ch->accel_channel = spdk_accel_get_io_channel()
        spdk_get_io_channel
    ch->completion_poller = SPDK_POLLER_REGISTER(malloc_completion_poller, ch, 0)
    TAILQ_INIT(&ch->completed_tasks)


#0  rpc_nvmf_create_subsystem (request=0xd16cb0, params=0xdab250) at nvmf_rpc.c:385
#1  0x00000000005a647c in jsonrpc_handler (request=0xd16cb0, method=0xdab210, params=0xdab250) at rpc.c:124
#2  0x00000000005a8598 in jsonrpc_server_handle_request (request=0xd16cb0, method=0xdab210, params=0xdab250) at jsonrpc_server_tcp.c:222
#3  0x00000000005a7028 in parse_single_request (request=0xd16cb0, values=0xdab1d0) at jsonrpc_server.c:75
#4  0x00000000005a760f in jsonrpc_parse_request (conn=0x7ffff3e62040, json=0x7ffff3e62058, size=339) at jsonrpc_server.c:205
#5  0x00000000005a87b8 in jsonrpc_server_conn_recv (conn=0x7ffff3e62040) at jsonrpc_server_tcp.c:284
#6  0x00000000005a8b8a in spdk_jsonrpc_server_poll (server=0x7ffff3e62010) at jsonrpc_server_tcp.c:402
#7  0x00000000005a6775 in spdk_rpc_accept () at rpc.c:213
#8  0x0000000000596788 in rpc_subsystem_poll (arg=0x0) at rpc.c:21

SPDK_RPC_REGISTER("nvmf_create_subsystem", rpc_nvmf_create_subsystem, SPDK_RPC_RUNTIME)
创建nvme子系统: nvmf_create_subsystem -> rpc_nvmf_create_subsystem
tgt = spdk_nvmf_get_tgt(req->tgt_name)
subsystem = spdk_nvmf_subsystem_create
  spdk_nvmf_tgt_find_subsystem
  nvmf_valid_nqn
  NVMF_SUBSYSTEM_DEFAULT_NAMESPACES 32
  spdk_bit_array_find_first_clear
  subsystem = calloc
  subsystem->used_listener_ids = spdk_bit_array_create
  RB_INSERT(subsystem_tree, &tgt->subsystems, subsystem)
spdk_nvmf_subsystem_set_sn <- if (req->serial_number)
spdk_nvmf_subsystem_set_mn SPDK_Controll
spdk_nvmf_subsystem_set_allow_any_host
spdk_nvmf_subsystem_set_ana_reporting
nvmf_subsystem_set_cntlid_range
spdk_nvmf_subsystem_start(subsystem, rpc_nvmf_subsystem_started, request)
    nvmf_subsystem_state_change(subsystem, 0, SPDK_NVMF_SUBSYSTEM_ACTIVE, cb_fn, cb_arg) -> nvmf：子系统暂停仅暂停管理队列此外，用户可以指定一个命名空间以在操作期间也暂停。 这允许在发生对其他命名空间的 I/O 时管理主机、侦听器以及添加命名空间。 暂停特定命名空间还允许删除该命名空间，而不会影响子系统中其他命名空间的 I/O
        __sync_val_compare_and_swap(&subsystem->changing_state, false, true) -> 互斥, 避免两次改状态, type __sync_val_compare_and_swap (type *ptr, type oldval type newval, ...) // 比较*ptr与oldval的值，如果两者相等，则将newval更新到*ptr并返回操作之前*ptr的值, https://zhuanlan.zhihu.com/p/32303037, 原子操作, 这对于避免双重状态更改和命中断言非常重要
        SPDK_DTRACE_PROBE3 -> bpf trace
        nvmf_subsystem_get_intermediate_state
        nvmf_subsystem_set_state
            exchanged = __atomic_compare_exchange_n(&subsystem->state, &actual_old_state, state, false, __ATOMIC_RELAXED, __ATOMIC_RELAXED) -> __atomic_compare_exchange_n函数来比较value的值与期望值是否相等。如果相等，则将新值存储到value中，并返回true；否则返回false, 我们还需要指定内存顺序（memory order），以确保正确的同步。在本例中，我们使用了std::memory_order_acq_rel和std::memory_order_acquire，它们分别表示“获取/释放”顺序和“获取”顺序。具体选择哪种内存顺序取决于特定的应用场景
        spdk_for_each_channel  subsystem_state_change_on_pg -> 在与 io_device 关联的每个通道上调用“fn”。 这是异步发生的，因此可以在 spdk_for_each_channel 返回后调用 fn 。 将为每个通道串行调用“fn”，这样对“fn”的两次调用不会在时间上重叠。 调用“fn”后，调用 spdk_for_each_channel_continue() 继续迭代
            spdk_io_channel_iter_get_ctx
            spdk_io_channel_iter_get_channel
            group = spdk_io_channel_get_ctx(ch)
            switch (ctx->requested_state) -> Nvmeof请求状态机
                case SPDK_NVMF_SUBSYSTEM_ACTIVE
                    nvmf_poll_group_add_subsystem(group, ctx->subsystem, subsystem_state_change_continue, i)
                        poll_group_update_subsystem(group, subsystem) -> nvmf：为子系统添加同步原语，这允许用户暂停子系统，进行一些修改，然后恢复它
                            if (ns_changed)
                                if (ctrlr->admin_qpair->group == group) 
                                    nvmf_ctrlr_async_event_ns_notice(ctrlr)
                                        nvmf_ctrlr_mask_aen(ctrlr, SPDK_NVME_ASYNC_EVENT_NS_ATTR_CHANGE_MASK_BIT)
                                        event.bits.async_event_type = SPDK_NVME_ASYNC_EVENT_TYPE_NOTICE;
                                        event.bits.async_event_info = SPDK_NVME_ASYNC_EVENT_NS_ATTR_CHANGED;
                                        event.bits.log_page_identifier = SPDK_NVME_LOG_CHANGED_NS_LIST;
                                        nvmf_ctrlr_async_event_notification(ctrlr, &event)
                                            nvmf_ctrlr_queue_pending_async_event(ctrlr, event)
                                                STAILQ_INSERT_TAIL(&ctrlr->async_events, nvmf_event, link)
                                            _nvmf_request_complete(req)
                                    nvmf_ctrlr_async_event_ana_change_notice(ctrlr)
                        sgroup->state = SPDK_NVMF_SUBSYSTEM_ACTIVE
                        cb_fn(cb_arg, rc) -> subsystem_state_change_continue
                            spdk_for_each_channel_continue
                            thread_get_io_channel
                            spdk_thread_send_msg(thread, _call_channel, i) -> i->fn(i) -> subsystem_state_change_on_pg
                            spdk_thread_send_msg(i->orig_thread, _call_completion, i)
                            poll_group_update_subsystem



NvmeOf子系统状态机:
enum spdk_nvmf_subsystem_state {
	SPDK_NVMF_SUBSYSTEM_INACTIVE = 0,
	SPDK_NVMF_SUBSYSTEM_ACTIVATING,
	SPDK_NVMF_SUBSYSTEM_ACTIVE,
	SPDK_NVMF_SUBSYSTEM_PAUSING,
	SPDK_NVMF_SUBSYSTEM_PAUSED,
	SPDK_NVMF_SUBSYSTEM_RESUMING,
	SPDK_NVMF_SUBSYSTEM_DEACTIVATING,
	SPDK_NVMF_SUBSYSTEM_NUM_STATES,
};


绑定命名空间: nvmf_subsystem_add_ns -> rpc_nvmf_subsystem_add_ns
spdk_nvmf_tgt_find_subsystem
spdk_nvmf_subsystem_pause nvmf_rpc_ns_paused <- subsystem_state_change_done 将 NVMe-oF 子系统从活动状态转换为暂停状态。 在暂停状态下，整个子系统的所有管理队列都被冻结。 如果提供了命名空间 ID，则该命名空间的所有命令都会停止，并且该命名空间的传入命令将排队，直到子系统恢复
  spdk_for_each_channel subsystem_state_change_on_pg
  ...
  ns_opts.anagrpid = ctx->ns_params.anagrpid -> nvmf：通过 RPC 将 ns 添加到子系统时使 anagrpid 可配置
  spdk_nvmf_subsystem_add_ns_ext
    spdk_nvmf_ns_opts_get_defaults
    _nvmf_subsystem_get_ns
    ns = calloc(1, sizeof(*ns))
    spdk_bdev_open_ext(bdev_name, true, nvmf_ns_event, ns, &ns->desc)
      bdev_get_by_name
        RB_FIND(bdev_name_tree, &g_bdev_mgr.bdev_names, &find) -> 去红黑树上找块设备
      bdev_desc_alloc
        spdk_bdev_get_memory_domains
        desc->callback.event_fn = event_cb -> nvmf_ns_event
      bdev_open
        bdev_start_qos -> no qos
    spdk_bdev_desc_get_bdev
    spdk_bdev_get_md_size
    spdk_bdev_module_claim_bdev
    spdk_bdev_io_type_supported
      bdev_io_type_supported io_type=SPDK_BDEV_IO_TYPE_ZCOPY
    spdk_mem_all_zero
    spdk_bdev_is_zoned
    spdk_bdev_io_type_supported
    spdk_bdev_get_max_zone_append_size
    _nvmf_subsystem_get_first_zoned_ns
        struct spdk_nvmf_ns *ns = spdk_nvmf_subsystem_get_first_ns(subsystem)
            nvmf_subsystem_get_next_allocated_nsid(subsystem, 0)
            _nvmf_subsystem_get_ns(subsystem, first_nsid)
        ns = spdk_nvmf_subsystem_get_next_ns(subsystem, ns)
    nvmf_subsystem_ns_changed
      nvmf_ctrlr_ns_changed
    resume: -> nvmf_rpc_ns_paused
    spdk_nvmf_subsystem_resume(subsystem, nvmf_rpc_ns_resumed, ctx)
      


io路径, iopath
接收initiator的io
nvmf_rdma_qpair_process_pending
nvmf_rdma_request_parse_sgl
  nvme_io_qpair_print_command
    RDMA_REQUEST_STATE_READY_TO_EXECUTE
    nvme_io_qpair_print_command
    case SPDK_NVME_OPC_WRITE
    bdev_malloc_writev
      SPDK_DEBUGLOG(bdev_malloc, "wrote %zu bytes to offset %#" PRIx64 ", iovcnt=%d\n"

nvmf_rdma_request_fill_iovs

#0  bdev_malloc_writev (mdisk=0xd9a1b0, ch=0xdab470, task=0x200013aa0b60, bdev_io=0x200013aa0780) at bdev_malloc.c:290
#1  0x0000000000408d32 in _bdev_malloc_submit_request (mch=0xdab440, bdev_io=0x200013aa0780) at bdev_malloc.c:407
#2  0x0000000000408f53 in bdev_malloc_submit_request (ch=0xdab3e0, bdev_io=0x200013aa0780) at bdev_malloc.c:462
#3  0x000000000056a705 in bdev_submit_request (bdev=0xd9a1b0, ioch=0xdab3e0, bdev_io=0x200013aa0780) at bdev.c:1415 -> .submit_request			= bdev_malloc_submit_request,
#4  0x000000000056d238 in bdev_io_do_submit (bdev_ch=0xdab330, bdev_io=0x200013aa0780) at bdev.c:2636
#5  0x000000000056ed36 in _bdev_io_submit (ctx=0x200013aa0780) at bdev.c:3333
#6  0x000000000056f2ec in bdev_io_submit (bdev_io=0x200013aa0780) at bdev.c:3453
#7  0x000000000056f460 in _bdev_io_submit_ext (desc=0xdab010, bdev_io=0x200013aa0780) at bdev.c:3505
#8  0x0000000000573a29 in bdev_writev_blocks_with_md (desc=0xdab010, ch=0xdab2d0, iov=0x20005a47b040, iovcnt=7, md_buf=0x0, offset_blocks=20, num_blocks=101, domain=0x0, domain_ctx=0x0, seq=0x0, cb=0x4d9715 <nvmf_bdev_ctrlr_complete_cmd>, cb_arg=0x20005a47b000)
    at bdev.c:5438
---Type <return> to continue, or q <return> to quit---
#9  0x0000000000573b2c in spdk_bdev_writev_blocks (desc=0xdab010, ch=0xdab2d0, iov=0x20005a47b040, iovcnt=7, offset_blocks=20, num_blocks=101, cb=0x4d9715 <nvmf_bdev_ctrlr_complete_cmd>, cb_arg=0x20005a47b000) at bdev.c:5465  -> 向给定通道上的 bdev 提交写入请求。 这与 spdk_bdev_write 的不同之处在于允许在分散收集列表中描述数据缓冲区。 某些物理设备对数据提出内存对齐要求，并且可能无法直接从提供的缓冲区中传输出去。 在这种情况下，请求可能会失败
#10 0x00000000004da249 in nvmf_bdev_ctrlr_write_cmd (bdev=0xd9a1b0, desc=0xdab010, ch=0xdab2d0, req=0x20005a47b000) at ctrlr_bdev.c:345
#11 0x00000000004d44a3 in nvmf_ctrlr_process_io_cmd (req=0x20005a47b000) at ctrlr.c:4261
#12 0x00000000004d5376 in spdk_nvmf_request_exec (req=0x20005a47b000) at ctrlr.c:4568
#13 0x000000000050b800 in nvmf_rdma_request_process (rtransport=0xd9a440, rdma_req=0x20005a47b000) at rdma.c:2239


.submit_request	bdev_daos_submit_request | bdev_nvme_submit_request

bdev_nvme_submit_request
  spdk_io_channel_get_ctx
  nbdev_io->io_path = bdev_nvme_find_io_path(nbdev_ch)
  _bdev_nvme_submit_request
    bdev_nvme_writev
      spdk_nvme_ns_cmd_write_with_md | spdk_nvme_ns_cmd_write | spdk_nvme_ns_cmd_writev | spdk_nvme_ns_cmd_writev_ext | spdk_nvme_ns_cmd_writev_with_md
        _nvme_ns_cmd_rw SPDK_NVME_OPC_WRITE -> 对命名空间、IO队列对和LBA等参数的封装。这里面比较重要的是增加了一个操作码（OP Code）SPDK_NVME_OPC_WRITE
            nvme_allocate_request
                req = STAILQ_FIRST(&qpair->free_req)
                qpair->num_outstanding_reqs++
                NVME_INIT_REQUEST(req, cb_fn, cb_arg, *payload, payload_size, md_size)
            _nvme_ns_cmd_split_request_sgl
                _nvme_add_child_request
                    nvme_request_add_child(parent, child)
                        TAILQ_INSERT_TAIL(&parent->children, child, child_tailq)
                        child->cb_fn = nvme_cb_complete_child
                            nvme_request_remove_child
                            nvme_complete_request -> cb_fn(cb_arg, cpl)
                            nvme_free_request -> STAILQ_INSERT_HEAD(&req->qpair->free_req, req, stailq)
            or _nvme_ns_cmd_split_request_prp
            _nvme_ns_cmd_setup_request
                *(uint64_t *)&cmd->cdw10 = lba
                cmd->fuse = (io_flags & SPDK_NVME_IO_FLAGS_FUSE_MASK)
                ...
                cmd->cdw15 = (cmd->cdw15 << 16 | apptag)
        nvme_qpair_submit_request(qpair, req) -> _nvme_qpair_submit_request(qpair, req)
            nvme_qpair_check_enabled(qpair)
            nvme_transport_qpair_submit_request(qpair, req)
                qpair_submit_request -> nvme_rdma_qpair_submit_request
                    nvme_rdma_qpair
                    nvme_rdma_req_get
                    spdk_rdma_qp_queue_send_wrs
                    ibv_wr_set_sge_list
            spdk_nvme_print_command(qpair->id, &req->cmd)
                nvme_admin_qpair_print_command
                or nvme_io_qpair_print_command
        or nvme_ns_map_failure_rc

参考io流程: https://zhuanlan.zhihu.com/p/587842815
nvme_transport_qpair_submit_request
   spdk_nvme_transport->ops.qpair_submit_request
   nvme_rdma_qpair_submit_request
       nvme_rdma_req_init
           nvme_rdma_build_sgl_request
       nvme_rdma_qpair_queue_send_wr
           spdk_rdma_qp_queue_send_wrs
               ibv_wr_send
               ibv_wr_send_inv
               ibv_wr_rdma_read
               ibv_wr_rdma_write
               ibv_wr_set_sge_list
           nvme_rdma_qpair_submit_sends
               spdk_rdma_qp_flush_send_wrs
                   ibv_wr_complete
                   or ibv_post_send(spdk_rdma_qp->qp, spdk_rdma_qp->send_wrs.first, bad_wr)
                   spdk_rdma_qp->stats->send.doorbell_updates++



rte_eal_init, 在启动DPDK进程时会传入启动参数，这些参数会分成两部分，一部分是DPDK的eal配置参数，如占用的CPU处理核的列表，分配的大页内存大小，网卡的黑白名单等；另外一部分是进程自定义参数，由开发者自行定义参数列表和含义。参数列表中eal配置参数放在前面，进程自定义参数在后面，中间通过“–”进行分割, https://pxiaoer.blog/2022/04/11/04-dpu%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91sdk-dpdk%E4%B8%89/, https://zhuanlan.zhihu.com/p/610197943, dpu: https://pxiaoer.blog/category/machinelearning/dpu/, 
  rte_cpu_is_supported -> RTE_COMPILE_TIME_CPUFLAGS -> cpu功能表 -> const struct feature_entry rte_cpu_feature_table[]
  if (!__atomic_compare_exchange_n(&run_once, &has_run, 1, 0, -> , bool __atomic_compare_exchange_n(type *ptr, type *expected, type desired, bool weak, int success_memorder, int failure_memorder), 此函数的意义类似于三目表达式, 比较ptr、expected指向内容，若相同则将desired中的值写到ptr, 否则将ptr中的值写入expected, 返回ptr的值
  eal_reset_internal_config(internal_conf)
  eal_log_level_parse(argc, argv) -> 尽早设置日志级别
  eal_save_args(argc, argv) -> 将DPDK进程的启动参数保留下来, 在启动DPDK进程时会传入启动参数，这些参数会分成两部分，一部分是DPDK的eal配置参数，如占用的CPU处理核的列表，分配的大页内存大小，网卡的黑白名单等；另外一部分是进程自定义参数，由开发者自行定义参数列表和含义。参数列表中eal配置参数放在前面，进程自定义参数在后面，中间通过“–”进行分割,分配两段内存空间并交给两个全局的变量eal_args和eal_app_args，后将两部分参数分别存放在这两段内存空间当中
  rte_eal_cpu_init -> CPU处理核信息初始化, lscpu, 该例子的主机中，有一个socket。socket可以对应主板上的一个CPU插槽，就是一块CPU芯片。对于一般的PC机来说，只有一个socket，大型服务器一般有多个socket。一个socket中一般会有多个core，即一个独立的处理单元，在单线程模式下，一个独立的处理单元包含一套标准的寄存器和L1级缓存，同时刻只能运行一个线程；而多线程模式下，一套处理单元有两套寄存器，同时刻可以运行两个线程，这两个线程共享L1级缓存。上例中所显示的主机的CPU有8个处理单元，因为开启了多线程，每个处理单元线程数是2，这样逻辑上讲一共有16个线程(或CPU), 而DPDK的处理核lcore则对应的是线程(或CPU)的编号
  eal_parse_args -> DPDK参数解析
  eal_plugins_init -> 如果是Windows系统，是不支持插件动态载入的，eal_plugins_init()的内容为空。如果是Linux系统时，如果能通过执行系统调用dlopen打开动态链接库librte_eal.so.XX，则调用eal_plugin_add将默认动态库的目录default_solib_dir添加到全局变量solib_list当中(此外还可以通过DPDK的启动参数-d，将自定义目录添加到solib_list中), 由于在初始化阶段加入solib_list中的可能是动态库本身也可能是动态库所在的目录，eal_plugins_init()会在遍历solib_list中将目录中的*.so文件全部遍历之后加入solib_list中；如果已经是*.so本身，则设置solib的lib_handle
  eal_trace_init -> 初始化跟踪功能, 调用eal_trace_init()初始化跟踪功能，该功能是对一些事件和变量进行跟踪，用于多线程同步和时间度量等。该功能比较复杂，后续分析过程中会详细介绍，此处先略过
  eal_option_device_parse -> 解析设备, 前面提到DPDK的参数时，-a和-b参数分别指定网卡设备的白黑名单，这些信息是存放在devopt_list这个全局变量当中的，eal_option_device_parse()会遍历其中的每个设备，并转换成新的数据结构存放在devargs_list当中
  rte_config_init -> 初始化配置, rte_config_init()中，会根据process_type进行不同的初始化任务。process_type是由eal的启动参数指定的，目前支持两种模式：primary和secondary。process_type在解析参数时存放在internal_config中，然后此处赋值到rte_config中
  rte_eal_intr_init -> 中断初始化
  rte_mp_channel_init -> 初始化进程间通信文件
  eal_mp_dev_hotplug_init -> 设备热插拔初始化
  rte_bus_scan -> scan()方法的主要作用是用来遍历所有的设备，并将设备与注册在该总线上的驱动进行匹配，当匹配成功时，建立驱动和设备之间的对应关系。DPDK只提供了接口要求各总线实现该接口，具体如何实现则由各总线设备自行定义
  rte_eal_using_phys_addrs -> 检查是否允许直接物理地址访问
  rte_eal_iova_mode
  ...
  RTE_LCORE_FOREACH_WORKER(i) 
  eal_worker_thread_create: https://blog.csdn.net/xxt99/article/details/125760084
    pthread_create((pthread_t *)&lcore_config[lcore_id].thread_id.opaque_id eal_worker_thread_loop -> eal_thread_loop, dpdk多线程: https://blog.csdn.net/xxt99/article/details/125760084
      ret = f(fct_arg) -> 执行回调, 函数f为空的话就让出cpu，直到f不为空为止
  rte_thread_set_affinity_by_id -> 设置亲和性
  rte_eal_mp_remote_launch(sync_func, NULL, SKIP_MAIN) -> 在所有 lcore 上启动函数。 检查每个 WORKER lcore 是否处于 WAIT 状态，然后为每个 lcore 调用 rte_eal_remote_launch()
  ...
  reactor 线程模型: https://mp.weixin.qq.com/s?__biz=MzI3NDA4ODY4MA==&mid=2653336301&idx=1&sn=893bceb88d7e775863ebc006e83f5532&chksm=f0cb456ac7bccc7cb720366f69941164e4e80cd5ceaef33bcbe16e7c761046aadc6e7137e4d7&scene=21#wechat_redirect









A user of the NVMe-oF target library begins by creating a target using spdk_nvmf_tgt_create(), setting up a set of addresses on which to accept connections by calling spdk_nvmf_tgt_listen_ext(), then creating a subsystem using spdk_nvmf_subsystem_create().
NVMe-oF 目标库的用户首先使用 spdk_nvmf_tgt_create() 创建目标，通过调用 spdk_nvmf_tgt_listen_ext() 设置一组接受连接的地址，然后使用 spdk_nvmf_subsystem_create() 创建子系统。

Subsystems begin in an inactive state and must be activated by calling spdk_nvmf_subsystem_start(). Subsystems may be modified at run time, but only when in the paused or inactive state. A running subsystem may be paused by calling spdk_nvmf_subsystem_pause() and resumed by calling spdk_nvmf_subsystem_resume().
子系统以非活动状态开始，必须通过调用 spdk_nvmf_subsystem_start() 激活。子系统可以在运行时修改，但仅限于处于暂停或非活动状态时。正在运行的子系统可以通过调用 spdk_nvmf_subsystem_pause() 暂停并通过调用 spdk_nvmf_subsystem_resume() 恢复。





nvmf_check_subsystem_active
  ns_info = &sgroup->ns_info[nsid - 1]



#0  0x00007ffff7309dd0 in rdma_listen () from /lib64/librdmacm.so.1
#1  0x000000000050da5a in nvmf_rdma_listen (transport=0xd9a440, trid=0xda7370, listen_opts=0x7fffffffd830) at rdma.c:2924
#2  0x00000000004f424b in spdk_nvmf_transport_listen (transport=0xd9a440, trid=0xda7078, opts=0x7fffffffd830) at transport.c:410
#3  0x00000000004e83b8 in spdk_nvmf_tgt_listen_ext (tgt=0xd82570, trid=0xda7078, opts=0xda72b1) at nvmf.c:683
#4  0x00000000004eded4 in nvmf_rpc_listen_paused (subsystem=0xda9450, cb_arg=0xda7010, status=0) at nvmf_rpc.c:756
#5  0x00000000004dee65 in subsystem_state_change_done (i=0xdab280, status=0) at subsystem.c:625
#6  0x00000000005a0bad in _call_completion (ctx=0xdab280) at thread.c:2508
#7  0x000000000059ce96 in msg_queue_run_batch (thread=0xd296d0, max_msgs=8) at thread.c:841
#8  0x000000000059d7e9 in thread_poll (thread=0xd296d0, max_msgs=0, now=4341767202243019) at thread.c:1063
#9  0x000000000059dae9 in spdk_thread_poll (thread=0xd296d0, max_msgs=0, now=4341767202243019) at thread.c:1156
#10 0x000000000055f86c in _reactor_run (reactor=0xd29140) at reactor.c:914
#11 0x000000000055f95b in reactor_run (arg=0xd29140) at reactor.c:952
#12 0x000000000055fdd9 in spdk_reactors_start () at reactor.c:1068
#13 0x000000000055c298 in spdk_app_start (opts_user=0x7fffffffde50, start_fn=0x407c15 <nvmf_tgt_started>, arg1=0x0) at app.c:808
#14 0x0000000000407d1d in main (argc=5, argv=0x7fffffffe018) at nvmf_main.c:47
rpc_nvmf_subsystem_add_listener
  nvmf_rpc_listen_paused
    spdk_nvmf_tgt_listen_ext
      spdk_nvmf_transport_listen
        rc = transport->ops->listen(transport, &listener->trid, opts) -> rdma_listen

rpc_listen_address_to_trid -> 设置传输id
  SPDK_NVMF_ADRFAM_IPV4
  address->traddr -> 172.17.29.63, address: {transport = 0xda7310 "rdma", adrfam = 0x0, traddr = 0xda7330 "172.17.29.63", trsvcid = 0xda7350 "4520"}
spdk_nvmf_listen_opts_init
spdk_nvmf_subsystem_pause(subsystem, 0, nvmf_rpc_listen_paused, ctx)



执行rpc:
#0  rpc_nvmf_subsystem_add_listener (request=0xd16cb0, params=0xdabd70) at nvmf_rpc.c:853
#1  0x00000000005a650a in jsonrpc_handler (request=0xd16cb0, method=0xdabd30, params=0xdabd70) at rpc.c:124
#2  0x00000000005a8626 in jsonrpc_server_handle_request (request=0xd16cb0, method=0xdabd30, params=0xdabd70) at jsonrpc_server_tcp.c:222
#3  0x00000000005a70b6 in parse_single_request (request=0xd16cb0, values=0xdabcf0) at jsonrpc_server.c:75
#4  0x00000000005a769d in jsonrpc_parse_request (conn=0x7ffff3e62040, json=0x7ffff3e62058, size=248) at jsonrpc_server.c:205
#5  0x00000000005a8846 in jsonrpc_server_conn_recv (conn=0x7ffff3e62040) at jsonrpc_server_tcp.c:284
#6  0x00000000005a8c18 in spdk_jsonrpc_server_poll (server=0x7ffff3e62010) at jsonrpc_server_tcp.c:402
#7  0x00000000005a6803 in spdk_rpc_accept () at rpc.c:213
#8  0x0000000000596816 in rpc_subsystem_poll (arg=0x0) at rpc.c:21
#9  0x000000000059d5bf in thread_execute_timed_poller (thread=0xd296d0, poller=0xd90fd0, now=4342423016780193) at thread.c:1007
#10 0x000000000059d8d6 in thread_poll (thread=0xd296d0, max_msgs=0, now=4342423016780193) at thread.c:1097
#11 0x000000000059dae9 in spdk_thread_poll (thread=0xd296d0, max_msgs=0, now=4342423016780193) at thread.c:1156
#12 0x000000000055f86c in _reactor_run (reactor=0xd29140) at reactor.c:914
#13 0x000000000055f95b in reactor_run (arg=0xd29140) at reactor.c:952
#14 0x000000000055fdd9 in spdk_reactors_start () at reactor.c:1068
#15 0x000000000055c298 in spdk_app_start (opts_user=0x7fffffffde50, start_fn=0x407c15 <nvmf_tgt_started>, arg1=0x0) at app.c:808
#16 0x0000000000407d1d in main (argc=5, argv=0x7fffffffe018) at nvmf_main.c:47




io_unit_size

通知机制, 




nvme_rdma_create_rsps
  rsp->rdma_wr.type = RDMA_WR_TYPE_RECV







host:
nvme discover -t rdma -a 172.17.29.63 -s 4420
tgt:
#0  ibv_post_recv (qp=0xd91178, wr=0x2000070d3000, bad_wr=0x7fffffffd730) at /usr/include/infiniband/verbs.h:3336
#1  0x000000000055957b in spdk_rdma_qp_flush_recv_wrs (spdk_rdma_qp=0xda7260, bad_wr=0x7fffffffd730) at common.c:361
#2  0x00000000005075e9 in nvmf_rdma_resources_create (opts=0x7fffffffd7b0) at rdma.c:833
#3  0x0000000000508197 in nvmf_rdma_qpair_initialize (qpair=0xda9010) at rdma.c:1059
#4  0x0000000000510c0d in nvmf_rdma_poll_group_add (group=0xda4da0, qpair=0xda9010) at rdma.c:4192
#5  0x00000000004f4c50 in nvmf_transport_poll_group_add (group=0xda4da0, qpair=0xda9010) at transport.c:692
#6  0x00000000004e8fe3 in spdk_nvmf_poll_group_add (group=0xd29060, qpair=0xda9010) at nvmf.c:1066
#7  0x00000000004e8cc5 in _nvmf_poll_group_add (_ctx=0xda9fe0) at nvmf.c:982
#8  0x000000000059ce08 in msg_queue_run_batch (thread=0xd82a90, max_msgs=8) at thread.c:841
#9  0x000000000059d75b in thread_poll (thread=0xd82a90, max_msgs=0, now=3600577424951201) at thread.c:1063
#10 0x000000000059da5b in spdk_thread_poll (thread=0xd82a90, max_msgs=0, now=3600577424951201) at thread.c:1156
#11 0x000000000055f7de in _reactor_run (reactor=0xd29140) at reactor.c:914
#12 0x000000000055f8cd in reactor_run (arg=0xd29140) at reactor.c:952
#13 0x000000000055fd4b in spdk_reactors_start () at reactor.c:1068
#14 0x000000000055c20a in spdk_app_start (opts_user=0x7fffffffde70, start_fn=0x407c15 <nvmf_tgt_started>, arg1=0x0) at app.c:808
#15 0x0000000000407d1d in main (argc=3, argv=0x7fffffffe038) at nvmf_main.c:47



nvme_rdma_poller_create
spdk_rdma_srq_flush_recv_wrs
  ibv_post_srq_recv


这里我们如果要查看读取的数据的话，那还需要使用date-file参数。比如这里我们想从nand写10个block的数据（这里设定一个block的大小为4K），并且将读取数据进行观察的话，那么使用如下命令即可：
nvme write /dev/nvme0n1 -s 0 -c 10 -z 40980 -d examples.desktop
dd if=/dev/urandom of=4k bs=4k count=1
rm -rf 4k; for i in {0..4095};do printf a >> 4k;done; stat 4k
gdb --args ./nvme write /dev/nvme2n1 --start-block=0 --block-count=1 --data-size=4k --data=./4k -v
tgt_log:
[2023-07-28 11:22:13.614276] rdma.c:2069:nvmf_rdma_request_process: *DEBUG*: Request 0x2000070d4560 entering state RDMA_REQUEST_STATE_FREE
[2023-07-28 11:22:14.341645] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 9 involuntary context switches in the last second.
[2023-07-28 11:22:15.343998] reactor.c: 620:get_rusage: *INFO*: Reactor 0: 0 voluntary context switches and 7 involuntary context switches in the last second.
[2023-07-28 11:22:16.013980] rdma.c:2069:nvmf_rdma_request_process: *DEBUG*: Request 0x200059cd3000 entering state RDMA_REQUEST_STATE_NEW  # 1 状态机
[2023-07-28 11:22:16.013987] rdma.c:2069:nvmf_rdma_request_process: *DEBUG*: Request 0x200059cd3000 entering state RDMA_REQUEST_STATE_NEED_BUFFER  # 2
[2023-07-28 11:22:16.013992] rdma.c:1881:nvmf_rdma_request_parse_sgl: *DEBUG*: In-capsule data: offset 0x0, length 0x400 # 1024B
[2023-07-28 11:22:16.013996] rdma.c:2069:nvmf_rdma_request_process: *DEBUG*: Request 0x200059cd3000 entering state RDMA_REQUEST_STATE_READY_TO_EXECUTE # 3
[2023-07-28 11:22:16.014005] nvme_qpair.c:247:nvme_io_qpair_print_command: *NOTICE*: WRITE sqid:28 cid:33 nsid:1 lba:0 len:2 SGL DATA BLOCK OFFSET 0x0 len:0x400
[2023-07-28 11:22:16.014022] bdev_malloc.c: 308:bdev_malloc_writev: *DEBUG*: wrote 1024 bytes to offset 0, iovcnt=1
[2023-07-28 11:22:16.014031] accel.c: 179:accel_sequence_set_state: *DEBUG*: seq=0xe9e0a0, setting state: init -> await-virtbuf
[2023-07-28 11:22:16.014036] accel.c: 179:accel_sequence_set_state: *DEBUG*: seq=0xe9e0a0, setting state: await-virtbuf -> check-bouncebuf
[2023-07-28 11:22:16.014041] accel.c: 179:accel_sequence_set_state: *DEBUG*: seq=0xe9e0a0, setting state: check-bouncebuf -> await-bouncebuf
[2023-07-28 11:22:16.014046] accel.c: 179:accel_sequence_set_state: *DEBUG*: seq=0xe9e0a0, setting state: await-bouncebuf -> exec-task
[2023-07-28 11:22:16.014051] accel.c:1639:accel_process_sequence: *DEBUG*: Executing copy operation, sequence: 0xe9e0a0
[2023-07-28 11:22:16.014055] accel.c: 179:accel_sequence_set_state: *DEBUG*: seq=0xe9e0a0, setting state: exec-task -> await-task
[2023-07-28 11:22:16.014061] rdma.c:2069:nvmf_rdma_request_process: *DEBUG*: Request 0x200059cd3000 entering state RDMA_REQUEST_STATE_EXECUTING  # 4
[2023-07-28 11:22:16.014067] accel.c: 179:accel_sequence_set_state: *DEBUG*: seq=0xe9e0a0, setting state: await-task -> complete-task
[2023-07-28 11:22:16.014072] accel.c: 179:accel_sequence_set_state: *DEBUG*: seq=0xe9e0a0, setting state: complete-task -> next-task
[2023-07-28 11:22:16.014076] accel.c:1239:accel_sequence_complete: *DEBUG*: Completed sequence: 0xe9e0a0 with status: 0
[2023-07-28 11:22:16.014083] nvme_qpair.c: 477:spdk_nvme_print_completion: *NOTICE*: SUCCESS (00/00) qid:28 cid:33 cdw0:0 sqhd:0000 p:0 m:0 dnr:0
[2023-07-28 11:22:16.014090] rdma.c:2069:nvmf_rdma_request_process: *DEBUG*: Request 0x200059cd3000 entering state RDMA_REQUEST_STATE_EXECUTED # 5
[2023-07-28 11:22:16.014095] rdma.c:2069:nvmf_rdma_request_process: *DEBUG*: Request 0x200059cd3000 entering state RDMA_REQUEST_STATE_READY_TO_COMPLETE #6
[2023-07-28 11:22:16.014100] rdma.c:2069:nvmf_rdma_request_process: *DEBUG*: Request 0x200059cd3000 entering state RDMA_REQUEST_STATE_COMPLETING #7
[2023-07-28 11:22:16.014116] rdma.c:2069:nvmf_rdma_request_process: *DEBUG*: Request 0x200059cd3000 entering state RDMA_REQUEST_STATE_COMPLETED #8
[2023-07-28 11:22:16.014121] rdma.c:2069:nvmf_rdma_request_process: *DEBUG*: Request 0x200059cd3000 entering state RDMA_REQUEST_STATE_FREE #9
[2023-07-28 11:22:16.174240] rdma.c:2069:nvmf_rdma_request_process: *DEBUG*: Request 0x2000070d4560 entering state RDMA_REQUEST_STATE_NEW
[2023-07-28 11:22:16.174246] rdma.c:2069:nvmf_rdma_request_process: *DEBUG*: Request 0x2000070d4560 entering state RDMA_REQUEST_STATE_READY_TO_EXECUTE


nvmf_rdma_request_process
...
do -> loop
switch (rdma_req->state)
case RDMA_REQUEST_STATE_NEW -> 开始状态机, nvmf：将RDMA请求处理变成状态机，围绕请求处理形式化状态机。 通过调用 spdk_nvmf_rdma_request_process() 来推进状态。 这大大澄清了实现并清理了一些极端情况。 不幸的是，差异也很大，而且似乎没有办法减少它
  rdma_recv = rdma_req->recv
  rdma_req->req.cmd = (union nvmf_h2c_msg *)rdma_recv->sgl[0].addr
  nvmf_rdma_check_fused_ordering(rtransport, rqpair, rdma_req) -> nvmf/rdma：向 tgt 层连续发出融合命令，RDMA 读取可能会导致命令以与从主机接收到的顺序不同的顺序提交到目标层。 通常这很好，但不适用于融合命令。 因此，在融合命令到达 nvmf_rdma_request_process() 时对其进行跟踪。 如果我们发现一对没有有效 FUSED 设置的顺序命令（即 NONE/SECOND、FIRST/NONE、FIRST/FIRST），我们会将请求标记为“fused_failed”，稍后会在它们正常运行之前使它们失败 发送到目标层。 当我们确实找到一对有效的融合命令（FIRST 后跟 SECOND）时，我们将等到两者都 READY_TO_EXECUTE，然后将它们连续提交到目标层
  spdk_nvmf_req_get_xfer
    if (cmd->opc == SPDK_NVME_OPC_FABRIC) -> 127
      spdk_nvme_opc_get_data_transfer -> SPDK_NVME_DATA_BIDIRECTIONAL 3
    SPDK_NVME_SGL_TYPE_DATA_BLOCK
  rdma_req->state = RDMA_REQUEST_STATE_NEED_BUFFER
  STAILQ_INSERT_TAIL(&rgroup->group.pending_buf_queue, &rdma_req->req, buf_link)
...
case RDMA_REQUEST_STATE_NEED_BUFFER
  nvmf_rdma_request_parse_sgl(rtransport, device, rdma_req)
    In-capsule data
    req->iov[0].iov_base = rdma_req->recv->buf + offset
  rdma_req->state = RDMA_REQUEST_STATE_READY_TO_EXECUTE
...
case RDMA_REQUEST_STATE_READY_TO_EXECUTE
  rdma_req->state = RDMA_REQUEST_STATE_EXECUTING
  spdk_nvmf_request_exec(&rdma_req->req) -> 执行请求, 支持zcopy零拷贝
    nvmf_check_subsystem_active(req)
    spdk_nvme_print_command -> SPDK_NVME_OPC_WRITE -> nvme_io_qpair_print_command -> IO写命令
      case SPDK_NVME_OPC_WRITE -> WRITE sqid:28 cid:33 nsid:1 lba:0 len:2 SGL DATA BLOCK OFFSET 0x0 len:0x400
    TAILQ_INSERT_TAIL(&qpair->outstanding, req, link)
    status = nvmf_ctrlr_process_io_cmd(req)
      ns = _nvmf_subsystem_get_ns(ctrlr->subsys, nsid) -> nvmf：不仅为每个子系统侦听器创建 ANA 状态，还为每个 ANA 组创建 ANA 状态 将 ANA 状态从每个子系统侦听器细化为每个 ANA 组的每个子系统侦听器。 将每个 ANA 组的 ANA 状态数组添加到子系统侦听器。 该数组由 ANA 组 ID - 1 索引。然后在 I/O 路径中，我们通过 ctrlr->listener->ana_state[ns->anagrpid - 1] 获取 ANA 状态。 当 FFFFFFFFh 被指定为获取功能和设置功能命令的 NSID 时，NVMe 规范指示存在 NVM 子系统特定的 ANA 状态。 对于这些，我们返回优化状态。 更新 nvmf_subsystem_get_listeners RPC 以返回基础 ANA 组的所有 ANA 状态。 nvmf_subsystem_get_listeners RPC尚未成熟，尚未在测试代码中使用。 因此兼容性并不是最重要的
      ana_state = nvmf_ctrlr_get_ana_state(ctrlr, ns->anagrpid)
      SPDK_DTRACE_PROBE3_TICKS nvmf_request_io_exec_path -> trace
      nvmf_ns_reservation_request_check(ns_info, ctrlr, req) -> 检查当前控制器（主机）是否允许使用 NVMe 命令, 保留命令?
      bdev = ns->bdev
      desc = ns->desc
      ch = ns_info->channel
      if (spdk_nvmf_request_using_zcopy(req)) -> 零拷贝?
      nvmf_bdev_ctrlr_write_cmd(bdev, desc, ch, req)
        nvmf_bdev_ctrlr_get_rw_params
        spdk_bdev_writev_blocks(desc, ch, req->iov, req->iovcnt, start_lba, num_blocks, nvmf_bdev_ctrlr_complete_cmd, req) -> 向给定通道上的 bdev 提交写入请求。 这与 spdk_bdev_write 的不同之处在于允许在分散收集列表中描述数据缓冲区。 某些物理设备对数据提出内存对齐要求，并且可能无法直接从提供的缓冲区中传输出去。 在这种情况下，请求可能会失败
          bdev_writev_blocks_with_md
            bdev_io_valid_blocks
            bdev_io = bdev_channel_get_io(channel)
                bdev_io = spdk_mempool_get(g_bdev_mgr.bdev_io_pool) -> rte_mempool_get((struct rte_mempool *)mp, &ele)
            bdev_io->type = SPDK_BDEV_IO_TYPE_WRITE
            bdev_io->u.bdev.iovs = iov
            bdev_io->u.bdev.iovcnt = iovcnt
            bdev_io_init(bdev_io, bdev, cb_arg, cb)
                bdev_io->internal.status = SPDK_BDEV_IO_STATUS_PENDING
                bdev_io->internal.split = bdev_io_should_split(bdev_io)
                    bdev_rw_should_split(bdev_io)
                        bdev_io->u.bdev.iovcnt > max_segs -> split
            bdev_io->u.bdev.accel_sequence = seq;
            _bdev_io_submit_ext(desc, bdev_io) -> bdev：从 spdk_bdev_io 中删除 spdk_bdev_ext_io_opts spdk_bdev_ext_io_opts 结构用于在提交 bdev IO 请求时传递额外选项，而无需修改/添加函数来处理新选项。 此外，该结构还有一个大小字段，允许在不破坏 ABI 的情况下添加新字段（因此必须提高库的主要版本）。 它也是 spdk_bdev_io 的一部分，将其从该结构中删除有几个原因： 1. 大小字段仅在通过指针传递的结构中才有意义。 spdk_bdev_ext_io_opts 确实作为指向 spdk_bdev_{readv,writev}_blocks_ext() 的指针传递，但它也嵌入在 spdk_bdev_io (internal.ext_opts_copy) 中，这也是 API 的一部分。 这意味着每次向 spdk_bdev_ext_io_opts 添加新字段时，spdk_bdev_io 的大小也会发生变化，因此我们无论如何都需要更改 libspdk_bdev 的主要版本，从而使 spdk_bdev_ext_io_opts.size 无用。 2. size字段也使得internal.ext_opts使用起来很麻烦，因为每次访问它的一个字段时，我们都需要检查大小。 目前代码并没有这样做，因为最初引入此结构时所有现有的 spdk_bdev_ext_io_opts 字段都存在，但我们需要在访问任何新字段之前检查大小。 3. spdk_bdev_ext_io_opts有一个metadata字段，而spdk_bdev_io已经有u.bdev.md_buf，这意味着我们在spdk_bdev_io中的几个不同的地方存储相同的东西（u.bdev.md_buf，u.bdev.ext_opts->metadata，internal。 ext_opts->元数据）。 因此，此补丁从 spdk_bdev_io 中删除对 spdk_bdev_ext_io_opts 的所有引用，并将其替换为 spdk_bdev_io 中缺少的字段（memory_domain、memory_domain_ctx）。 不幸的是，此更改破坏了 API，并需要更改支持 spdk_bdev_io.u.bdev.ext_opts 的 bdev 模块
              bdev_io_needs_sequence_exec -> false, bdev：写请求的加速序列支持现在可以提交包含一系列加速操作的写入请求，这些操作需要在实际写入数据之前执行。 如果 bdev 支持加速序列并且不需要拆分请求，则此类请求将直接传递到 bdev 模块（以便它可以将后续操作附加到加速序列）。 如果这些条件中的任何一个不满足，bdev 层将在将请求传递给 bdev 模块之前执行所有累积的加速操作。 不提交带有加速序列的拆分 IO 的原因是我们也需要拆分该加速序列。 目前，accel 中没有这样的功能，因此我们以与底层 bdev 模块不支持 accel 序列相同的方式处理这种情况（它在 bdev_io 拆分之前执行）
              bdev_io_submit(bdev_io)
                spdk_bdev_io_get_thread(bdev_io)
                TAILQ_INSERT_TAIL(&ch->io_submitted, bdev_io, internal.ch_link)
                _bdev_io_submit(bdev_io)
                  bdev_io_do_submit(bdev_ch, bdev_io)
                    spdk_likely(TAILQ_EMPTY(&shared_resource->nomem_io))
                    bdev_submit_request(bdev, ch, bdev_io) -> bdev->fn_table->submit_request(ioch, bdev_io) -> bdev_malloc_submit_request(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io)
                      _bdev_malloc_submit_request
                        case SPDK_BDEV_IO_TYPE_WRITE -> write
                        bdev_malloc_writev
                          bdev_malloc_check_iov_len
                          spdk_accel_append_copy -> 将复制操作附加到序列。 序列中的复制操作很特殊，因为它不能保证数据将被实际复制。 如果可能的话，它只会更改序列中某些操作的源/目标缓冲区
                            seq = accel_sequence_get(accel_ch)
                            accel_sequence_get_task
                            task->op_code = ACCEL_OPC_COPY
                            TAILQ_INSERT_TAIL(&seq->tasks, task, seq_link)
                            ...
                          spdk_accel_sequence_finish(bdev_io->u.bdev.accel_sequence, malloc_sequence_done, task)
                            accel_process_sequence(seq)
                              accel_sequence_set_state(seq, ACCEL_SEQUENCE_STATE_AWAIT_VIRTBUF)
                              accel_sequence_set_state(seq, ACCEL_SEQUENCE_STATE_CHECK_BOUNCEBUF)
                              accel_sequence_set_state(seq, ACCEL_SEQUENCE_STATE_EXEC_TASK)
                              ...
                              rc = accel_submit_task(accel_ch, task) -> module->submit_tasks(module_ch, task) -> sw_accel_submit_tasks
                                case ACCEL_OPC_COPY
                                _sw_accel_copy_iovs
                                  for (len = spdk_ioviter_first(&iter, src_iovs, src_iovcnt
                                  memcpy(dst, src, len) -> 最终执行了一次内存拷贝
                                _add_to_comp_list(sw_ch, accel_task, rc)
                          spdk_accel_submit_copy malloc_done
        return SPDK_NVMF_REQUEST_EXEC_STATUS_ASYNCHRONOUS -> status
    if (status == SPDK_NVMF_REQUEST_EXEC_STATUS_COMPLETE)
      _nvmf_request_complete(req)




app启动创建了两个线程, 一个控制线程, 一个遥测线程
#0  0x0000000000406980 in pthread_create@plt () -> rte_ctrl_thread_create(&intr_thread, "eal-intr-thread", NULL -> 创建控制线程。 创建具有给定名称和属性的控制线程。 新线程的亲和力基于调用 rte_eal_init() 时检索到的 CPU 亲和力，然后排除数据平面和服务 lcore。 如果设置线程名称失败，则会忽略该错误并记录一条调试消息
#1  0x00000000005f8623 in rte_ctrl_thread_create (thread=0xb5a940 <intr_thread>, name=0x8448d6 "eal-intr-thread", attr=0x0, start_routine=0x61730c <eal_intr_thread_main>, arg=0x0) at ../lib/eal/common/eal_common_thread.c:310
#2  0x000000000061754a in rte_eal_intr_init () at ../lib/eal/linux/eal_interrupts.c:1191
#3  0x0000000000610f46 in rte_eal_init (argc=11, argv=0xd16cb0) at ../lib/eal/linux/eal.c:1038
#4  0x00000000004bde4a in spdk_env_init (opts=0x7fffffffdac0) at init.c:608
#5  0x000000000055ad41 in app_setup_env (opts=0x7fffffffdb90) at app.c:372
#6  0x000000000055bffc in spdk_app_start (opts_user=0x7fffffffde50, start_fn=0x407c15 <nvmf_tgt_started>, arg1=0x0) at app.c:743
#7  0x0000000000407d1d in main (argc=5, argv=0x7fffffffe018) at nvmf_main.c:47

telemetry_v2_init

...
rte_eal_intr_init
  rte_ctrl_thread_create(&intr_thread, "eal-intr-thread", NULL eal_intr_thread_main
    pthread_create(thread, attr, ctrl_thread_start, (void *)params) -> eal_intr_thread_main
    for (;;)
      int pfd = epoll_create(1)
      epoll_ctl
      AILQ_FOREACH(src, &intr_sources, next) -> 中断源列表
      eal_intr_handle_interrupts
        epoll_wait
        eal_intr_process_interrupts
          active_cb.cb_fn(active_cb.cb_arg) -> 执行中断源回调


rte_dev_event_monitor_start
  rte_intr_callback_register(intr_handle, dev_uev_handler, NULL)





TAILQ_INSERT_TAIL(&intr_sources, src, next) 注册中断源

默认配置:
(gdb) p internal_config
$8 = {
  memory = 0, 
  force_nchannel = 0, 
  force_nrank = 0, 
  no_hugetlbfs = 0, 
  hugepage_file = {
    unlink_before_mapping = false, 
    unlink_existing = true
  }, 
  no_pci = 0, 
  no_hpet = 1, 
  vmware_tsc_map = 0, 
  no_shconf = 0, 
  in_memory = 0, 
  create_uio_dev = 0, 
  process_type = RTE_PROC_PRIMARY, 
  force_sockets = 0, 
  socket_mem = {0 <repeats 32 times>}, 
  force_socket_limits = 0, 
  socket_limit = {0 <repeats 32 times>}, 
---Type <return> to continue, or q <return> to quit---
  base_virtaddr = 0, 
  legacy_mem = 0, 
  match_allocations = 0, 
  single_file_segments = 0, 
  syslog_facility = 24, 
  vfio_intr_mode = RTE_INTR_MODE_NONE, 
  vfio_vf_token = '\000' <repeats 15 times>, 
  hugefile_prefix = 0x0, 
  hugepage_dir = 0x0, 
  user_mbuf_pool_ops_name = 0x0, 
  num_hugepage_sizes = 0, 
  hugepage_info = {{
      hugepage_sz = 0, 
      hugedir = '\000' <repeats 4095 times>, 
      num_pages = {0 <repeats 32 times>}, 
      lock_descriptor = -1
    }, {
      hugepage_sz = 0, 
      hugedir = '\000' <repeats 4095 times>, 
      num_pages = {0 <repeats 32 times>}, 
---Type <return> to continue, or q <return> to quit---
      lock_descriptor = -1
    }, {
      hugepage_sz = 0, 
      hugedir = '\000' <repeats 4095 times>, 
      num_pages = {0 <repeats 32 times>}, 
      lock_descriptor = -1
    }}, 
  iova_mode = RTE_IOVA_DC, 
  ctrl_cpuset = {
    __bits = {0 <repeats 16 times>}
  }, 
  init_complete = 0, 
  no_telemetry = 0, 
  max_simd_bitwidth = {
    forced = false, 
    bitwidth = 256
  }, 
  huge_worker_stack_size = 0
}




daos_bdev:
_bdev_daos_submit_request
  case SPDK_BDEV_IO_TYPE_WRITE
    bdev_daos_writev
      daos_event_init
      d_iov_set
      dfs_write



daos iopath:
---------------------------- DAOS ----------------------------
vos_blob_format_cb
  bio_write_blob_hdr
    bio_write
      bio_rw
        bio_rwv
          bio_iod_post 提交io描述
            dma_rw
              nvme_rw
---------------------------- SPDK ----------------------------
                spdk_blob_io_write rw_completion
                  blob_request_submit_op
                    blob_request_submit_op_single
                      bs_batch_write_dev
                        blob_bdev->bs_dev.write
                        bdev_blob_write
                          spdk_bdev_write_blocks -> bdev_io_submit
---------------------------- SPDK ----------------------------                          

qemu check:
qemu-system-x86_64 -device vhost-user-scsi-pci,help
qemu-system-x86_64 -device vhost-user-blk-pci,help

start vhost:
HUGEMEM=4096 scripts/setup.sh
build/bin/vhost -S /var/tmp -m 0x3

scsi
scripts/rpc.py bdev_malloc_create 64 512 -b Malloc0
scripts/rpc.py vhost_create_scsi_controller --cpumask 0x1 vhost.0 -> rpc_vhost_create_scsi_controller
scripts/rpc.py vhost_scsi_controller_add_target vhost.0 0 Malloc0 -> rpc_vhost_scsi_controller_add_target
scripts/rpc.py vhost_scsi_controller_remove_target vhost.0 0

blk
scripts/rpc.py vhost_create_blk_controller --cpumask 0x1 vhost.1 Malloc0
scripts/rpc.py vhost_create_blk_controller --cpumask 0x1 -r vhost.1 Malloc0

qemu:
-object memory-backend-file,id=mem,size=1G,mem-path=/dev/hugepages,share=on -numa node,memdev=mem
-drive file=guest_os_image.qcow2,if=none,id=disk -device ide-hd,drive=disk,bootindex=0
其次，通过为启动映像指定 bootindex=0，确保 QEMU 从虚拟机映像启动，而不是从 SPDK malloc 块设备启动。


scsi
-chardev socket,id=char0,path=/var/tmp/vhost.0 -device vhost-user-scsi-pci,id=scsi0,chardev=char0

vhost_blk
-chardev socket,id=char1,path=/var/tmp/vhost.1 -device vhost-user-blk-pci,id=blk0,chardev=char1

 taskset -c 2,3 qemu-system-x86_64 \
  --enable-kvm \
  -cpu host -smp 2 \
  -m 1G -object memory-backend-file,id=mem0,size=1G,mem-path=/dev/hugepages,share=on -numa node,memdev=mem0 \
  -drive file=guest_os_image.qcow2,if=none,id=disk \
  -device ide-hd,drive=disk,bootindex=0 \
  -chardev socket,id=spdk_vhost_scsi0,path=/var/tmp/vhost.0 \
  -device vhost-user-scsi-pci,id=scsi0,chardev=spdk_vhost_scsi0,num_queues=2 \
  -chardev socket,id=spdk_vhost_blk0,path=/var/tmp/vhost.1 \
  -device vhost-user-blk-pci,chardev=spdk_vhost_blk0,num-queues=2




start vhost_target:
export LD_LIBRARY_PATH=/snap/core22/1122/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH
build/bin/vhost -S /var/tmp -m 0x3 -> app/vhost/vhost.c -> main
    spdk_app_opts_init
    spdk_app_parse_args(argc, argv, &opts, "f:S:", NULL, vhost_parse_arg, vhost_usage)
    opts.name = "vhost"
    spdk_app_start(&opts, vhost_started, NULL)
        vhost_started -> no impl
[2024-07-06 14:42:06.618134] [ DPDK EAL parameters: vhost --no-shconf -c 0x3 --huge-unlink --log-level=lib.eal:6 --log-level=lib.cryptodev:5 --log-level=user1:6 --iova-mode=pa --base-virtaddr=0x200000000000 --match-allocations --file-prefix=spdk_pid106081 ]



vhost_spec, req api, 
enum vhost_user_request {
    VHOST_USER_NONE = 0,
    VHOST_USER_GET_FEATURES = 1,
    VHOST_USER_SET_FEATURES = 2,
    VHOST_USER_SET_OWNER = 3,
    VHOST_USER_RESET_OWNER = 4,
    VHOST_USER_SET_MEM_TABLE = 5,
    VHOST_USER_SET_LOG_BASE = 6,
    VHOST_USER_SET_LOG_FD = 7,
    VHOST_USER_SET_VRING_NUM = 8,
    VHOST_USER_SET_VRING_ADDR = 9,
    VHOST_USER_SET_VRING_BASE = 10,
    VHOST_USER_GET_VRING_BASE = 11,
    VHOST_USER_SET_VRING_KICK = 12,
    VHOST_USER_SET_VRING_CALL = 13,
    VHOST_USER_SET_VRING_ERR = 14,
    VHOST_USER_GET_PROTOCOL_FEATURES = 15,
    VHOST_USER_SET_PROTOCOL_FEATURES = 16,
    VHOST_USER_GET_QUEUE_NUM = 17,
    VHOST_USER_SET_VRING_ENABLE = 18,
    VHOST_USER_SET_STATUS = 39,
    VHOST_USER_GET_STATUS = 40,
};





rpc.py bdev_virtio_attach_controller --dev-type blk --trtype user --traddr /tmp/vhost.0 --vq-count 2 --vq-size 512 VirtioBlk0
or rpc.py bdev_virtio_attach_controller --dev-type blk --trtype pci --traddr 0000:01:00.0 VirtioBlk1
    rpc_bdev_virtio_attach_controller
        if (strcmp(req->dev_type, "blk") == 0)
            pci -> bdev_virtio_pci_blk_dev_create
            user -> bdev_virtio_user_blk_dev_create
                virtio_user_blk_dev_create
                    virtio_user_dev_init
                        dev = calloc(1, sizeof(*dev))
                        virtio_dev_construct(vdev, name, &virtio_user_ops, dev)
                        virtio_user_dev_setup
                            vhost_user_setup(dev)
                                fd = socket(AF_UNIX, SOCK_STREAM, 0)
                                connect(fd, (struct sockaddr *)&un, sizeof(un))
                            case VIRTIO_USER_BACKEND_VHOST_VDPA
                                dev->ops = &virtio_ops_vdpa
                        vhost_user_sock(dev, VHOST_USER_SET_OWNER, NULL)
                            switch (req)
                            case VHOST_USER_SET_MEM_TABLE
                                prepare_vhost_memory_user
                            vhost_user_write(vhostfd, &msg, len, fds, fd_num)
                            vhost_user_read(vhostfd, &msg)
                    virtio_dev_reset
                    virtio_blk_dev_init
            vfio-user -> bdev_virtio_vfio_user_blk_dev_create
                virtio_vfio_user_dev_init(&bvdev->vdev, name, path)
                    virtio_dev_construct(vdev, name, &virtio_vfio_user_ops, dev)
                        vdev->backend_ops = ops
                    spdk_vfio_user_setup
                        vfio_user_dev_setup(device) -> socket -> connect -> return fd
                            vfio_user_check_version
                                vfio_user_dev_send_request(dev, VFIO_USER_VERSION, req.payload,
                        vfio_user_get_dev_info -> VFIO_USER_DEVICE_GET_INFO
                        vfio_device_map_bars_and_config_region
                            vfio_user_get_dev_region_info(device, info, len, fds, VFIO_MAXIMUM_SPARSE_MMAP_REGIONS)
                                VFIO_USER_DEVICE_GET_REGION_INFO
                            vfio_device_setup_sparse_mmaps
                                region->mmaps[j].mem = mmap(NULL, region->mmaps[j].size, prot, MAP_SHARED,
                            vfio_device_map_region
                                region->mmaps[0].mem = mmap(NULL, region->size, prot, MAP_SHARED,
                    spdk_vfio_user_pci_bar_access(dev->ctx, VFIO_PCI_CONFIG_REGION_INDEX, 4, 2,
                        vfio_user_dev_mmio_access
                            vfio_user_dev_send_request(dev, VFIO_USER_REGION_WRITE
                            vfio_user_dev_send_request(dev, VFIO_USER_REGION_READ
                        memcpy(bar_addr, buf, len)
                    dev->pci_cap_region = VFIO_PCI_BAR4_REGION_INDEX
                virtio_dev_reset(&bvdev->vdev, VIRTIO_BLK_DEV_SUPPORTED_FEATURES)
                virtio_dev_has_feature(&bvdev->vdev, VIRTIO_BLK_F_MQ)
                virtio_dev_read_dev_config(&bvdev->vdev, offsetof(struct virtio_blk_config, num_queues),
                virtio_blk_dev_init(bvdev, num_queues)
        if (strcmp(req->dev_type, "scsi") == 0)
            pci -> bdev_virtio_pci_scsi_dev_create
            user -> bdev_virtio_user_scsi_dev_create
            vfio-user -> bdev_vfio_user_scsi_dev_create


static const struct virtio_dev_ops virtio_user_ops = {
	.read_dev_cfg	= virtio_user_read_dev_config,
	.write_dev_cfg	= virtio_user_write_dev_config,
	.get_status	= virtio_user_get_status,
	.set_status	= virtio_user_set_status,
        virtio_user_start_device -> VIRTIO_CONFIG_STATUS_DRIVER_OK
            ret = dev->ops->set_memory_table(dev)
            vhost_user_sock(dev, VHOST_USER_GET_QUEUE_NUM, &host_max_queues) -> forword to backend
            virtio_user_queue_setup(vdev, virtio_user_create_queue)
                vhost_user_sock(dev, VHOST_USER_SET_VRING_CALL, &file)
                file.fd = dev->callfds[queue_sel]
                dev->ops->set_vring_call(dev, &file)
            virtio_user_register_mem
                const struct spdk_mem_map_ops virtio_user_map_ops = {
                    .notify_cb = virtio_user_map_notify,
                        vhost_user_sock(dev, VHOST_USER_SET_MEM_TABLE, NULL)
                    .are_contiguous = NULL
                };
                dev->mem_map = spdk_mem_map_alloc(0, &virtio_user_map_ops, vdev)
            virtio_user_queue_setup(vdev, virtio_user_kick_queue)
                addr.desc_user_addr = (uint64_t)(uintptr_t)vring->desc
                addr.avail_user_addr = (uint64_t)(uintptr_t)vring->avail
                addr.used_user_addr = (uint64_t)(uintptr_t)vring->used
                dev->ops->set_vring_num(dev, &state)
                dev->ops->set_vring_base(dev, &state)
                dev->ops->set_vring_addr(dev, &addr)
                dev->ops->set_vring_kick(dev, &file)
            ret = dev->ops->enable_qp(dev, 0, 1)
	.get_features	= virtio_user_get_features,
	.set_features	= virtio_user_set_features,
	.destruct_dev	= virtio_user_destroy,
	.get_queue_size	= virtio_user_get_queue_size,
	.setup_queue	= virtio_user_setup_queue,
	.del_queue	= virtio_user_del_queue,
	.notify_queue	= virtio_user_notify_queue,
        write(dev->kickfds[vq->vq_queue_index], &buf, sizeof(buf)
	.dump_json_info = virtio_user_dump_json_info,
	.write_json_config = virtio_user_write_json_config,
};


vhost_create_blk_controller --cpumask 0x1 vhost.1 Malloc0  # -> /var/tmp/vhost.1
vhost_create_blk_controller -> rpc_vhost_create_blk_controller -> spdk vhost_blk commit: https://github.com/ssbandjl/spdk/commit/b727e804d61c5094e073c8503694b5d84b2d6adf
    spdk_vhost_blk_construct(req.ctrlr, req.cpumask, req.dev_name, req.transport, params)
        struct spdk_vhost_blk_dev *bvdev = NULL
        const char *transport_name = VIRTIO_BLK_DEFAULT_TRANSPORT -> vhost_user_blk
        bvdev->ops = virtio_blk_get_transport_ops(transport_name)
            TAILQ_FOREACH(ops, &g_spdk_virtio_blk_transport_ops, link) <- SPDK_VIRTIO_BLK_TRANSPORT_REGISTER(vhost_user_blk, &vhost_user_blk)
        spdk_bdev_open_ext(dev_name, true, bdev_event_cb, bvdev, &bvdev->bdev_desc)
        bdev = spdk_bdev_desc_get_bdev(bvdev->bdev_desc)
        vdev->virtio_features = SPDK_VHOST_BLK_FEATURES_BASE
        在启用多队列的情况下启动 qemu 时，vhost 设备将启动/停止多次，与队列数有关，因为当时不知道此设备使用的确切队列数。一旦获得有效的 IO 队列，目标就必须停止并启动设备。在停止和启动 vhost 设备时，后端 bdev io 设备将被重复删除和创建。在 struct spdk_vhost_blk_dev 中保存 bdev 引用，这样 io 设备就不会被删除
        bvdev->dummy_io_channel = spdk_bdev_get_io_channel(bvdev->bdev_desc) -> vhost_blk：加速 vhost 设备启动，当使用 vhost-user-blk multiqueue（将 num-queues 设置为大于 1）启动 qemu 时，vhost 设备将被启动/停止多次（与队列数有关），因为 vhost-user 后端不知道此设备使用的队列的确切数量。一旦获得有效的 IO 队列，目标就必须停止并启动设备。当停止和启动 vhost 设备时，后端 bdev io 设备将被重复删除和创建。如果后端 bdev 是一个分布式系统，则成本与网络 RTT 一样大。在此补丁中，添加一个 dummy_io_channel 来保存对 io 设备的引用，这样 io 设备就不会被删除
        vhost_dev_register(vdev, name, cpumask, params, &vhost_blk_device_backend, &vhost_blk_user_device_backend)
    free_rpc_vhost_blk_ctrlr(&req)


static const struct spdk_vhost_user_dev_backend vhost_blk_user_device_backend = {
    .session_ctx_size = sizeof(struct spdk_vhost_blk_session) - sizeof(struct spdk_vhost_session),
    .start_session =  vhost_blk_start,
    .stop_session = vhost_blk_stop,
    .alloc_vq_tasks = alloc_vq_task_pool,
        vq->tasks = spdk_zmalloc(sizeof(struct spdk_vhost_user_blk_task) * task_cnt, SPDK_CACHE_LINE_SIZE, NULL, SPDK_ENV_LCORE_ID_ANY, SPDK_MALLOC_DMA)
        task = &((struct spdk_vhost_user_blk_task *)vq->tasks)[j]
        task->bvsession = bvsession;
        task->req_idx = j;
        task->vq = vq;
    .register_vq_interrupt = vhost_blk_vq_register_interrupt, -> _vhost_blk_vq_register_interrupt
        vq->intr = spdk_interrupt_register(vq->vring.kickfd, vdev_vq_worker, vq, "vdev_vq_worker") -> interrupt
             process_packed_vq(bvsession, vq) <- if (packed_ring)
                resubmit_cnt = submit_inflight_desc(bvsession, vq) -> lib/vhost：将 submit_inflight_desc() 添加到 CPU 使用率统计中，submit_inflight_desc() 实际上做了一些有意义的工作，因此当它真正处理任务时，轮询器应该返回 BUSY 状态
                    vhost_vq_used_ring_enqueue
                        vhost_vq_used_signal
                            rte_vhost_vring_call(vsession->vid, virtqueue->vring_idx)
                    process_packed_inflight_blk_task(vq, req_idx)
                        blk_task_enqueue(task)
                            if (task->vq->packed.packed_ring)
                                vhost_vq_packed_ring_enqueue
                            else
                                vhost_vq_used_ring_enqueue
                        blk_task_inc_task_cnt(task)
                        blk_task_init(task)
                        blk_iovs_inflight_queue_setup(task->bvsession, vq, task->req_idx, blk_task->iovs, &blk_task->iovcnt, &blk_task->payload_size) -> set desc -> iovs
                            vhost_inflight_queue_get_desc
                                *desc = &desc_array[req_idx]
                            blk_iovs_packed_desc_setup -> set desc
                        vhost_user_process_blk_request(task)
                            virtio_blk_process_request(vdev, bvsession->io_channel, &user_task->blk_task, vhost_user_blk_request_finish, NULL) -> vhost：添加 virtio_blk 抽象，此补丁为自定义传输添加了 virtio_blk 抽象，其中首先使用“vhost_user_blk”。添加了 spdk_virtio_blk_transport_ops，描述了每个传输要实现的必要回调。请使用 SPDK_VIRTIO_BLK_TRANSPORT_REGISTER 注册传输。传输可以使用 virtio_blk_process_request() 来处理来自其队列的传入 I/O。添加了 virtio_blk_create_transport RPC 来创建已注册的传输之一，可能带有自定义 JSON 参数。向 vhost_create_blk_controller RPC 添加了“transport”参数，以指定哪个传输应该创建控制器。默认情况下使用 vhost_user_blk 传输 -> commit: https://github.com/ssbandjl/spdk/commit/b727e804d61c5094e073c8503694b5d84b2d6adf
                                struct virtio_blk_outhdr req
                                    /* VIRTIO_BLK_T* */
                                    __virtio32 type;
                                    /* io priority. */
                                    __virtio32 ioprio;
                                    /* Sector (ie. 512 byte offset) */
                                    __virtio64 sector;
                                task->cb = cb -> vhost_user_blk_request_finish
                                iov = &task->iovs[0]
                                memcpy(&req, iov->iov_base, sizeof(req))
                                switch (type)
                                case VIRTIO_BLK_T_IN:
                                    spdk_bdev_readv(bvdev->bdev_desc, ch,&task->iovs[1], iovcnt, req.sector * 512,payload_len,blk_request_complete_cb, task)
                                case VIRTIO_BLK_T_OUT
                                    spdk_bdev_writev(bvdev->bdev_desc, ch, &task->iovs[1], iovcnt, req.sector * 512, payload_len, blk_request_complete_cb, task)
                                        bdev_bytes_to_blocks(spdk_bdev_desc_get_bdev(desc)
                                        spdk_bdev_writev_blocks(desc, ch, iov, iovcnt, offset_blocks, num_blocks, cb, cb_arg)
                                case VIRTIO_BLK_T_DISCARD
                                case VIRTIO_BLK_T_WRITE_ZEROES
                                case VIRTIO_BLK_T_FLUSH
                                case VIRTIO_BLK_T_GET_ID
                                default
                                    blk_request_finish(VIRTIO_BLK_S_UNSUPP, task)
                    else process_blk_task(vq, req_idx)
                vhost_vq_packed_ring_is_avail
                process_packed_blk_task(vq, vq->last_avail_idx)
             or process_vq(bvsession, vq) -> split ring
             vhost_session_vq_used_signal
                rte_vhost_vring_call
                    vhost_vring_call_packed
                    or vhost_vring_call_split
                        eventfd_write(vq->callfd, (eventfd_t) 1)
        or spdk_interrupt_register(vq->vring.kickfd, no_bdev_vdev_vq_worker, vq, "no_bdev_vdev_vq_worker")
};


blk_request_complete_cb
    spdk_bdev_free_io(bdev_io)
    blk_request_finish(success ? VIRTIO_BLK_S_OK : VIRTIO_BLK_S_IOERR, task)
        task->cb(status, task, task->cb_arg) -> vhost_user_blk_request_finish
            blk_task_enqueue(user_task)
            blk_task_finish(user_task)
                blk_task_dec_task_cnt(task)
                task->used = false

static const struct spdk_vhost_dev_backend vhost_blk_device_backend = {
    .type = VHOST_BACKEND_BLK,
    .vhost_get_config = vhost_blk_get_config,
        blkcfg.size_max = SPDK_BDEV_LARGE_BUF_MAX_SIZE; -> (64 * 1024)
        blkcfg.seg_max = spdk_min(SPDK_VHOST_IOVS_MAX - 2 - 1, SPDK_BDEV_IO_NUM_CHILD_IOV - 2 - 1)
        blkcfg.capacity = (blkcnt * blk_size) / 512
        blkcfg.num_queues = SPDK_VHOST_MAX_VQUEUES -> 256 -> QEMU can overwrite this value when started 
        
    .dump_info_json = vhost_blk_dump_info_json,
    .write_config_json = vhost_blk_write_config_json,
    .remove_device = vhost_blk_destroy,
    .set_coalescing = vhost_blk_set_coalescing,
    .get_coalescing = vhost_blk_get_coalescing,
};




vhost_user.c


rpc_vhost_create_scsi_controller
    spdk_vhost_scsi_dev_construct
        vhost_dev_register(&svdev->vdev, name, cpumask, NULL, &spdk_vhost_scsi_device_backend, &spdk_vhost_scsi_user_device_backend)
            if (spdk_vhost_dev_find(name))
                TAILQ_FOREACH(vdev, &g_vhost_devices, tailq)
            if (vdev->backend->type == VHOST_BACKEND_SCSI)
                vhost_user_dev_register(vdev, name, &cpumask, user_backend)
                    vdev->path = strdup(path)
                    user_dev = calloc(1, sizeof(*user_dev))
                    vhost_user_dev_set_coalescing(user_dev, SPDK_VHOST_COALESCING_DELAY_BASE_US, SPDK_VHOST_VQ_IOPS_COALESCING_THRESHOLD) -> 设置合并/延迟参数
                        uint64_t delay_time_base = delay_base_us * spdk_get_ticks_hz() / 1000000ULL
                        user_dev->coalescing_delay_us = delay_base_us
                        user_dev->coalescing_iops_threshold = iops_threshold
                    vhost_register_unix_socket(path, name, vdev->virtio_features, vdev->disabled_features, vdev->protocol_features)
                        rte_vhost_driver_register(path, RTE_VHOST_USER_ASYNC_COPY) -> vhost：添加 vhost-user 客户端模式，向 rte_vhost_driver_register() 添加新参数（标志）。当设置 RTE_VHOST_USER_CLIENT 标志时，DPDK vhost-user 充当客户端模式。这些标志还将允许未来的扩展而不会再次破坏 API。剩下的就是：分配一个 unix 套接字，绑定/监听服务器，连接客户端。此扩展仅适用于 vhost-user，因此当为 vhost-cuse 提供任何标志时，我们只需退出并报告错误 -> vhost：引入异步入队注册 API，执行大型内存复制通常占用大部分 CPU 周期，并成为 vhost-user 入队操作中的热点。为了将大型复制从 CPU 卸载到 DMA 设备，引入了异步 API，CPU 只需将复制作业提交给 DMA，而无需等待其复制完成。因此，数据传输期间无需 CPU 干预。我们可以节省宝贵的 CPU 周期并提高基于 vhost-user 的应用程序的整体吞吐量。此补丁引入了用于 vhost 异步数据入队操作的注册/取消注册 API。除了注册 API 实现外，还定义了异步入队数据路径所需的数据结构和异步回调函数的原型
                            vsocket = malloc(sizeof(struct vhost_user_socket))
                            1ULL << VIRTIO_NET_F_HOST_TSO4
                            VIRTIO_NET_F_HOST_TSO6
                            VIRTIO_NET_F_HOST_UFO
                            RTE_VHOST_USER_CLIENT
                                vhost_user_reconnect_init
                                    rte_ctrl_thread_create(&reconn_tid, "vhost_reconn", NULL, vhost_user_client_reconnect, NULL)
                                        vhost_user_connect_nonblock
                                            connect(fd, un, sz)
                                        vhost_user_add_connection
                                            vid = vhost_new_device()
                                                dev->flags = VIRTIO_DEV_BUILTIN_VIRTIO_NET
                                            vhost_attach_vdpa_device(vid, vsocket->vdpa_dev)
                                            vsocket->notify_ops->new_connection(vid)
                                            fdset_add(&vhost_user.fdset, fd, vhost_user_read_cb, NULL, conn)
                                            TAILQ_INSERT_TAIL(&vsocket->conn_list, conn, next)
                            create_unix_socket(vsocket)
                            vhost_user.vsockets[vhost_user.vsocket_cnt++] = vsocket
                        rte_vhost_driver_set_features(path, virtio_features)
                        rte_vhost_driver_callback_register(path, &g_spdk_vhost_ops)
                            vsocket->notify_ops = ops
                        rte_vhost_driver_start(path)
            else -> vdev->backend->type == VHOST_BACKEND_BLK
                virtio_blk_construct_ctrlr(vdev, name, &cpumask, params, user_backend)
                    bvdev->ops->create_ctrlr(vdev, cpumask, address, params, (void *)user_backend) -> vhost_user_blk_create_ctrlr
            TAILQ_INSERT_TAIL(&g_vhost_devices, vdev, tailq)
    free_rpc_vhost_scsi_ctrlr

static const struct spdk_vhost_dev_backend spdk_vhost_scsi_device_backend = {
    .type = VHOST_BACKEND_SCSI,
    .dump_info_json = vhost_scsi_dump_info_json,
    .write_config_json = vhost_scsi_write_config_json,
    .remove_device = vhost_scsi_dev_remove,
    .set_coalescing = vhost_user_set_coalescing,
    .get_coalescing = vhost_user_get_coalescing,
};

static const struct spdk_vhost_user_dev_backend spdk_vhost_scsi_user_device_backend = {
    .session_ctx_size = sizeof(struct spdk_vhost_scsi_session) - sizeof(struct spdk_vhost_session),
    .start_session =  vhost_scsi_start,
        spdk_scsi_dev_allocate_io_channels
    .stop_session = vhost_scsi_stop,
    .alloc_vq_tasks = alloc_vq_task_pool,
};






rpc_vhost_scsi_controller_add_target
    vdev = spdk_vhost_dev_find(req.ctrlr)
    spdk_vhost_scsi_dev_add_tgt(vdev, req.scsi_target_num, req.bdev_name)
        svdev = to_scsi_dev(vdev)
        state->status = VHOST_SCSI_DEV_ADDING
        state->dev = spdk_scsi_dev_construct_ext(target_name, bdev_names_list, lun_id_list, 1, SPDK_SPC_PROTOCOL_IDENTIFIER_SAS, vhost_scsi_lun_resize, svdev, vhost_scsi_lun_hotremove, svdev)
            spdk_scsi_dev_add_lun_ext(dev, bdev_name_list[i], lun_id_list[i], resize_cb, resize_ctx, hotremove_cb, hotremove_ctx)
                scsi_dev_find_free_lun(dev, lun_id, &prev_lun)
                lun = scsi_lun_construct(bdev_name, resize_cb, resize_ctx, hotremove_cb, hotremove_ctx)
                    spdk_bdev_open_ext(bdev_name, true, bdev_event_cb, lun, &lun->bdev_desc)
                        bdev_desc_alloc(bdev, event_cb, event_ctx, &desc)
                            desc->memory_domains_supported = spdk_bdev_get_memory_domains(bdev, NULL, 0) > 0
                            desc->callback.event_fn = event_cb
                        bdev_open(bdev, write, desc)
                            bdev_start_qos(bdev)
                                spdk_bdev_for_each_channel(bdev, bdev_enable_qos_msg, ctx, bdev_enable_qos_done)
                            TAILQ_INSERT_TAIL(&bdev->internal.open_descs, desc, link)
                        -> bdev_event_cb
                            case SPDK_BDEV_EVENT_REMOVE
                            case SPDK_BDEV_EVENT_RESIZE
                                bvdev->ops->bdev_event(type, vdev, bdev_event_cpl_cb, (void *)type) -> bdev_event_cpl_cb
                                    if (type == SPDK_BDEV_EVENT_REMOVE)
                                        spdk_put_io_channel(bvdev->dummy_io_channel)
                                        spdk_bdev_close(bvdev->bdev_desc)
                TAILQ_INSERT_HEAD(&dev->luns, lun, tailq)
                or TAILQ_INSERT_AFTER(&dev->luns, prev_lun, lun, tailq)
        spdk_scsi_dev_add_port(state->dev, 0, "vhost")
            port = spdk_scsi_dev_find_port_by_id(dev, id)
            port = scsi_dev_find_free_port(dev)
            scsi_port_construct(port, id, dev->num_ports, name)
        vhost_user_dev_foreach_session(vdev, vhost_scsi_session_add_tgt, vhost_scsi_dev_add_tgt_cpl_cb, (void *)(uintptr_t)scsi_tgt_num)
            spdk_thread_send_msg(vdev->thread, foreach_session, ev_ctx)
    free_rpc_vhost_scsi_ctrlr_add_target(&req)



vhost_scsi_session_add_tgt
    spdk_scsi_dev_allocate_io_channels(svsession->scsi_dev_state[scsi_tgt_num].dev)
        TAILQ_FOREACH_SAFE(lun, &dev->luns, tailq, tmp_lun)
            scsi_lun_allocate_io_channel(lun)
                lun->io_channel = spdk_bdev_get_io_channel(lun->bdev_desc)
                    spdk_get_io_channel(__bdev_to_io_dev(spdk_bdev_desc_get_bdev(desc)))
                        dev = io_device_get(io_device)
                        ch = thread_get_io_channel(thread, dev)
                        RB_INSERT(io_channel_tree, &thread->io_channels, ch)
                        dev->create_cb(io_device, (uint8_t *)ch + sizeof(*ch))
    if (vhost_dev_has_feature(vsession, VIRTIO_SCSI_F_HOTPLUG))
        eventq_enqueue(svsession, scsi_tgt_num, VIRTIO_SCSI_T_TRANSPORT_RESET, VIRTIO_SCSI_EVT_RESET_RESCAN)


iscsi bdev:
The SPDK iSCSI bdev driver depends on libiscsi and hence is not enabled by default.,In order to use it, build SPDK with an extra `--with-iscsi-initiator` configure option.  The following command creates an `iSCSI0` bdev from a single LUN exposed at given iSCSI URL with `iqn.2016-06.io.spdk:init` as the reported initiator IQN

rpc.py bdev_iscsi_create -b iSCSI0 -i iqn.2016-06.io.spdk:init --url iscsi://127.0.0.1/iqn.2016-06.io.spdk:disk1/0
SPDK_RPC_REGISTER("bdev_iscsi_create", rpc_bdev_iscsi_create, SPDK_RPC_RUNTIME)
    create_iscsi_disk(req.name, req.url, req.initiator_iqn, bdev_iscsi_create_cb, request)
        req = calloc(1, sizeof(struct bdev_iscsi_conn_req))
        req->context = iscsi_create_context(initiator_iqn)
        req->create_cb = cb_fn
        iscsi_parse_full_url
        use libiscsi lib as iscsi client
        iscsi_full_connect_async(req->context, iscsi_url->portal, iscsi_url->lun,iscsi_connect_cb, req) -> 异步调用连接 lun，该函数将连接到门户、登录并验证 lun 是否可用
            iscsi_connect_cb
                iscsi_inquiry_task(iscsi, req->lun, 1,SCSI_INQUIRY_PAGECODE_LOGICAL_BLOCK_PROVISIONING,255, bdev_iscsi_inquiry_lbp_cb, req)
                    bdev_iscsi_inquiry_lbp_cb
                        task = iscsi_inquiry_task(context, req->lun, 1,SCSI_INQUIRY_PAGECODE_BLOCK_LIMITS,255, bdev_iscsi_inquiry_bl_cb, req)
                            bdev_iscsi_inquiry_bl_cb
                                scsi_datain_unmarshall
                                iscsi_readcapacity16_task(context, req->lun, iscsi_readcapacity16_cb, req)
                                    iscsi_readcapacity16_cb
                                        readcap16 = scsi_datain_unmarshall(task)
                                        status = create_iscsi_lun(req, readcap16->returned_lba + 1, readcap16->block_length, &bdev,  readcap16->lbppbe)
                                            lun = calloc(sizeof(*lun), 1)
                                            lun->bdev.module = &g_iscsi_bdev_module
                                            lun->bdev.blocklen = block_size;
                                            lun->bdev.blockcnt = num_blocks;
                                            lun->bdev.fn_table = &iscsi_fn_table
                                            spdk_io_device_register(lun, bdev_iscsi_create_cb, bdev_iscsi_destroy_cb, sizeof(struct bdev_iscsi_io_channel)
                                            spdk_bdev_register(&lun->bdev)
                                            lun->no_main_ch_poller = SPDK_POLLER_REGISTER(bdev_iscsi_no_main_ch_poll, lun,
                                                bdev_iscsi_poll_lun
                                                    pfd.events = iscsi_which_events(lun->context)
                                                    iscsi_service(lun->context, pfd.revents)
                                complete_conn_req -> req->create_cb -> bdev_iscsi_create_cb
        iscsi_set_initiator_username_pwd
        TAILQ_INSERT_TAIL(&g_iscsi_conn_req, req, link)
        g_conn_poller = SPDK_POLLER_REGISTER(iscsi_bdev_conn_poll, NULL, BDEV_ISCSI_CONNECTION_POLL_US)
            TAILQ_FOREACH_SAFE(req, &g_iscsi_conn_req, link, tmp)
            poll(&pfd, 1, 0)
            iscsi_service(context, pfd.revents) -> libiscsi -> iscsi_process_pdu




bdev_iscsi_create_cb
    spdk_json_write_string(w, spdk_bdev_get_name(bdev))
    spdk_jsonrpc_end_result(request, w)





vhost_user.h
typedef enum VhostUserRequest



struct spdk_bdev_io_stat {
    uint64_t bytes_read;
    uint64_t num_read_ops;
    uint64_t bytes_written;
    uint64_t num_write_ops;
    uint64_t bytes_unmapped;
    uint64_t num_unmap_ops;
    uint64_t bytes_copied;
    uint64_t num_copy_ops;
    uint64_t read_latency_ticks;
    uint64_t max_read_latency_ticks;
    uint64_t min_read_latency_ticks;
    uint64_t write_latency_ticks;
    uint64_t max_write_latency_ticks;
    uint64_t min_write_latency_ticks;
    uint64_t unmap_latency_ticks;
    uint64_t max_unmap_latency_ticks;
    uint64_t min_unmap_latency_ticks;
    uint64_t copy_latency_ticks;
    uint64_t max_copy_latency_ticks;
    uint64_t min_copy_latency_ticks;
    uint64_t ticks_rate;

    /* This data structure is privately defined in the bdev library.
     * This data structure is only used by the bdev_get_iostat RPC now.
     */
    struct spdk_bdev_io_error_stat *io_error;

    /* For efficient deep copy, no members should be added after io_error. */
};


SPDK_VIRTIO_BLK_TRANSPORT_REGISTER(vhost_user_blk, &vhost_user_blk);
static const struct spdk_virtio_blk_transport_ops vhost_user_blk = {
    .name = "vhost_user_blk",

    .dump_opts = vhost_user_blk_dump_opts,

    .create = vhost_user_blk_create,
    .destroy = vhost_user_blk_destroy,

    .create_ctrlr = vhost_user_blk_create_ctrlr,
        vhost_user_dev_register(vdev, address, cpumask, custom_opts)
    .destroy_ctrlr = vhost_user_blk_destroy_ctrlr,

    .bdev_event = vhost_user_bdev_event_cb,
        case SPDK_BDEV_EVENT_REMOVE:
            vhost_user_bdev_remove_cb(vdev, cb, cb_arg);
        case SPDK_BDEV_EVENT_RESIZE:
            vhost_user_blk_resize_cb(vdev, cb, cb_arg);
    .set_coalescing = vhost_user_set_coalescing,
    .get_coalescing = vhost_user_get_coalescing,
};





vhost sample:
1. start qemu server:
qemu-system-x86_64 -machine accel=kvm -cpu host \
    -m $mem -object memory-backend-file,id=mem,size=$mem,mem-path=/dev/hugepages,share=on \
    -mem-prealloc -numa node,memdev=mem \
    -chardev socket,id=char1,path=/tmp/sock0,server \
    -netdev type=vhost-user,id=hostnet1,chardev=char1  \
    -device virtio-net-pci,netdev=hostnet1,id=net1,mac=52:54:00:00:00:14 \
    ...


2. Start the vswitch as client: vswitch(client) -> qemu(server)
./dpdk-vhost -l 0-3 -n 4 --socket-mem 1024 --socket-file /tmp/sock0 --client
dpdk/examples/vhost/main.c -> main
    reset_dma
    us_vhost_parse_args
    nb_ports = rte_eth_dev_count_avail()
    mbuf_pool = rte_pktmbuf_pool_create("MBUF_POOL", total_num_mbufs, MBUF_CACHE_SIZE, 0, MBUF_DATA_SIZE, rte_socket_id())
    port_init(portid)
    rte_ctrl_thread_create(&tid, "print-stats", NULL, print_stats, NULL)
    RTE_LCORE_FOREACH_WORKER(lcore_id)
        rte_eal_remote_launch(switch_worker, NULL, lcore_id)
    rte_vhost_async_dma_configure
    get_async_flag_by_socketid
    rte_vhost_driver_register
    rte_vhost_driver_set_features(file, VIRTIO_NET_FEATURES)
    rte_vhost_driver_disable_features(file, 1ULL << VIRTIO_NET_F_MRG_RXBUF)
    rte_vhost_driver_callback_register(file, &virtio_net_device_ops)
    rte_vhost_driver_start(file)
        vsocket = find_vhost_user_socket(path)
    RTE_LCORE_FOREACH_WORKER(lcore_id)
        rte_eal_wait_lcore(lcore_id)
    rte_vhost_async_dma_unconfigure
    rte_eal_cleanup()



static const struct rte_vhost_device_ops virtio_net_device_ops =
{
  .new_device =  new_device,
        vdev = rte_zmalloc("vhost device", sizeof(*vdev), RTE_CACHE_LINE_SIZE)
        vhost_txbuff[i * RTE_MAX_VHOST_DEVICE + vid] = rte_zmalloc("vhost bufftable", sizeof(struct vhost_bufftable), RTE_CACHE_LINE_SIZE)
        int socketid = get_socketid_by_vid(vid)
        init_vid2socketid_array(vid, socketid)
        vhost_async_channel_register(vid)
            rte_vhost_async_channel_register(vid, VIRTIO_RXQ)
                async_channel_register(dev, vq) -> vhost：保留对 virtqueue 索引的引用，在 dev->virtqueue[] 数组中拥有对 vq 索引的反向引用，可以统一内部 API，只需传递 dev 和 vq。它还允许在日志消息中显示 vq 索引。删除不需要的 virtqueue 索引检查（例如在所有可用 virtqueue 上循环调用的静态帮助程序）。尽快移动 virtqueue 索引有效性检查
                    async = rte_zmalloc_socket(NULL, sizeof(struct vhost_async), 0, node)
                    async->pkts_info = rte_malloc_socket(NULL, vq->size * sizeof(struct async_inflight_info), RTE_CACHE_LINE_SIZE, node)
            rte_vhost_async_channel_register(vid, VIRTIO_TXQ)
        init_vhost_queue_ops(vid)
            vdev_queue_ops[vid].enqueue_pkt_burst = builtin_enqueue_pkts
                vs_enqueue_pkts(dev, queue_id, pkts, count)
                    rte_prefetch0(&vr->avail->ring[start_idx & (vr->size - 1)])
                    enqueue_pkt(dev, vr, pkts[i], desc_idx)
                    __atomic_add_fetch(&vr->used->idx, count, __ATOMIC_RELEASE)
                    queue->last_used_idx += count
                    rte_vhost_vring_call(dev->vid, queue_id)
            vdev_queue_ops[vid].dequeue_pkt_burst = builtin_dequeue_pkts
            else
                vdev_queue_ops[vid].enqueue_pkt_burst = async_enqueue_pkts
        vs_vhost_net_setup(vdev)
            rte_vhost_get_negotiated_features
            rte_vhost_get_mem_table
            rte_vhost_get_vring_numm
            rte_vhost_get_vhost_vring
                if (vq_is_packed(dev)) {
                    vring->desc_packed = vq->desc_packed;
                    vring->driver_event = vq->driver_event;
                    vring->device_event = vq->device_event;
                } else {
                    vring->desc = vq->desc;
                    vring->avail = vq->avail;
                    vring->used = vq->used;
                }
                vring->callfd  = vq->callfd;
                vring->kickfd  = vq->kickfd;
                vring->size    = vq->size;
        rte_vhost_enable_guest_notification(vid, VIRTIO_RXQ, 0); -> 使用vid替换virtio_net dev
            vhost_enable_guest_notification
                return vhost_enable_notify_packed(dev, vq, enable);
                or vhost_enable_notify_split
        rte_vhost_enable_guest_notification(vid, VIRTIO_TXQ, 0);
  .destroy_device = destroy_device,
  .vring_state_changed = vring_state_changed,
};




struct virtio_user_backend_ops virtio_ops_vdpa = {
	.setup = vhost_vdpa_setup,
	.destroy = vhost_vdpa_destroy,
	.get_backend_features = vhost_vdpa_get_backend_features,
	.set_owner = vhost_vdpa_set_owner,
	.get_features = vhost_vdpa_get_features,
	.set_features = vhost_vdpa_set_features,
	.set_memory_table = vhost_vdpa_set_memory_table,
        vhost_vdpa_iotlb_batch_begin(dev)
            msg.type = VHOST_IOTLB_MSG_V2
            msg.iotlb.type = VHOST_IOTLB_BATCH_BEGIN
            write(data->vhostfd, &msg, sizeof(msg)
        vhost_vdpa_dma_unmap(dev, NULL, 0, SIZE_MAX)
            msg.iotlb.type = VHOST_IOTLB_INVALIDATE
            write(data->vhostfd, &msg, sizeof(msg)
        if (rte_eal_iova_mode() == RTE_IOVA_VA) -> rte_eal_get_configuration()->iova_mode
            rte_memseg_contig_walk_thread_unsafe(vhost_vdpa_map_contig, dev)
                vhost_vdpa_dma_map(dev, ms->addr, ms->iova, len)
                    msg.iotlb.type = VHOST_IOTLB_UPDATE
                    msg.iotlb.iova = iova
        rte_memseg_walk_thread_unsafe(vhost_vdpa_map, dev)
            vhost_vdpa_dma_map(dev, ms->addr, ms->iova, len)
        vhost_vdpa_iotlb_batch_end(dev)
            msg.iotlb.type = VHOST_IOTLB_BATCH_END
	.set_vring_num = vhost_vdpa_set_vring_num, -> vhost_vdpa_ioctl(data->vhostfd, VHOST_SET_VRING_NUM, state)
	.set_vring_base = vhost_vdpa_set_vring_base, -> vhost_vdpa_ioctl(data->vhostfd, VHOST_SET_VRING_BASE, state)
	.get_vring_base = vhost_vdpa_get_vring_base,
	.set_vring_call = vhost_vdpa_set_vring_call,
        vhost_vdpa_ioctl(data->vhostfd, VHOST_SET_VRING_CALL, file)
	.set_vring_kick = vhost_vdpa_set_vring_kick, -> vhost_vdpa_ioctl(data->vhostfd, VHOST_SET_VRING_KICK, file)
	.set_vring_addr = vhost_vdpa_set_vring_addr, -> vhost_vdpa_ioctl(data->vhostfd, VHOST_SET_VRING_ADDR, addr)
	.get_status = vhost_vdpa_get_status,
	.set_status = vhost_vdpa_set_status,
	.get_config = vhost_vdpa_get_config,
	.set_config = vhost_vdpa_set_config,
	.enable_qp = vhost_vdpa_enable_queue_pair,
        vhost_vdpa_set_vring_enable(dev, &state)
            vhost_vdpa_ioctl(data->vhostfd, VHOST_VDPA_SET_VRING_ENABLE, state)
	.dma_map = vhost_vdpa_dma_map_batch,
	.dma_unmap = vhost_vdpa_dma_unmap_batch,
	.update_link_state = vhost_vdpa_update_link_state,
	.get_intr_fd = vhost_vdpa_get_intr_fd,
};



struct rte_vhost_mem_region {
	uint64_t guest_phys_addr;
	uint64_t guest_user_addr;
	uint64_t host_user_addr;
	uint64_t size;
	void	 *mmap_addr;
	uint64_t mmap_size;
	int fd;
};


often math, tool
#define SHIFT_2MB		21 /* (1 << 21) == 2MB */
#define VALUE_2MB		(1ULL << SHIFT_2MB)
#define MASK_2MB		(VALUE_2MB - 1)

#define SHIFT_4KB		12 /* (1 << 12) == 4KB */
#define VALUE_4KB		(1ULL << SHIFT_4KB)
#define MASK_4KB		(VALUE_4KB - 1)

#define _2MB_OFFSET(ptr)	(((uintptr_t)(ptr)) & MASK_2MB)
#define _2MB_PAGE(ptr)		FLOOR_2MB((uintptr_t)(ptr))
#define FLOOR_2MB(x)		(((uintptr_t)(x)) & ~MASK_2MB)
#define CEIL_2MB(x)		FLOOR_2MB(((uintptr_t)(x)) + VALUE_2MB - 1)





vhost_user_mmap_region




virtio_user_scsi_dev_create
    virtio_scsi_dev_init
        virtio_dev_start(vdev, max_queues, SPDK_VIRTIO_SCSI_QUEUE_NUM_FIXED)
            virtio_dev_set_status(vdev, VIRTIO_CONFIG_S_DRIVER_OK)
                if (!(virtio_dev_is_started(dev)))
                    vfu_virtio_dev_start






static struct spdk_vfu_pci_device vfu_virtio_device_info region bar4:
.access_cb = virtio_vfu_access_bar4,


virtio_vfu_access_bar4
    virtio_vfu_pci_common_cfg
        case VIRTIO_PCI_COMMON_STATUS
            virtio_dev_set_status(dev, value)



virtio_blk_dev_init
    bdev_virtio_blk_ch_create_cb
        ch->poller = SPDK_POLLER_REGISTER(bdev_virtio_poll, ch, 0)
            bdev_virtio_poll   
                process_scan_resp
                    process_read_cap_10
                        virtio_scsi_dev_add_tgt
                            bdev->fn_table = &virtio_fn_table

static const struct spdk_bdev_fn_table virtio_fn_table = {
	.destruct		= bdev_virtio_disk_destruct,
	.submit_request		= bdev_virtio_submit_request,
        _bdev_virtio_submit_request
        bdev_virtio_command
            req->type = VIRTIO_BLK_T_IN
            bdev_virtio_blk_send_io(ch, bdev_io)
                struct bdev_virtio_blk_io_channel *virtio_channel = spdk_io_channel_get_ctx(ch)
                struct virtqueue *vq = virtio_channel->vq
                struct virtio_blk_io_ctx *io_ctx = (struct virtio_blk_io_ctx *)bdev_io->driver_ctx
                virtqueue_req_start(vq, bdev_io, bdev_io->u.bdev.iovcnt + 2)
                    vq->req_start = vq->vq_desc_head_idx
                    dxp = &vq->vq_descx[vq->req_start]
                virtqueue_req_add_iovs(vq, &io_ctx->iov_req, 1, SPDK_VIRTIO_DESC_RO)
                    for (i = 0; i < iovcnt; ++i)
                        desc = &vq->vq_ring.desc[new_head]
                        desc->addr  = (uintptr_t)iovs[i].iov_base
                        or desc->addr = spdk_vtophys(iovs[i].iov_base, NULL)
                        desc->flags = desc_type | VRING_DESC_F_NEXT -> desc flag
                virtqueue_req_add_iovs(vq, bdev_io->u.bdev.iovs, bdev_io->u.bdev.iovcnt,
                virtqueue_req_add_iovs(vq, &io_ctx->iov_resp, 1, SPDK_VIRTIO_DESC_WR)
                virtqueue_req_flush(vq)
                    finish_req(vq)
                    vring_used_event(&vq->vq_ring) = vq->vq_used_cons_idx - vq->vq_nentries - 1 -> #define vring_used_event(vr) ((vr)->avail->ring[(vr)->num]) -> #define vring_avail_event(vr) (*(__virtio16 *)&(vr)->used->ring[(vr)->num])
                    vring_need_event(vring_avail_event(&vq->vq_ring), vq->vq_avail_idx, vq->vq_avail_idx - reqs_finished)
                    virtio_dev_backend_ops(vq->vdev)->notify_queue(vq->vdev, vq)
	.io_type_supported	= bdev_virtio_io_type_supported,
	.get_io_channel		= bdev_virtio_get_io_channel,
	.dump_info_json		= bdev_virtio_dump_info_json,
	.write_config_json	= bdev_virtio_write_config_json,
};



scsi
...
thread_poll
poller_rc = thread_execute_poller(thread, poller)
vdev_worker(void *arg)
    process_vq(svsession, &vsession->virtqueue[q_idx])
        unlikely reqs_cnt = vhost_vq_avail_ring_get(vq, reqs, SPDK_COUNTOF(reqs))
        rte_vhost_set_inflight_desc_split(vsession->vid, vq->vring_idx, reqs[i])
        process_scsi_task(vsession, vq, reqs[i])
            task = &((struct spdk_vhost_scsi_task *)vq->tasks)[req_idx]
            scsi_task_init(task)
            result = process_request(task)
                task_data_setup(task, &req)
                    spdk_scsi_task_construct(&task->scsi, vhost_scsi_task_cpl, vhost_scsi_task_free_cb)
                    vhost_vq_get_desc(vsession, task->vq, task->req_idx, &desc, &desc_table, &desc_table_len)
                        if (vhost_vring_desc_is_indirect(*desc))
                            *desc_table = vhost_gpa_to_vva(vsession, (*desc)->addr, sizeof(**desc) * *desc_table_size)
                    *req = vhost_gpa_to_vva(vsession, desc->addr, sizeof(**req))
                        vva = (void *)rte_vhost_va_from_guest_pa(vsession->mem, addr, &newlen)
                    vhost_vring_desc_get_next(&desc, desc_table, desc_table_len)
                vhost_scsi_task_init_target
                    task->scsi.target_port = spdk_scsi_dev_find_port_by_id(task->scsi_dev, 0)
                    task->scsi.lun = spdk_scsi_dev_get_lun(state->dev, lun_id)
            task_submit(task)
                spdk_scsi_dev_queue_task(task->scsi_dev, &task->scsi)
                    scsi_lun_execute_task
                        _scsi_lun_execute_task
                            bdev_scsi_execute
                                bdev_scsi_process_block
                                    bdev_scsi_readwrite
                                        spdk_bdev_readv_blocks
                                            bdev_readv_blocks_with_md
                                                _bdev_io_submit_ext
                                                    bdev_io_submit
                                                        _bdev_io_submit


cleanup_pci_devices
    TAILQ_FOREACH_SAFE(dev, &g_pci_hotplugged_devices, internal.tailq, tmp)
        vtophys_pci_device_added -> 将具有 DMA 功能的 PCI 设备报告给 vtophys 转换代码。增加 SPDK 管理的具有 DMA 功能的活动设备的引用计数。必须在创建 rte_pci_device 后调用此函数
            TAILQ_INSERT_TAIL(&g_vtophys_pci_devices, vtophys_dev, tailq)
            #if VFIO_ENABLED
                struct spdk_vfio_dma_map *dma_map
                TAILQ_FOREACH(dma_map, &g_vfio.maps, tailq) 
                    ioctl(g_vfio.fd, VFIO_IOMMU_MAP_DMA, &dma_map->map)





virtio blk OP:
#define VIRTIO_BLK_T_IN 0
#define VIRTIO_BLK_T_OUT 1
#define VIRTIO_BLK_T_FLUSH 4
#define VIRTIO_BLK_T_GET_ID 8
#define VIRTIO_BLK_T_GET_LIFETIME 10
#define VIRTIO_BLK_T_DISCARD 11
#define VIRTIO_BLK_T_WRITE_ZEROES 13
#define VIRTIO_BLK_T_SECURE_ERASE 14






vfu_virtio_blk_vring_poll
    struct vfu_virtio_dev *dev = blk_endpoint->virtio.dev
    for (i = 0; i < dev->num_queues; i++)
        vfu_virtio_vq_flush_irq(dev, vq)
            virtio_queue_event_is_suppressed(dev, vq) -> 被压制
                is_suppressed = vq->avail.driver_event->flags & VRING_PACKED_EVENT_FLAG_DISABLE
                or is_suppressed = vq->avail.avail->flags & VRING_AVAIL_F_NO_INTERRUPT
            if (!virtio_endpoint->coalescing_delay_us) -> 禁用中断合并
            spdk_get_ticks() < vq->next_event_time)
            vfu_virtio_vq_post_irq(dev, vq)
                vfu_irq_trigger(vfu_ctx, vq->vector)
                    eventfd_write(vfu_ctx->irqs->efds[subindex], val)
            vq->next_event_time = spdk_get_ticks() + delay_us * spdk_get_ticks_hz() / (1000000ULL)
        count += vfu_virito_dev_process_packed_ring(dev, vq)
            desc = &vq->desc.desc_packed[vq->last_avail_idx]
            if (!virtio_vring_packed_is_avail(desc, vq->packed.avail_phase)) -> check desc
            req = vfu_virtio_dev_get_req(virtio_endpoint, vq) -> init req
                req = STAILQ_FIRST(&vq->free_reqs)
                STAILQ_REMOVE_HEAD(&vq->free_reqs, link)
                req->num_descs = 0
            virtio_dev_packed_iovs_setup(dev, vq, vq->last_avail_idx, desc, req)
                if (virtio_vring_packed_desc_is_indirect(current_desc)) -> 间接描述符
                    desc_table = virtio_vring_packed_desc_to_iov(dev, current_desc, req->indirect_sg, req->indirect_iov)
                        spdk_vfu_map_one(virtio_endpoint->endpoint, desc->addr, desc->len, sg, iov, PROT_READ | PROT_WRITE)
                            vfu_addr_to_sgl(endpoint->vfu_ctx, (void *)(uintptr_t)addr, len, sg, 1, prot) -> 获取客户机物理地址范围并填充分散/聚集条目数组，这些条目可以单独映射到程序的虚拟内存中。由于内存映射方式的限制，单个线性客户机物理地址跨度可能需要拆分为多个分散/聚集区域。，在使用此函数之前必须先调用 vfu_setup_device_dma()
                            vfu_sgl_get(endpoint->vfu_ctx, sg, iov, 1, 0)
                virtio_vring_packed_desc_to_iov(dev, desc, virtio_req_to_sg_t(req, req->iovcnt), &req->iovs[req->iovcnt])
            virtio_endpoint->io_outstanding++
            virtio_endpoint->virtio_ops.exec_request(virtio_endpoint, vq, req)
        else count += vfu_virito_dev_process_split_ring(dev, vq)
            virtio_endpoint->virtio_ops.exec_request(virtio_endpoint, vq, req)

struct vfu_virtio_ops virtio_blk_ops = {
	.get_device_features = virtio_blk_get_supported_features,
	.alloc_req = virtio_blk_alloc_req,
        blk_req = calloc(1, sizeof(*blk_req) + dma_sg_size() * (VIRTIO_DEV_MAX_IOVS + 1))
	.free_req = virtio_blk_free_req,
	.exec_request = virtio_blk_process_req,
        iov = &req->iovs[0]
        hdr = iov->iov_base
        case VIRTIO_BLK_T_IN
            spdk_bdev_readv(blk_endpoint->bdev_desc, blk_endpoint->io_channel, &req->iovs[1], iovcnt, hdr->sector * 512, payload_len, blk_request_complete_cb, blk_req)
        case VIRTIO_BLK_T_OUT
            spdk_bdev_writev(blk_endpoint->bdev_desc, blk_endpoint->io_channel, &req->iovs[1], iovcnt, hdr->sector * 512, payload_len, blk_request_complete_cb, blk_req)
        ...
        case VIRTIO_BLK_T_FLUSH
            spdk_bdev_flush(blk_endpoint->bdev_desc, blk_endpoint->io_channel, 0, flush_bytes, blk_request_complete_cb, blk_req)
	.get_config = virtio_blk_get_device_specific_config,
	.start_device = virtio_blk_start,
        blk_endpoint->io_channel = spdk_bdev_get_io_channel(blk_endpoint->bdev_desc)
        blk_endpoint->ring_poller = SPDK_POLLER_REGISTER(vfu_virtio_blk_vring_poll, blk_endpoint, 0) -> spdk_poller for exec and unregister
	.stop_device = virtio_blk_stop,
};


vfu_virtio_dev_start
    ret = virtio_endpoint->virtio_ops.start_device(virtio_endpoint)



module/vfu_device: add virtio-blk emulation, Here we use vfu-tgt library and emulate a virtio-blk device as the first use case of vfu-tgt library.

Usage example with QEMU:

1. build/bin/spdk_tgt
2. scripts/rpc.py bdev_malloc_create -b malloc0 $((512)) 512
3. scripts/rpc.py vfu_virtio_create_blk_endpoint vfu.0 --bdev-name malloc0 \
--cpumask=0x1 --num-queues=2 \
--qsize=256 --packed-ring
4. Start QEMU with '-device vfio-user-pci,socket=/spdk/vfu.0'

__attribute__((constructor)) _vfu_virtio_blk_pci_model_register
    spdk_vfu_register_endpoint_ops(&vfu_virtio_blk_ops)
        TAILQ_INSERT_TAIL(&g_pci_device_ops, pci_ops, link)

struct spdk_vfu_endpoint_ops vfu_virtio_blk_ops = {
	.name = "virtio_blk",
	.init = vfu_virtio_blk_endpoint_init,
        vfu_virtio_endpoint_setup(&blk_endpoint->virtio, endpoint, basename, endpoint_name, &virtio_blk_ops)
            snprintf(path, PATH_MAX, "%s%s_bar4", basename, endpoint_name)
            open(path, O_RDWR | O_CREAT, S_IRUSR | S_IWUSR)
            ftruncate(virtio_endpoint->devmem_fd, VIRTIO_PCI_BAR4_LENGTH)
            virtio_endpoint->doorbells = mmap(NULL, VIRTIO_PCI_NOTIFICATIONS_LENGTH, PROT_READ | PROT_WRITE, MAP_SHARED, virtio_endpoint->devmem_fd, VIRTIO_PCI_NOTIFICATIONS_OFFSET)
            virtio_endpoint->virtio_ops = *ops
            virtio_endpoint->num_queues = VIRTIO_DEV_MAX_VQS -> 64
            virtio_endpoint->qsize = VIRTIO_VQ_DEFAULT_SIZE -> 128
	.get_device_info = vfu_virtio_blk_get_device_info,
	.get_vendor_capability = vfu_virtio_get_vendor_capability,
	.post_memory_add = vfu_virtio_post_memory_add,
        virtio_dev_map_vq(dev, &dev->vqs[i])
            phys_addr = ((((uint64_t)vq->desc_hi) << 32) | vq->desc_lo)
            vfu_virtio_map_q(dev, &vq->desc, phys_addr, len)
                addr = spdk_vfu_map_one(virtio_endpoint->endpoint, phys_addr, len, mapping->sg, &mapping->iov, PROT_READ | PROT_WRITE)
            vfu_virtio_map_q(dev, &vq->avail, phys_addr, len)
            vfu_virtio_map_q(dev, &vq->used, phys_addr, len)
	.pre_memory_remove = vfu_virtio_pre_memory_remove,
	.reset_device = vfu_virtio_pci_reset_cb,
	.quiesce_device = vfu_virtio_quiesce_cb,
	.destruct = vfu_virtio_blk_endpoint_destruct,
	.attach_device = vfu_virtio_attach_device,
        for (j = 0; j <= vq->qsize; j++)
            req = vfu_virtio_vq_alloc_req(virtio_endpoint, vq)
                endpoint->virtio_ops.alloc_req(endpoint, vq)
            req->indirect_sg = virtio_req_to_sg_t(req, VIRTIO_DEV_MAX_IOVS) -> (dma_sg_t *)(req->sg + iovcnt * dma_sg_size())
            req->dev = dev
            STAILQ_INSERT_TAIL(&vq->free_reqs, req, link)
	.detach_device = vfu_virtio_detach_device,
};



static struct spdk_bdev_module g_iscsi_bdev_module = {
	.name		= "iscsi",
	.module_init	= bdev_iscsi_initialize,
	.module_fini	= bdev_iscsi_finish,
	.config_json	= bdev_iscsi_config_json,
	.get_ctx_size	= bdev_iscsi_get_ctx_size,
};


static const struct spdk_bdev_fn_table iscsi_fn_table = {
	.destruct		= bdev_iscsi_destruct,
	.submit_request		= bdev_iscsi_submit_request,
        _bdev_iscsi_submit_request
            switch (bdev_io->type)
            case SPDK_BDEV_IO_TYPE_READ:
                spdk_bdev_io_get_buf
            case SPDK_BDEV_IO_TYPE_WRITE
                bdev_iscsi_writev
                    iscsi_write16_task bdev_iscsi_command_cb
                    scsi_task_add_data_in_buffer
	.io_type_supported	= bdev_iscsi_io_type_supported,
	.get_io_channel		= bdev_iscsi_get_io_channel,
        spdk_get_io_channel(lun)
	.dump_info_json		= bdev_iscsi_dump_info_json,
	.write_config_json	= bdev_iscsi_write_config_json,
};


blockdev_write_read_block
    blockdev_write_read


./hello_bdev -b iSCSI0 -m 0x2 -r /tmp/spdk.sock -c /root/xb/project/stor/spdk/examples/bdev/hello_world/bdev_iscsi.json
examples/bdev/hello_world/hello_bdev.c -> main
    hello_context.bdev_name = g_bdev_name -> Malloc0
    spdk_app_start(&opts, hello_start, &hello_context)
        spdk_bdev_open_ext(hello_context->bdev_name, true, hello_bdev_event_cb, NULL,
            hello_bdev_event_cb -> do nothing
        hello_context->bdev_io_channel = spdk_bdev_get_io_channel(hello_context->bdev_desc)
        hello_context->buff_size = spdk_bdev_get_block_size(hello_context->bdev) * spdk_bdev_get_write_unit_size(hello_context->bdev)
        hello_context->buff = spdk_dma_zmalloc(hello_context->buff_size, buf_align, NULL) -> 512B
        spdk_bdev_is_zoned(hello_context->bdev)
        hello_write(hello_context)
            spdk_bdev_write(hello_context->bdev_desc, hello_context->bdev_io_channel, hello_context->buff, 0, hello_context->buff_size, write_complete, hello_context)
                write_complete
                    spdk_bdev_free_io(bdev_io)
                    hello_read(hello_context)
                        spdk_bdev_read(hello_context->bdev_desc, hello_context->bdev_io_channel, hello_context->buff, 0, hello_context->buff_size, read_complete, hello_context)


spdk_pci_device_map_bar(pci_dev, 0, &hw->bar_va, &hw->bar_pa, &hw->bar_size) -> BAR0
    dev->map_bar(dev, bar, mapped_addr, phys_addr, size) -> map_bar_rte
        res = dpdk_pci_device_get_mem_resource(device->dev_handle, bar)
            g_dpdk_fn_table->pci_device_get_mem_resource(dev, bar)
                &dev->mem_resource[bar]
        *mapped_addr = res->addr;
        *phys_addr = (uint64_t)res->phys_addr;
        *size = (uint64_t)res->len;
#if VFIO_ENABLED
    if (rte_eal_iova_mode() == RTE_IOVA_VA)
        vtophys_iommu_map_dma_bar((uint64_t)(*mapped_addr), (uint64_t) * mapped_addr, *size)
    else
        vtophys_iommu_map_dma_bar((uint64_t)(*mapped_addr), *phys_addr, *size)




subsystem_nvme_create




rpc.py bdev_rbd_register_cluster rbd_cluster
rpc.py bdev_rbd_unregister_cluster rbd_cluster
rpc.py bdev_rbd_create rbd foo 512 -c rbd_cluster
rpc.py bdev_rbd_delete Rbd0
rpc.py bdev_rbd_resize Rbd0 4096


scripts/ceph/ceph.conf

SPDK_RPC_REGISTER("bdev_rbd_register_cluster", rpc_bdev_rbd_register_cluster, SPDK_RPC_RUNTIME) -> bdev_rbd_register_cluster(&req)
    spdk_call_unaffinitized(_bdev_rbd_register_cluster, info)
        rbd_register_cluster((const char *)info->name, (const char *)info->user_id, (const char *const *)info->config_param, (const char *)info->config_file, (const char *)info->key_file, info->core_mask)
            rados_create(&entry->cluster, user_id)
            rados_conf_read_file(entry->cluster, entry->config_file)
            rados_conf_set(entry->cluster, "keyring", key_file)
            rados_connect(entry->cluster)



SPDK_RPC_REGISTER("bdev_rbd_create", rpc_bdev_rbd_create, SPDK_RPC_RUNTIME)
    bdev_rbd_create
        bdev_rbd_init
        spdk_io_device_register(rbd, bdev_rbd_create_cb, bdev_rbd_destroy_cb, sizeof(struct bdev_rbd_io_channel),rbd_name)
        spdk_bdev_register(&rbd->disk)






SPDK_NVMF_TRANSPORT_REGISTER(muser, &spdk_nvmf_transport_vfio_user)
const struct spdk_nvmf_transport_ops spdk_nvmf_transport_vfio_user = {
	.name = "VFIOUSER",
	.type = SPDK_NVME_TRANSPORT_VFIOUSER,
	.opts_init = nvmf_vfio_user_opts_init,
	.create = nvmf_vfio_user_create,
	.destroy = nvmf_vfio_user_destroy,

	.listen = nvmf_vfio_user_listen,
        vfio_user_dev_info_fill
        vfio_user_register_accept_poller
	.stop_listen = nvmf_vfio_user_stop_listen,
	.cdata_init = nvmf_vfio_user_cdata_init,
	.listen_associate = nvmf_vfio_user_listen_associate,

	.listener_discover = nvmf_vfio_user_discover,

	.poll_group_create = nvmf_vfio_user_poll_group_create,
        vfio_user_poll_group_add_intr(vu_group, group)
            vu_group->intr_fd = eventfd(0, EFD_NONBLOCK)
            vu_group->intr = SPDK_INTERRUPT_REGISTER(vu_group->intr_fd, vfio_user_poll_group_intr, vu_group)
                spdk_fd_group_add(thread->fgrp, efd, _interrupt_wrapper, intr, intr->name)
                    epoll_ctl(epfd, EPOLL_CTL_ADD, efd, &epevent)
                vfio_user_poll_group_intr
                    eventfd_read(vu_group->intr_fd, &val)
                    vfio_user_poll_group_process(ctx)
                        nvmf_vfio_user_poll_group_poll(&vu_group->group) -> nvmf/vfio-user：支持中断模式下的多个轮询组，为了支持分配给除控制器主轮询组之外的轮询组的 SQ，我们需要确保在唤醒和处理控制器中断时轮询这些 SQ。由于它们将在单独的 SPDK 线程中运行，我们将安排所有轮询组在收到与 vfio-user 消息到达相对应的中断时唤醒。这可能意味着不必要的唤醒：我们（目前）没有一种机制来仅唤醒与特定 SQ 写入相对应的轮询组。此外，由于我们没有每个控制器的轮询组的概念，因此最终会轮询整个轮询组中的所有 SQ，而不仅仅是与我们正在处理的控制器相对应的 SQ。由于这在许多情况下存在潜在的性能问题，因此它默认为禁用
                            TAILQ_FOREACH_SAFE(sq, &vu_group->sqs, link, tmp)
                                nvmf_vfio_user_sq_poll(sq)
                                    handle_suppressed_irq(ctrlr, sq)
                                        spdk_ivdt_dcache(cq_dbl_headp(cq))
                                        vfu_irq_trigger(ctrlr->endpoint->vfu_ctx, cq->iv); -> Triggers an interrupt -> libvfio-user
                                    spdk_ivdt_dcache(sq_dbl_tailp(sq)) -> #define _spdk_ivdt_dcache(pdata)	asm volatile("dc civac, %0" : : "r"(pdata) : "memory"); -> arm Data cache clean and invalidate by VA to PoC
                                    spdk_rmb()
                                    count = handle_sq_tdbl_write(ctrlr, new_tail, sq)
                                        eventfd_write
                                        cmd = &queue[*sq_headp(sq)]
                                        sq_head_advance
                                        consume_cmd
                                            handle_cmd_req(ctrlr, cmd, sq)
                                                map_admin_cmd_req(ctrlr, req)
                                                    iovcnt = vfio_user_map_cmd(ctrlr, req, req->iov, len)
                                                        nvme_map_cmd(req, &req->cmd->nvme_cmd, iov, NVMF_REQ_MAX_BUFFERS, length, 4096, _map_one)
                                                            nvme_cmd_map_prps
                                                                vva = gpa_to_vva
                                                            nvme_cmd_map_sgls
                                                            _map_one(void *prv, uint64_t addr, uint64_t len, int prot) -> map_one
                                                                vfu_addr_to_sgl
                                                                vfu_sgl_get
                                                                index_to_sg_t(vu_req->sg, vu_req->iovcnt)
                                                                    (dma_sg_t *)((uintptr_t)arr + i * dma_sg_size())
                                                or  map_io_cmd_req(ctrlr, req)
                                                spdk_nvmf_request_exec(req)
                        vfio_user_poll_group_rearm(vu_group)
                            vfio_user_sq_rearm
            spdk_poller_register_interrupt(group->poller, set_intr_mode_noop, vu_group)
        TAILQ_INSERT_TAIL(&vu_transport->poll_groups, vu_group, link)
	.get_optimal_poll_group = nvmf_vfio_user_get_optimal_poll_group,
	.poll_group_destroy = nvmf_vfio_user_poll_group_destroy,
	.poll_group_add = nvmf_vfio_user_poll_group_add,
        admin = nvmf_qpair_is_admin_queue(&sq->qpair)
        vu_req = get_nvmf_vfio_user_req(sq)
        vu_req->cb_fn = handle_queue_connect_rsp
	.poll_group_remove = nvmf_vfio_user_poll_group_remove,
	.poll_group_poll = nvmf_vfio_user_poll_group_poll,
	.req_free = nvmf_vfio_user_req_free,
	.req_complete = nvmf_vfio_user_req_complete,
	.qpair_fini = nvmf_vfio_user_close_qpair,
	.qpair_get_local_trid = nvmf_vfio_user_qpair_get_local_trid,
	.qpair_get_peer_trid = nvmf_vfio_user_qpair_get_peer_trid,
	.qpair_get_listen_trid = nvmf_vfio_user_qpair_get_listen_trid,
	.qpair_abort_request = nvmf_vfio_user_qpair_abort_request,

	.poll_group_dump_stat = nvmf_vfio_user_poll_group_dump_stat,
};


没有 SPDK 事件框架的 NVMe-oF 目标，概述此示例用于展示如何使用 nvmf 库。在此示例中，我们希望鼓励用户使用 RPC 命令，因此我们仅支持 RPC 样式。用法此示例的用法与 nvmf_tgt 非常相似，不同之处在于您必须使用 RPC 命令来设置 nvmf 目标。首先，启动此示例应用程序。您可以使用 -m 来指定要使用的核心数。您可以使用 -h 来显示其他参数。./nvmf -m 0xf -r /var/tmp/spdk.sock然后，您需要使用 RPC 命令来配置 nvmf 目标。您可以使用 -h 来获取可以使用的 RPC 命令数。由于此示例是关于 nvmf 的，所以我认为您可以专注于 nvmf 命令和 bdev 命令。 ./scripts/rpc.py -h 接下来，您应该使用 RPC cmd 来设置 nvmf 目标。 
./scripts/rpc.py nvmf_create_transport -t RDMA -g nvmf_example 
./scripts/rpc.py nvmf_create_subsystem -t nvmf_example -s SPDK00000000000001 -a -m 32 nqn.2016-06.io.spdk:cnode1 
./scripts/rpc.py bdev_malloc_create -b Malloc1 128 512 
./scripts/rpc.py nvmf_subsystem_add_ns -t nvmf_example nqn.2016-06.io.spdk:cnode1 Malloc1 
./scripts/rpc.py nvmf_subsystem_add_listener -t rdma -f Ipv4 -a 192.168.0.10 -s 4420 -p nvmf_example nqn.2016-06.io.spdk:cnode1 最后，启动启动器连接 nvmf 示例目标并测试 IO $ROOT_SPDK/example/nvme/perf/perf -q 64 -o 4095 -w randrw -M 30 -l -t 60 -r “trtype:RDMA adrfam:IPv4 traddr:192.168.0.10 trsvcid:4420 subnqn:nqn.2016-06.io.spdk:cnode1”

examples/nvmf/nvmf/nvmf.c -> main
    nvmf_init_threads
        spdk_thread_lib_init_ext(nvmf_reactor_thread_op, nvmf_reactor_thread_op_supported, -> 每当 SPDK 创建一个新的轻量级线程时，它都会调用 nvmf_schedule_spdk_thread 要求应用程序通过 spdk_thread_poll() 开始轮询它。SPDK 中的每个轻量级线程都可以选择分配额外的内存供应用程序框架使用。分配的额外内存的大小是第二个参数
        SPDK_ENV_FOREACH_CORE(i) -> 每个 CPU 核心生成一个系统线程。系统线程称为反应器。SPDK 将生成必须映射到 nvmf_schedule_spdk_thread 中的反应器的轻量级线程。每个 CPU 核心使用单个系统线程是此应用程序独有的选择。SPDK 本身不需要这种特定的线程模型。例如，另一个可行的线程模型是使用工作队列将轻量级线程动态调度到线程池上
            nvmf_reactor->threads = spdk_ring_create(SPDK_RING_TYPE_MP_SC, 1024, SPDK_ENV_SOCKET_ID_ANY)
            TAILQ_INSERT_TAIL(&g_reactors, nvmf_reactor, link)
            spdk_env_thread_launch_pinned(i, nvmf_reactor_run, nvmf_reactor)
    nvmf_setup_signal_handlers
        sigact.sa_handler = nvmf_shutdown_cb
    spdk_thread_send_msg(g_init_thread, nvmf_target_app_start, NULL) -> nvmf_target_advance_state
        nvmf_create_nvmf_tgt
            spdk_nvmf_tgt_create
                tgt->subsystem_ids = spdk_bit_array_create(tgt->max_subsystems)
                    spdk_bit_array_resize
                spdk_io_device_register(tgt,nvmf_tgt_create_poll_group,	nvmf_tgt_destroy_poll_group,sizeof(struct spdk_nvmf_poll_group), tgt->name)
    nvmf_reactor_run(g_main_reactor)
        spdk_ring_dequeue(nvmf_reactor->threads, (void **)&lw_thread, 1)
        spdk_thread_poll(thread, 0, 0)





spdk vfio, ref: https://github.com/nutanix/libvfio-user/blob/master/docs/spdk.md
git clone https://github.com/spdk/spdk --branch v23.05
cd spdk
git submodule update --init --recursive
./configure --with-vfio-user
make
LD_LIBRARY_PATH=build/lib:dpdk/build/lib build/bin/nvmf_tgt &
rm -f /var/run/{cntrl,bar0}
scripts/rpc.py nvmf_create_transport -t VFIOUSER && \
	scripts/rpc.py bdev_malloc_create 512 512 -b Malloc0 && \
	scripts/rpc.py nvmf_create_subsystem nqn.2019-07.io.spdk:cnode0 -a -s SPDK0 && \
	scripts/rpc.py nvmf_subsystem_add_ns nqn.2019-07.io.spdk:cnode0 Malloc0 && \
	scripts/rpc.py nvmf_subsystem_add_listener nqn.2019-07.io.spdk:cnode0 -t VFIOUSER -a /var/run -s 0

qemu-orcl/build/qemu-system-x86_64 \
	-m 4G -object memory-backend-file,id=mem0,size=4G,mem-path=/dev/hugepages,share=on,prealloc=yes -numa node,memdev=mem0 \
	-device vfio-user-pci,socket=/var/run/cntrl
migrate -d tcp:<destination host IP address>:4444


https://intelkevinputnam.github.io/cloud-hypervisor-docs-HTML/docs/vfio-user.html
sudo scripts/setup.sh
rm ~/images/test-disk.raw
truncate ~/images/test-disk.raw -s 128M
mkfs.ext4  ~/images/test-disk.raw
sudo killall ./build/bin/nvmf_tgt
sudo ./build/bin/nvmf_tgt -i 0 -e 0xFFFF -m 0x1 &
sleep 2
sudo ./scripts/rpc.py nvmf_create_transport -t VFIOUSER
sudo rm -rf /tmp/nvme-vfio-user
sudo mkdir -p /tmp/nvme-vfio-user
sudo ./scripts/rpc.py bdev_aio_create ~/images/test-disk.raw test 512
sudo ./scripts/rpc.py nvmf_create_subsystem nqn.2019-07.io.spdk:cnode -a -s test
sudo ./scripts/rpc.py nvmf_subsystem_add_ns nqn.2019-07.io.spdk:cnode test
sudo ./scripts/rpc.py nvmf_subsystem_add_listener nqn.2019-07.io.spdk:cnode -t VFIOUSER -a /tmp/nvme-vfio-user -s 0
sudo chown $USER.$USER -R /tmp/nvme-vfio-user



scripts/rpc.py vfu_virtio_blk_add_bdev vfu.0 --bdev-name Null0 --num-queues=4 --qsize=128 --packed-ring


执行RPC:
SPDK_RPC_REGISTER("vfu_virtio_create_blk_endpoint", rpc_vfu_virtio_create_blk_endpoint, SPDK_RPC_RUNTIME)
rpc_vfu_virtio_create_blk_endpoint
    spdk_vfu_create_endpoint(req.name, req.cpumask, "virtio_blk")
        vfu_parse_core_mask
        spdk_vfu_get_endpoint_by_name(endpoint_name)
            TAILQ_FOREACH_SAFE(endpoint, &g_endpoint, link, tmp)
        ops = tgt_get_pci_device_ops(dev_type_name)
            TAILQ_FOREACH_SAFE(pci_ops, &g_pci_device_ops, link, tmp) -> 从全局操作表中查找
        basename = tgt_get_base_path() -> g_endpoint_path_dirname
        endpoint->endpoint_ctx = ops->init(endpoint, basename, endpoint_name) -> vfu_virtio_blk_endpoint_init
        tgt_endpoint_realize(endpoint)
            ret = endpoint->ops.get_device_info(endpoint, &pci_dev) -> vfu_virtio_blk_get_device_info
                vfu_virtio_get_device_info(&blk_endpoint->virtio, device_info)
                    memcpy(device_info, &vfu_virtio_device_info, sizeof(*device_info))
                    device_info->regions[VFU_PCI_DEV_BAR4_REGION_IDX].fd = virtio_endpoint->devmem_fd
                device_info->id.did = PCI_DEVICE_ID_VIRTIO_BLK_MODERN
            endpoint->vfu_ctx = vfu_create_ctx
                vfu_ctx = calloc(1, sizeof(vfu_ctx_t))
                vfu_ctx->tran = &tran_sock_ops
                vfu_setup_device_nr_irqs(vfu_ctx, VFU_DEV_ERR_IRQ, 1) -> vfu_ctx->irq_count[type] = count
                vfu_ctx->tran->init(vfu_ctx) -> tran_sock_init
            vfu_pci_init(endpoint->vfu_ctx, VFU_PCI_TYPE_EXPRESS, PCI_HEADER_TYPE_NORMAL, 0)
                cfg_space = calloc(1, size)
                vfu_ctx->pci.config_space = cfg_space
                vfu_ctx->reg_info[VFU_PCI_DEV_CFG_REGION_IDX].size = size
            vfu_pci_set_id
            vfu_pci_set_class
            cap_size = endpoint->ops.get_vendor_capability(endpoint, buf, 256, vendor_cap_idx)
            cap_offset = vfu_pci_add_capability(endpoint->vfu_ctx, 0, 0, vendor_cap)
            vfu_setup_region(endpoint->vfu_ctx, region_idx, region->len, region->access_cb, region->flags, region->nr_sparse_mmaps ? sparse_mmap : NULL, region->nr_sparse_mmaps, region->fd, region->offset)
                copyin_mmap_areas(reg, mmap_areas, nr_mmap_areas)
                    memcpy(reg_info->mmap_areas, mmap_areas, size)
            vfu_setup_device_dma(endpoint->vfu_ctx, tgt_memory_region_add_cb, tgt_memory_region_remove_cb)
                vfu_ctx->dma = dma_controller_create(vfu_ctx, MAX_DMA_REGIONS, MAX_DMA_SIZE)
                    dma = malloc(offsetof(dma_controller_t, regions) +
            vfu_setup_device_reset_cb(endpoint->vfu_ctx, tgt_device_reset_cb)
            vfu_setup_device_quiesce_cb(endpoint->vfu_ctx, tgt_device_quiesce_cb)
            vfu_setup_device_nr_irqs
            vfu_realize_ctx
            endpoint->pci_config_space = vfu_pci_get_config_space(endpoint->vfu_ctx)
            init_pci_config_space
                p->hdr.bars[0].raw = 0x0
                p->hdr.intr.ipin = ipin
                ...
        endpoint->thread = spdk_thread_create(endpoint_name, &cpumask)
        spdk_thread_send_msg(endpoint->thread, tgt_endpoint_start_thread, endpoint)
            endpoint->accept_poller = SPDK_POLLER_REGISTER(tgt_accept_poller, endpoint, 1000) -> 服务收到vfio-user协议请求
                vfu_attach_ctx -> tran_sock_attach
                    ts->conn_fd = accept(ts->listen_fd, NULL, NULL)
                    ret = tran_negotiate(vfu_ctx, &ts->client_cmd_socket_fd) -> switch ctx
                endpoint->vfu_ctx_poller = SPDK_POLLER_REGISTER(tgt_vfu_ctx_poller, endpoint, 1000)
                    vfu_run_ctx(vfu_ctx)
                    do
                        err = get_request(vfu_ctx, &msg)
                        handle_request(vfu_ctx, msg)
                            switch (msg->hdr.cmd) -> 根据不同的协议类型进行处理
                            case VFIO_USER_DMA_MAP
                                handle_dma_map(vfu_ctx, msg, msg->in.iov.iov_base)
                            ...
                endpoint->ops.attach_device(endpoint)
    vfu_virtio_blk_add_bdev(req.name, req.bdev_name, req.num_queues, req.qsize, req.packed_ring)
        spdk_bdev_open_ext(bdev_name, true, bdev_event_cb, blk_endpoint, &blk_endpoint->bdev_desc) -> open and set bdev_desc -> 关联bdev
        virtio_blk_update_config(&blk_endpoint->blk_cfg, blk_endpoint->bdev, blk_endpoint->virtio.num_queues)
            blk_cfg->blk_size = spdk_bdev_get_block_size(bdev)


SPDK_RPC_REGISTER("vfu_tgt_set_base_path", rpc_vfu_set_base_path, SPDK_RPC_RUNTIME)
rpc_vfu_set_base_path
    spdk_vfu_set_socket_path
        snprintf(g_endpoint_path_dirname, sizeof(g_endpoint_path_dirname) - 2, "%s", basename)


struct transport_ops tran_sock_ops = {
    .init = tran_sock_init,
        snprintf(addr.sun_path, sizeof(addr.sun_path), "%s", vfu_ctx->uuid)
        bind -> listen
        vfu_ctx->tran_data = ts
    .get_poll_fd = tran_sock_get_poll_fd,
    .attach = tran_sock_attach,
    .get_request_header = tran_sock_get_request_header,
    .recv_body = tran_sock_recv_body,
    .reply = tran_sock_reply,
    .recv_msg = tran_sock_recv_msg,
    .send_msg = tran_sock_send_msg,
    .detach = tran_sock_detach,
    .fini = tran_sock_fini
};



tgt_memory_region_add_cb
    spdk_mem_register(info->mapping.iov_base, info->mapping.iov_len) ->  注册新的虚拟内存到 g_mem_reg_map 上，并且触发所有的回调
        spdk_mem_map_translate(g_mem_reg_map, (uint64_t)seg_vaddr, NULL)
        spdk_mem_map_set_translation(g_mem_reg_map, (uint64_t)vaddr, VALUE_2MB, seg_len == 0 ? REG_MAP_REGISTERED | REG_MAP_NOTIFY_START : REG_MAP_REGISTERED)
        map->ops.notify_cb(map->cb_ctx, map, SPDK_MEM_MAP_NOTIFY_REGISTER, seg_vaddr, seg_len)
    endpoint->ops.post_memory_add(endpoint, map_start, map_end)



handle_queue_connect_rsp
    start_ctrlr
        vu_ctrlr->vfu_ctx_poller = SPDK_POLLER_REGISTER(vfio_user_poll_vfu_ctx, vu_ctrlr, 1000)
        vu_ctrlr->vfu_ctx_poller = SPDK_POLLER_REGISTER(vfio_user_poll_vfu_ctx, vu_ctrlr, 0)
        vu_ctrlr->intr = SPDK_INTERRUPT_REGISTER(vu_ctrlr->intr_fd, vfio_user_ctrlr_intr, vu_ctrlr)




static struct spdk_bdev_module bdev_zoned_if = {
	.name = "bdev_zoned_block",
	.module_init = zone_block_init,
	.module_fini = zone_block_finish,
	.config_json = zone_block_config_json,
	.examine_config = zone_block_examine,
	.get_ctx_size = zone_block_get_ctx_size,
};





SPDK_RPC_REGISTER("bdev_zone_block_create", rpc_zone_block_create, SPDK_RPC_RUNTIME)
    vbdev_zone_block_create
        zone_block_insert_name
            TAILQ_INSERT_TAIL(&g_bdev_configs, name, link)





static struct spdk_bdev_module g_ftl_if = {
	.name		= "ftl",
	.module_init	= bdev_ftl_initialize,
	.module_fini	= bdev_ftl_finish,
	.examine_disk	= bdev_ftl_examine,
	.get_ctx_size	= bdev_ftl_get_ctx_size,
};




static struct spdk_bdev_module gpt_if = {
	.name = "gpt",
	.module_init = vbdev_gpt_init,
	.get_ctx_size = vbdev_gpt_get_ctx_size,
	.examine_disk = vbdev_gpt_examine,

};



SPDK_RPC_REGISTER("bdev_nvme_attach_controller", rpc_bdev_nvme_attach_controller, SPDK_RPC_RUNTIME)
    spdk_nvme_ctrlr_get_default_ctrlr_opts
        nvme_driver_init()
            if (spdk_process_is_primary())
                g_spdk_nvme_driver = spdk_memzone_reserve(SPDK_NVME_DRIVER_NAME,
            else g_spdk_nvme_driver = spdk_memzone_lookup(SPDK_NVME_DRIVER_NAME)
            nvme_robust_mutex_init_shared
            g_spdk_nvme_driver->hotplug_fd = spdk_pci_event_listen()
                socket, bind
    bdev_nvme_get_default_ctrlr_opts
        opts->reconnect_delay_sec = g_opts.reconnect_delay_sec
    spdk_nvme_transport_id_populate_trstring
    ctrlr = nvme_ctrlr_get_by_name(ctx->req.name)
        TAILQ_FOREACH(nbdev_ctrlr, &g_nvme_bdev_ctrlrs, tailq)
    bdev_nvme_create(&trid, ctx->req.name, ctx->names, ctx->req.max_bdevs, rpc_bdev_nvme_attach_controller_done, ctx, &ctx->req.drv_opts, &ctx->req.bdev_opts, multipath)
        nvme_ctrlr_get(trid)
        ctx->probe_ctx = spdk_nvme_connect_async(trid, &ctx->drv_opts, attach_cb);
            nvme_driver_init()
            probe_cb = nvme_connect_probe_cb
            nvme_probe_internal(probe_ctx, true)
        ctx->poller = SPDK_POLLER_REGISTER(bdev_nvme_async_poll, ctx, 1000)


create nvme contrller:
connect_attach_cb
    nvme_ctrlr_create
        nvme_bdev_ctrlr_create
            TAILQ_INSERT_TAIL(&g_nvme_bdev_ctrlrs, nbdev_ctrlr, tailq)





const struct spdk_nvmf_transport_ops spdk_nvmf_transport_rdma = {
	.name = "RDMA",
	.type = SPDK_NVME_TRANSPORT_RDMA,
	.opts_init = nvmf_rdma_opts_init,
	.create = nvmf_rdma_create,
	.dump_opts = nvmf_rdma_dump_opts,
	.destroy = nvmf_rdma_destroy,
	.listen = nvmf_rdma_listen,
	.stop_listen = nvmf_rdma_stop_listen,
	.cdata_init = nvmf_rdma_cdata_init,
	.listener_discover = nvmf_rdma_discover,
	.poll_group_create = nvmf_rdma_poll_group_create,
	.get_optimal_poll_group = nvmf_rdma_get_optimal_poll_group,
	.poll_group_destroy = nvmf_rdma_poll_group_destroy,
	.poll_group_add = nvmf_rdma_poll_group_add,
	.poll_group_remove = nvmf_rdma_poll_group_remove,
	.poll_group_poll = nvmf_rdma_poll_group_poll,
	.req_free = nvmf_rdma_request_free,
	.req_complete = nvmf_rdma_request_complete,
        nvmf_rdma_request_process(rtransport, rdma_req)
	.qpair_fini = nvmf_rdma_close_qpair,
	.qpair_get_peer_trid = nvmf_rdma_qpair_get_peer_trid,
	.qpair_get_local_trid = nvmf_rdma_qpair_get_local_trid,
	.qpair_get_listen_trid = nvmf_rdma_qpair_get_listen_trid,
	.qpair_abort_request = nvmf_rdma_qpair_abort_request,
	.poll_group_dump_stat = nvmf_rdma_poll_group_dump_stat,
};


spdk_nvmf_qpair_disconnect
    qpair->state_cb = _nvmf_qpair_destroy




struct spdk_nvme_ctrlr {

}



static const struct spdk_bdev_fn_table nvmelib_fn_table = {
	.destruct		= bdev_nvme_destruct,
	.submit_request		= bdev_nvme_submit_request,
        nbdev_io->submit_tsc = spdk_bdev_io_get_submit_tsc(bdev_io)
        nbdev_io->io_path = bdev_nvme_find_io_path(nbdev_ch)
        _bdev_nvme_submit_request(nbdev_ch, bdev_io)
            switch (bdev_io->type)
            case SPDK_BDEV_IO_TYPE_READ:
                bdev_nvme_readv(nbdev_io,
					     bdev_io->u.bdev.iovs,
					     bdev_io->u.bdev.iovcnt,
					     bdev_io->u.bdev.md_buf,
					     bdev_io->u.bdev.num_blocks,
					     bdev_io->u.bdev.offset_blocks,
					     bdev->dif_check_flags,
					     bdev_io->u.bdev.memory_domain,
					     bdev_io->u.bdev.memory_domain_ctx)
            case SPDK_BDEV_IO_TYPE_WRITE
                bdev_nvme_writev

	.io_type_supported	= bdev_nvme_io_type_supported,
	.get_io_channel		= bdev_nvme_get_io_channel,
	.dump_info_json		= bdev_nvme_dump_info_json,
	.write_config_json	= bdev_nvme_write_config_json,
	.get_spin_time		= bdev_nvme_get_spin_time,
	.get_module_ctx		= bdev_nvme_get_module_ctx,
	.get_memory_domains	= bdev_nvme_get_memory_domains,
	.reset_device_stat	= bdev_nvme_reset_device_stat,
	.dump_device_stat_json	= bdev_nvme_dump_device_stat_json,
};



const struct spdk_nvme_transport_ops pcie_ops = {
	.name = "PCIE",
	.type = SPDK_NVME_TRANSPORT_PCIE,
	.ctrlr_construct = nvme_pcie_ctrlr_construct,
	.ctrlr_scan = nvme_pcie_ctrlr_scan,
	.ctrlr_destruct = nvme_pcie_ctrlr_destruct,
	.ctrlr_enable = nvme_pcie_ctrlr_enable,

	.ctrlr_get_registers = nvme_pcie_ctrlr_get_registers,
	.ctrlr_set_reg_4 = nvme_pcie_ctrlr_set_reg_4,
	.ctrlr_set_reg_8 = nvme_pcie_ctrlr_set_reg_8,
	.ctrlr_get_reg_4 = nvme_pcie_ctrlr_get_reg_4,
	.ctrlr_get_reg_8 = nvme_pcie_ctrlr_get_reg_8,

	.ctrlr_get_max_xfer_size = nvme_pcie_ctrlr_get_max_xfer_size,
	.ctrlr_get_max_sges = nvme_pcie_ctrlr_get_max_sges,

	.ctrlr_reserve_cmb = nvme_pcie_ctrlr_reserve_cmb,
	.ctrlr_map_cmb = nvme_pcie_ctrlr_map_io_cmb,
	.ctrlr_unmap_cmb = nvme_pcie_ctrlr_unmap_io_cmb,

	.ctrlr_enable_pmr = nvme_pcie_ctrlr_enable_pmr,
	.ctrlr_disable_pmr = nvme_pcie_ctrlr_disable_pmr,
	.ctrlr_map_pmr = nvme_pcie_ctrlr_map_io_pmr,
	.ctrlr_unmap_pmr = nvme_pcie_ctrlr_unmap_io_pmr,

	.ctrlr_create_io_qpair = nvme_pcie_ctrlr_create_io_qpair,
	.ctrlr_delete_io_qpair = nvme_pcie_ctrlr_delete_io_qpair,
	.ctrlr_connect_qpair = nvme_pcie_ctrlr_connect_qpair,
	.ctrlr_disconnect_qpair = nvme_pcie_ctrlr_disconnect_qpair,

	.qpair_abort_reqs = nvme_pcie_qpair_abort_reqs,
	.qpair_reset = nvme_pcie_qpair_reset,
	.qpair_submit_request = nvme_pcie_qpair_submit_request,
        tr->req = req
        g_nvme_pcie_build_req_table[payload_type][sgl_supported](qpair, req, tr, dword_aligned)
            nvme_pcie_qpair_build_hw_sgl_request
                phys_addr = nvme_pcie_vtophys(qpair->ctrlr, virt_addr, &mapping_length)
                    spdk_vtophys(buf, size)
                        paddr_2mb = spdk_mem_map_translate(g_vtophys_map, vaddr, size) -> spdk用户态页表原理: https://zhuanlan.zhihu.com/p/133830777
        nvme_pcie_qpair_build_metadata(qpair, tr, sgl_supported, mptr_sgl_supported, dword_aligned) -> nvme/pcie：将 nvme_pcie_qpair_submit_request() 移至 pcie 公共层，PCIE 和 VFIOUSER 都可以使用此功能，唯一的区别是 VFIOUSER 应该使用 IOVA=VA 进行 vtophys 转换，因此这里我们将把该功能移至公共 PCIe 层作为第一步
            tr->meta_sgl.address = nvme_pcie_vtophys
            req->cmd.mptr = nvme_pcie_vtophys
        nvme_pcie_qpair_submit_tracker(qpair, tr)
            nvme_pcie_copy_command_mmio(&pqpair->cmd[pqpair->sq_tail], &req->cmd)
            or nvme_pcie_copy_command(&pqpair->cmd[pqpair->sq_tail], &req->cmd) -> Copy the command from the tracker to the submission queue
                _mm_stream_si128(&d128[0], _mm_load_si128(&s128[0]))
                ...
            nvme_pcie_qpair_ring_sq_doorbell(qpair)
                spdk_unlikely(pqpair->flags.has_shadow_doorbell)
                spdk_wmb()
                spdk_mmio_write_4(pqpair->sq_tdbl, pqpair->sq_tail)
                    spdk_compiler_barrier()
                    *addr = val
	.qpair_process_completions = nvme_pcie_qpair_process_completions,
	.qpair_iterate_requests = nvme_pcie_qpair_iterate_requests,
	.admin_qpair_abort_aers = nvme_pcie_admin_qpair_abort_aers,

	.poll_group_create = nvme_pcie_poll_group_create,
	.poll_group_connect_qpair = nvme_pcie_poll_group_connect_qpair,
	.poll_group_disconnect_qpair = nvme_pcie_poll_group_disconnect_qpair,
	.poll_group_add = nvme_pcie_poll_group_add,
	.poll_group_remove = nvme_pcie_poll_group_remove,
	.poll_group_process_completions = nvme_pcie_poll_group_process_completions,
	.poll_group_destroy = nvme_pcie_poll_group_destroy,
	.poll_group_get_stats = nvme_pcie_poll_group_get_stats,
	.poll_group_free_stats = nvme_pcie_poll_group_free_stats
};



const struct spdk_nvme_transport_ops vfio_ops = {
	.name = "VFIOUSER",
	.type = SPDK_NVME_TRANSPORT_VFIOUSER,
	.ctrlr_construct = nvme_vfio_ctrlr_construct,
	.ctrlr_scan = nvme_vfio_ctrlr_scan,
	.ctrlr_destruct = nvme_vfio_ctrlr_destruct,
	.ctrlr_enable = nvme_vfio_ctrlr_enable,

	.ctrlr_get_registers = nvme_vfio_ctrlr_get_registers,
	.ctrlr_set_reg_4 = nvme_vfio_ctrlr_set_reg_4,
	.ctrlr_set_reg_8 = nvme_vfio_ctrlr_set_reg_8,
	.ctrlr_get_reg_4 = nvme_vfio_ctrlr_get_reg_4,
	.ctrlr_get_reg_8 = nvme_vfio_ctrlr_get_reg_8,

	.ctrlr_get_max_xfer_size = nvme_vfio_ctrlr_get_max_xfer_size,
	.ctrlr_get_max_sges = nvme_vfio_ctrlr_get_max_sges,

	.ctrlr_create_io_qpair = nvme_pcie_ctrlr_create_io_qpair,
	.ctrlr_delete_io_qpair = nvme_pcie_ctrlr_delete_io_qpair,
	.ctrlr_connect_qpair = nvme_pcie_ctrlr_connect_qpair,
	.ctrlr_disconnect_qpair = nvme_pcie_ctrlr_disconnect_qpair,
	.admin_qpair_abort_aers = nvme_pcie_admin_qpair_abort_aers,

	.qpair_reset = nvme_pcie_qpair_reset,
	.qpair_abort_reqs = nvme_pcie_qpair_abort_reqs,
	.qpair_submit_request = nvme_pcie_qpair_submit_request,
	.qpair_process_completions = nvme_pcie_qpair_process_completions,

	.poll_group_create = nvme_pcie_poll_group_create,
	.poll_group_connect_qpair = nvme_pcie_poll_group_connect_qpair,
	.poll_group_disconnect_qpair = nvme_pcie_poll_group_disconnect_qpair,
	.poll_group_add = nvme_pcie_poll_group_add,
	.poll_group_remove = nvme_pcie_poll_group_remove,
	.poll_group_process_completions = nvme_pcie_poll_group_process_completions,
	.poll_group_destroy = nvme_pcie_poll_group_destroy,
	.poll_group_get_stats = nvme_pcie_poll_group_get_stats,
	.poll_group_free_stats = nvme_pcie_poll_group_free_stats
};



host, nvme, rdma:
const struct spdk_nvme_transport_ops rdma_ops = {
	.name = "RDMA",
	.type = SPDK_NVME_TRANSPORT_RDMA,
	.ctrlr_construct = nvme_rdma_ctrlr_construct,
        contexts = rdma_get_devices(NULL)
        nvme_ctrlr_construct(&rctrlr->ctrlr)
            TAILQ_INIT(&ctrlr->active_io_qpairs)
            TAILQ_INIT(&ctrlr->active_procs);
            STAILQ_INIT(&ctrlr->register_operations);
            RB_INIT(&ctrlr->ns)
        rctrlr->ctrlr.adminq = nvme_rdma_ctrlr_create_qpair
            struct nvme_rdma_qpair *rqpair
            rqpair->state = NVME_RDMA_QPAIR_STATE_INVALID
            nvme_qpair_init(qpair, qid, ctrlr, qprio, num_requests, async)
        nvme_ctrlr_add_process(&rctrlr->ctrlr, 0)
            STAILQ_INIT(&ctrlr_proc->active_reqs)
            STAILQ_INIT(&ctrlr_proc->async_events)
            TAILQ_INSERT_TAIL(&ctrlr->active_procs, ctrlr_proc, tailq)
	.ctrlr_scan = nvme_fabric_ctrlr_scan,
        nvme_ctrlr_probe
        spdk_nvme_ctrlr_get_default_ctrlr_opts
        nvme_transport_ctrlr_construct
        nvme_ctrlr_process_init -> state_machine -> nvme 状态机管理
            switch (ctrlr->state)
            case NVME_CTRLR_STATE_INIT_DELAY
            case NVME_CTRLR_STATE_CONNECT_ADMINQ
            ...
        nvme_ctrlr_cmd_identify
            nvme_allocate_request_user_copy
            cmd->opc = SPDK_NVME_OPC_IDENTIFY
            nvme_ctrlr_submit_admin_request(ctrlr, req) -> nvme_qpair_submit_request(ctrlr->adminq, req)
        nvme_wait_for_completion -> nvme_wait_for_completion_robust_lock_timeout -> nvme_wait_for_completion_robust_lock_timeout_poll
            spdk_nvme_poll_group_process_completions -> poll_group_process_completions
            or spdk_nvme_qpair_process_completions -> qpair_process_completions
        nvme_ctrlr_connected
        nvme_fabric_ctrlr_discover
	.ctrlr_destruct = nvme_rdma_ctrlr_destruct,
	.ctrlr_enable = nvme_rdma_ctrlr_enable,
	.ctrlr_set_reg_4 = nvme_fabric_ctrlr_set_reg_4,
	.ctrlr_set_reg_8 = nvme_fabric_ctrlr_set_reg_8,
	.ctrlr_get_reg_4 = nvme_fabric_ctrlr_get_reg_4,
	.ctrlr_get_reg_8 = nvme_fabric_ctrlr_get_reg_8,
	.ctrlr_set_reg_4_async = nvme_fabric_ctrlr_set_reg_4_async,
	.ctrlr_set_reg_8_async = nvme_fabric_ctrlr_set_reg_8_async,
	.ctrlr_get_reg_4_async = nvme_fabric_ctrlr_get_reg_4_async,
	.ctrlr_get_reg_8_async = nvme_fabric_ctrlr_get_reg_8_async,
	.ctrlr_get_max_xfer_size = nvme_rdma_ctrlr_get_max_xfer_size,
	.ctrlr_get_max_sges = nvme_rdma_ctrlr_get_max_sges,
	.ctrlr_create_io_qpair = nvme_rdma_ctrlr_create_io_qpair, -> nvme_rdma_ctrlr_create_qpair
        nvme_qpair_init
	.ctrlr_delete_io_qpair = nvme_rdma_ctrlr_delete_io_qpair,
	.ctrlr_connect_qpair = nvme_rdma_ctrlr_connect_qpair,
        nvme_rdma_parse_addr(&dst_addr, family, ctrlr->trid.traddr, ctrlr->trid.trsvcid)
        rdma_create_id(rctrlr->cm_channel, &rqpair->cm_id, rqpair, RDMA_PS_TCP)
        nvme_rdma_resolve_addr
            rdma_set_option(rqpair->cm_id, RDMA_OPTION_ID, RDMA_OPTION_ID_REUSEADDR,
            rdma_resolve_addr
            nvme_rdma_process_event_start -> nvme_rdma_qpair_process_cm_event
	.ctrlr_disconnect_qpair = nvme_rdma_ctrlr_disconnect_qpair,
	.ctrlr_get_memory_domains = nvme_rdma_ctrlr_get_memory_domains,
        domains[0] = rqpair->memory_domain->domain
	.qpair_abort_reqs = nvme_rdma_qpair_abort_reqs,
	.qpair_reset = nvme_rdma_qpair_reset,
	.qpair_submit_request = nvme_rdma_qpair_submit_request,
	.qpair_process_completions = nvme_rdma_qpair_process_completions,
	.qpair_iterate_requests = nvme_rdma_qpair_iterate_requests,
	.admin_qpair_abort_aers = nvme_rdma_admin_qpair_abort_aers,
	.poll_group_create = nvme_rdma_poll_group_create,
	.poll_group_connect_qpair = nvme_rdma_poll_group_connect_qpair,
        nvme_rdma_poll_group
        nvme_rdma_ctrlr_connect_qpair_poll
            nvme_rdma_process_event_poll
                nvme_rdma_poll_events
                nvme_rdma_qpair_process_cm_event
        nvme_rdma_cq_process_completions
            ibv_poll_cq(cq, batch_size, wc)
            nvme_rdma_process_recv_completion
                spdk_rdma_srq_queue_recv_wrs
                nvme_rdma_request_ready
                nvme_rdma_qpair_submit_recvs
                    spdk_rdma_qp_flush_recv_wrs
                        ibv_post_recv
            or nvme_rdma_process_send_completion -> nvme_rdma_qpair_submit_recvs
        nvme_rdma_qpair_submit_sends
	.poll_group_disconnect_qpair = nvme_rdma_poll_group_disconnect_qpair,
	.poll_group_add = nvme_rdma_poll_group_add,
	.poll_group_remove = nvme_rdma_poll_group_remove,
	.poll_group_process_completions = nvme_rdma_poll_group_process_completions,
	.poll_group_destroy = nvme_rdma_poll_group_destroy,
	.poll_group_get_stats = nvme_rdma_poll_group_get_stats,
	.poll_group_free_stats = nvme_rdma_poll_group_free_stats,
};


static const struct virtio_dev_ops virtio_vfio_user_ops = {
	.read_dev_cfg	= virtio_vfio_user_read_dev_config,
	.write_dev_cfg	= virtio_vfio_user_write_dev_config,
	.get_status	= virtio_vfio_user_get_status,
	.set_status	= virtio_vfio_user_set_status,
	.get_features	= virtio_vfio_user_get_features,
	.set_features	= virtio_vfio_user_set_features,
	.destruct_dev	= virtio_vfio_user_destruct_dev,
	.get_queue_size	= virtio_vfio_user_get_queue_size,
	.setup_queue	= virtio_vfio_user_setup_queue,
	.del_queue	= virtio_vfio_user_del_queue,
	.notify_queue	= virtio_vfio_user_notify_queue
};


enum vfio_user_command {
	VFIO_USER_VERSION			= 1,
	VFIO_USER_DMA_MAP			= 2,
	VFIO_USER_DMA_UNMAP			= 3,
	VFIO_USER_DEVICE_GET_INFO		= 4,
	VFIO_USER_DEVICE_GET_REGION_INFO	= 5,
	VFIO_USER_DEVICE_GET_REGION_IO_FDS	= 6,
	VFIO_USER_DEVICE_GET_IRQ_INFO		= 7,
	VFIO_USER_DEVICE_SET_IRQS		= 8,
	VFIO_USER_REGION_READ			= 9,
	VFIO_USER_REGION_WRITE			= 10,
	VFIO_USER_DMA_READ			= 11,
	VFIO_USER_DMA_WRITE			= 12,
	VFIO_USER_DEVICE_RESET			= 13,
	VFIO_USER_DIRTY_PAGES			= 14,
	VFIO_USER_MAX,
};






desc flag:
include/linux/virtio_ring.h
/* This marks a buffer as continuing via the next field. */
#define VRING_DESC_F_NEXT	1
/* This marks a buffer as write-only (otherwise read-only). */
#define VRING_DESC_F_WRITE	2
/* This means the buffer contains a list of buffer descriptors. */
#define VRING_DESC_F_INDIRECT	4

/*
 * Mark a descriptor as available or used in packed ring.
 * Notice: they are defined as shifts instead of shifted values.
 */
#define VRING_PACKED_DESC_F_AVAIL	7
#define VRING_PACKED_DESC_F_USED	15

/* The Host uses this in used->flags to advise the Guest: don't kick me when
 * you add a buffer.  It's unreliable, so it's simply an optimization.  Guest
 * will still kick if it's out of buffers. */
#define VRING_USED_F_NO_NOTIFY	1
/* The Guest uses this in avail->flags to advise the Host: don't interrupt me
 * when you consume a buffer.  It's unreliable, so it's simply an
 * optimization.  */
#define VRING_AVAIL_F_NO_INTERRUPT	1

/* Enable events in packed ring. */
#define VRING_PACKED_EVENT_FLAG_ENABLE	0x0
/* Disable events in packed ring. */
#define VRING_PACKED_EVENT_FLAG_DISABLE	0x1
/*
 * Enable events for a specific descriptor in packed ring.
 * (as specified by Descriptor Ring Change Event Offset/Wrap Counter).
 * Only valid if VIRTIO_RING_F_EVENT_IDX has been negotiated.
 */
#define VRING_PACKED_EVENT_FLAG_DESC	0x2 -> 为打包环中的特定描述符启用事件。（由描述符环更改事件偏移/包装计数器指定）。仅当已协商 VIRTIO_RING_F_EVENT_IDX 时才有效

/*
 * Wrap counter bit shift in event suppression structure
 * of packed ring.
 */
#define VRING_PACKED_EVENT_F_WRAP_CTR	15

/* We support indirect buffer descriptors */
#define VIRTIO_RING_F_INDIRECT_DESC	28

/* The Guest publishes the used index for which it expects an interrupt
 * at the end of the avail ring. Host should ignore the avail->flags field. */
/* The Host publishes the avail index for which it expects a kick
 * at the end of the used ring. Guest should ignore the used->flags field. */
#define VIRTIO_RING_F_EVENT_IDX		29

/* Virtio ring descriptors: 16 bytes.  These can chain together via "next". */
struct vring_desc {
	/* Address (guest-physical). */
	__virtio64 addr;
	/* Length. */
	__virtio32 len;
	/* The flags as indicated above. */
	__virtio16 flags;
	/* We chain unused descriptors via this, too */
	__virtio16 next;
};

#define vring_used_event(vr) ((vr)->avail->ring[(vr)->num])
#define vring_avail_event(vr) (*(__virtio16 *)&(vr)->used->ring[(vr)->num])

static inline void
vring_init(struct vring *vr, unsigned int num, void *p,
	   unsigned long align)
{
	vr->num = num;
	vr->desc = p;
	vr->avail = (struct vring_avail *)((char *)p + num * sizeof(struct vring_desc
								   ));
	vr->used = (void *)(((uintptr_t)&vr->avail->ring[num] + sizeof(__virtio16)
			     + align - 1) & ~(align - 1));
}

static inline unsigned
vring_size(unsigned int num, unsigned long align)
{
	return ((sizeof(struct vring_desc) * num + sizeof(__virtio16) * (3 + num)
		 + align - 1) & ~(align - 1))
	       + sizeof(__virtio16) * 3 + sizeof(struct vring_used_elem) * num;
}




#define RTE_MAX_MEMSEG_PER_LIST 8192
spdk_dma_malloc
    rte_malloc_socket



accel_fw.md
accel.h
_add_to_comp_list
spdk_accel_submit_copy


static struct spdk_accel_module_if g_sw_module = {
	.module_init		= sw_accel_module_init,
	.module_fini		= sw_accel_module_fini,
	.write_config_json	= NULL,
	.get_ctx_size		= sw_accel_module_get_ctx_size,
	.name			= "software",
	.supports_opcode	= sw_accel_supports_opcode,
	.get_io_channel		= sw_accel_get_io_channel,
	.submit_tasks		= sw_accel_submit_tasks,
	.crypto_key_init	= sw_accel_crypto_key_init,
	.crypto_key_deinit	= sw_accel_crypto_key_deinit,
	.crypto_supports_tweak_mode	= sw_accel_crypto_supports_tweak_mode,
	.crypto_supports_cipher	= sw_accel_crypto_supports_cipher,
};

enum accel_opcode {
	ACCEL_OPC_COPY			= 0,
	ACCEL_OPC_FILL			= 1,
	ACCEL_OPC_DUALCAST		= 2,
	ACCEL_OPC_COMPARE		= 3,
	ACCEL_OPC_CRC32C		= 4,
	ACCEL_OPC_COPY_CRC32C		= 5,
	ACCEL_OPC_COMPRESS		= 6,
	ACCEL_OPC_DECOMPRESS		= 7,
	ACCEL_OPC_ENCRYPT		= 8,
	ACCEL_OPC_DECRYPT		= 9,
	ACCEL_OPC_XOR			= 10,
	ACCEL_OPC_LAST			= 11,
};



struct spdk_io_channel {
	struct spdk_thread		*thread;
	struct io_device		*dev;  -> NVMe controller or soft entity(blobstore)
	uint32_t			ref;
	uint32_t			destroy_ref;
	RB_ENTRY(spdk_io_channel)	node;
	spdk_io_channel_destroy_cb	destroy_cb;
	uint8_t				_padding[40];
};
模块将在该结构的末尾分配额外的内存来存储对特定于硬件的引用（即 NVMe 队列对，或对子设备 spdk_io_channels（即虚拟 bdevs）的引用




struct virtio_blk_config {
	/* The capacity (in 512-byte sectors). */
	__u64 capacity;
	/* The maximum segment size (if VIRTIO_BLK_F_SIZE_MAX) */
	__u32 size_max;
	/* The maximum number of segments (if VIRTIO_BLK_F_SEG_MAX) */
	__u32 seg_max;
	/* geometry of the device (if VIRTIO_BLK_F_GEOMETRY) */
	struct virtio_blk_geometry {
		__u16 cylinders;
		__u8 heads;
		__u8 sectors;
	} geometry;

	/* block size of device (if VIRTIO_BLK_F_BLK_SIZE) */
	__u32 blk_size;

	/* the next 4 entries are guarded by VIRTIO_BLK_F_TOPOLOGY  */
	/* exponent for physical block per logical block. */
	__u8 physical_block_exp;
	/* alignment offset in logical blocks. */
	__u8 alignment_offset;
	/* minimum I/O size without performance penalty in logical blocks. */
	__u16 min_io_size;
	/* optimal sustained I/O size in logical blocks. */
	__u32 opt_io_size;

	/* writeback mode (if VIRTIO_BLK_F_CONFIG_WCE) */
	__u8 wce;
	__u8 unused;

	/* number of vqs, only available when VIRTIO_BLK_F_MQ is set */
	__u16 num_queues;

	/* the next 3 entries are guarded by VIRTIO_BLK_F_DISCARD */
	/*
	 * The maximum discard sectors (in 512-byte sectors) for
	 * one segment.
	 */
	__u32 max_discard_sectors;
	/*
	 * The maximum number of discard segments in a
	 * discard command.
	 */
	__u32 max_discard_seg;
	/* Discard commands must be aligned to this number of sectors. */
	__u32 discard_sector_alignment;

	/* the next 3 entries are guarded by VIRTIO_BLK_F_WRITE_ZEROES */
	/*
	 * The maximum number of write zeroes sectors (in 512-byte sectors) in
	 * one segment.
	 */
	__u32 max_write_zeroes_sectors;
	/*
	 * The maximum number of segments in a write zeroes
	 * command.
	 */
	__u32 max_write_zeroes_seg;
	/*
	 * Set if a VIRTIO_BLK_T_WRITE_ZEROES request may result in the
	 * deallocation of one or more of the sectors.
	 */
	__u8 write_zeroes_may_unmap;

	__u8 unused1[3];
} __attribute__((packed));



/**
 * Admin opcodes
 */
enum spdk_nvme_admin_opcode {
    ...
	SPDK_NVME_OPC_GET_LOG_PAGE			= 0x02,
};




spdk_bdev_nvme_admin_passthru
    bdev_io = bdev_channel_get_io(channel)
    bdev_io->type = SPDK_BDEV_IO_TYPE_NVME_ADMIN
    bdev_io_submit(bdev_io)
        bdev_nvme_admin_passthru
            spdk_nvme_ctrlr_cmd_admin_raw
                nvme_ctrlr_submit_admin_request


/** bdev I/O type */
enum spdk_bdev_io_type {
	SPDK_BDEV_IO_TYPE_INVALID = 0,
	SPDK_BDEV_IO_TYPE_READ,
	SPDK_BDEV_IO_TYPE_WRITE,
	SPDK_BDEV_IO_TYPE_UNMAP,
	SPDK_BDEV_IO_TYPE_FLUSH,
	SPDK_BDEV_IO_TYPE_RESET,
	SPDK_BDEV_IO_TYPE_NVME_ADMIN,
	SPDK_BDEV_IO_TYPE_NVME_IO,
	SPDK_BDEV_IO_TYPE_NVME_IO_MD,
	SPDK_BDEV_IO_TYPE_WRITE_ZEROES,
	SPDK_BDEV_IO_TYPE_ZCOPY,
	SPDK_BDEV_IO_TYPE_GET_ZONE_INFO,
	SPDK_BDEV_IO_TYPE_ZONE_MANAGEMENT,
	SPDK_BDEV_IO_TYPE_ZONE_APPEND,
	SPDK_BDEV_IO_TYPE_COMPARE,
	SPDK_BDEV_IO_TYPE_COMPARE_AND_WRITE,
	SPDK_BDEV_IO_TYPE_ABORT,
	SPDK_BDEV_IO_TYPE_SEEK_HOLE,
	SPDK_BDEV_IO_TYPE_SEEK_DATA,
	SPDK_BDEV_IO_TYPE_COPY,
	SPDK_BDEV_NUM_IO_TYPES /* Keep last */
};



fio:
/* FIO imports this structure using dlsym */
struct ioengine_ops ioengine = {
	.name			= "spdk_bdev",
	.version		= FIO_IOOPS_VERSION,
	.flags			= FIO_RAWIO | FIO_NOEXTEND | FIO_NODISKUTIL | FIO_MEMALIGN | FIO_DISKLESSIO,
	.setup			= spdk_fio_setup,
	.init			= spdk_fio_init,
	/* .prep		= unused, */
	.queue			= spdk_fio_queue,
	/* .commit		= unused, */
	.getevents		= spdk_fio_getevents,
	.event			= spdk_fio_event,
	/* .errdetails		= unused, */
	/* .cancel		= unused, */
	.cleanup		= spdk_fio_cleanup,
	.open_file		= spdk_fio_open,
	.close_file		= spdk_fio_close,
	.invalidate		= spdk_fio_invalidate,
	/* .unlink_file		= unused, */
	/* .get_file_size	= unused, */
	/* .terminate		= unused, */
	.iomem_alloc		= spdk_fio_iomem_alloc,
	.iomem_free		= spdk_fio_iomem_free,
	.io_u_init		= spdk_fio_io_u_init,
	.io_u_free		= spdk_fio_io_u_free,
#if FIO_HAS_ZBD
	.get_zoned_model	= spdk_fio_get_zoned_model,
	.report_zones		= spdk_fio_report_zones,
	.reset_wp		= spdk_fio_reset_wp,
#endif
#if FIO_IOOPS_VERSION >= 30
	.get_max_open_zones	= spdk_fio_get_max_open_zones,
#endif
	.option_struct_size	= sizeof(struct spdk_fio_options),
	.options		= options,
};


spdk_fio_init
    spdk_fio_init_env
        pthread_create(&g_init_thread_id, NULL, &spdk_init_thread_poll, td->eo)
            spdk_thread_lib_init(spdk_fio_schedule_thread, sizeof(struct spdk_fio_thread))
            spdk_thread_send_msg(fio_thread->thread, spdk_fio_bdev_init_start, &done)
    spdk_thread_send_msg(fio_thread->thread, spdk_fio_bdev_open, td)
        for_each_file(td, f, i)
            spdk_bdev_open_ext(f->file_name, true, fio_bdev_event_cb, NULL,
            target->ch = spdk_bdev_get_io_channel(target->desc)
    while (spdk_fio_poll_thread(fio_thread) > 0) {}
        spdk_thread_poll(fio_thread->thread, 0, 0)






iscsi_poll_group_poll
